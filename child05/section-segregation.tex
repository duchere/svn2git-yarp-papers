


The world around us is made of objects, some of which can more or less
move independently. As adults, we can judge which parts of the world
are likely to move as a group. This is computationally a difficult
judgement to make, since regions grouped by easily-defined visual
features do not reliably correspond to physical groups. Robots and
infants could attempt to learn to make such judgements based on
experience. There is evidence of this in infants, and initial attempts
with robots.


Issues:

\begin{itemize}

\item Measures segregation abilities at given ages.

\item Experiences that show separate movement of particular object.

\item Successful exploitation of such experiences.

\item Degree of generalization to other objects and situations.

\item Role of behavior.

\end{itemize}

Computational:

\begin{itemize}

\item object complexity

\item scene/presentation complexity

\end{itemize}

The kinds of experiences a na\"{i}ve observer might find
useful in this regard are many and have not been well characterized.
However, it is clear that human infants do learn generic principles
for making educated guesses about which surfaces belong together as
part of the same unit and which do not.  By 4 to 5 months of age,
infants can parse simple displays into units based on something like
static gestalt principles, probably some subset of these (e.g.,
Needham, 1998, 2000).

Similar results have been obtained by researchers using partly
occluded objects (Johnson--+ display??)


Initial studies indicated that infants used some collection of
features to parse the displays (Needham, 1998; Needham, \& Baillargeon,
1997, 1998); subsequent studies suggested that object shape is the key
feature that young infants use to identify boundaries between adjacent
objects (Needham, 1999).

{
\bf What exactly is object shape?
}


These principles may lead to many incorrect parsings, but they will
also provide reasonable best guess interpretations of uniform objects
in complex displays.  So, it might be that extensive amounts of
experience are required to `train up' this system.
However, it might also be that infants learn on the basis of
relatively few exposures to key events (Baillargeon, 1999).  This
possibility was investigated within the context of object segregation
by asking how infants' parsing of a display would be altered
by a brief prior exposure to one of the objects in the test display.


In this paradigm, a test display was used that was ambiguous to
4.5-month-old infants who had no prior experience with the display.
Prior experience was given that would help disambiguate the display
for infants.  This experience consisted of a brief prior exposure
(visual only) to a portion of the test display.  If infants used this
prior experience to help them parse the test display, they should see
the display as two separate objects and look relaibly longer when they
moved as a whole than when they move separately.  Alternately, if the
prior experience was ineffective in altering infants'
interpretation of the display, they should look about equally at the
display, just as the infants in the initial study with no particular
prior experience did (Needham \& Baillargeon, 1998).  Prior experiences
with either portion of the test display were effective in facilitating
infants' parsing of the test display.  However, when we
introduced changes between the box seen during familiarization and
that seen as part of the test display, an unexpected pattern emerged.
Nearly any change in the object's features introduced between
familiarization and test prevented infants from benefitting from this
prior experience.  So, even when infants saw a blue box with yellow
squares prior to testing, and the box used in testing had white
squares but was otherwise identical, they did not apply this prior
experience to the parsing of the test display.  However, infants did
benefit from the prior exposure when it was not in the features of the
object but rather in its orientation (Needham, 2001).  A change in the
orientation of the box from horizontally to vertically oriented led to
the facilitation in parsing seen in some prior experiments.  Thus,
infants even as young as 4.5- to 5-months of age know that to probe
whether they have seen an object before, they must attend to the
object's features rather than its spatial orientation
(Needham, 2001).

{
\bf suggests conservative generalization, but see later.
}

These results also support two additional conclusions.  First,
infants' object representations include detailed information
about the object's features.  Because infants'
application of their prior experience to the parsing of the test
display was so dependent on something close to an exact match between
the features, one much conclude that a highly detailed representation
is formed on the initial exposure and maintained during the
inter-trial-interval.  Because these features are remembered and used
in the absence of the initial item and in the presence of a different
item, this is strong evidence for infants' representational
abilities.  Secondly, 4.5-month-old infants are conservative
generalizers -- they do not extend information from one object to
another very readily.  But would they extend information from a {\bf group}
of objects to a new object that is a member of that group?

{
\bf We are heavily into dealing with object identity here,
as evaluated via a segregation test.
}

This question was investigated by Needham, Dueker, \& Lockhead (2005)
in a study using the same test display and a similar procedure as in
Needham (2001).  Infants were given prior experiences with collections
of objects, no one of which was an effective cue to the composition of
the test display when seen prior to testing.  A set of three similar
objects seen simultaneously prior to test did facilitate 4.5-month-old
infants segregation of the test display.  But no subset of these three
objects seen prior to testing facilitated infants' segregation
of the test display.  Also, not just any three objects functioned in
this way -- sets that had no variation within them or that were
too different from the relevant test item provided no facilitation.
Thus, experience with multiple objects that are varied but that are
similar to the target item is important to infants' transfer
of their experience to the target display.



This finding was brought into the ``real'' world by investigating
infants' parsing of a test display consisting of a novel key
ring (Needham et al., submitted; see Figure X).  According to a strict
application of organizational principles using object features, the
display should be seen as composed of (at least) two separate
objects -- the keys on one side of the screen and the separate
ring on the other side.  However, to the extent that infants recognize
the display as a member of a familiar category -- key
rings -- they should group the keys and ring into a single unit
that should move as a whole.  Our findings indicate that by 8.5 months
of age, infants parse the display into a single unit, expecting the
keys and ring to move together.  Younger infants do not see the
display as a single unit, and instead parse the keys and ring into
separate units.  Infants of both ages parsed an altered display, in
which the identifiable portions of the key ring were hidden by
patterned covers, as composed of two separate units.  Together, these
findings provide evidence that the studies of controlled prior
exposure described in the previous section are consistent with the
process as it occurs under natural circumstances.  Infants'
ordinary experiences present them with multiple similar exemplars of
key rings, and these exposures build a representation that can then be
applied to novel (and yet similar) instances of the key ring category,
altering the interpretation that would come from featurally-based
principles alone.



\subsection{How does generalization change with development?}

Supporting a differentiation view of the development of
generalization, Bahrick's findings suggest that young (i.e.,
2-month-old) infants are more likely to generalize farther from the
specific experiences they received than infants just a few months
older (get citation).  This finding suggests that experience might
serve to initially narrow and then extend the range of stimuli over
which young children will generalize.

{
\bf Time course of visual generalization.
}

\subsection{Role of behavior in the development of these skills}

Infants do not come prepared to segregate objects into units that
adults would consider meaningful.  Rather, infants learn how object
features can be used to predict object boundaries.  More than twenty
years ago, Kellman \& Spelke (1983) suggested that infants may be born
with knowledge about solid, three-dimensional objects and that this
knowledge could help them interpret portions of a moving object as
connected to other portions that were moving in unison.  However, this
assertion was put to the test by Slater and his colleagues (19XX), a
test that resulted in a new conception of the neonate's visual
world.  Rather than interpreting common motion as a cue to object
unity, they interpreted the visible portions of a partly occluded
object as clearly separate from each other, even when undergoing
common motion.  This finding was important because it revealed one way
in which learning likely changes how infants interpret their visual
world.

Evidence from of one set of studies reveals that young
infants' difficulty in collecting the relevant information
from visual displays may limit their success in these tasks (Johnson \&
Aslin 1995, 1996, then Johnson's eye tracking stuff showing
that infants who look to the other side of the occluder, sampling
information from both sides of it, are the ones who perceive the
object parts as connected.  So, eye movements are an important factor
in this picture)

{\bf eye movements}

Although adjacent objects present a very similar kind of perceptual
problem (are these surfaces connected or not), the critical components
of success might be quite different.  Early work with adjacent objects
indicated that at 3 months of age, infants tend to group all touching
surfaces into a single unit (Kestenbaum, Termine, \& Spelke, 1987).
Subsequent experiments have revealed that soon after this point in
development, infants begin to analyze the perceptual differences
between adjacent surfaces and segregate surfaces with different
features (but not those with similar features) into separate units
(Needham 2000).  Although infants can use the boundary seam between
two objects as a source of information about the likely separation
between them (Kaufman \& Needham, submitted), other work comparing
boundary-occluded and fully visible versions of the same displays
suggests that boundary information is not the only information infants
use to parse the objects in a display (Needham, 1998).  
{\bf this paragraph seems a bit out of place}

These changes in perception do not occur in a vaccuum but rather in a
child who is also experiencing a range of other develomental changes.
One of these changes occurs in object
exploration -- infants' visual, oral, and manual
investigation of objects are showing huge improvements during this
same time period (Rochat, 1989).  Relations between infants'
tendency to explore objects more or less actively and their accurate
parsing of an object display has been shown (Needham, 2000), paving
the way for future studies of connections between object exploration
and object perception.




A central question:

\begin{quote}

How could perception change with experience?

\end{quote}





For young infants, common motion is an important cue for 
perception of object unity.  Others: alignment, good form,
depth cues.  Spatial separation.

Infants also use information about specific objects or
classes of objects to guide their judgement.  
NEEDHAM, CANTLON, \& ORMSBEE 2005.

Can use information from just one experience.

Reorganize based on: judgements based on fairly general
principles, versus judgements based on specific object
knowledge.

In robotics, work related to object segregation is quite
primitive.  It is strongly influenced by the field
of computer vision, where ``object segmentation'' is a classic,
much studied problem.  See FOO for a fuller review than we give
here.  Representative work in computer vision,
see eg Berkeley (Malik and co).

How is the problem formalized in vision?  We want a function
which maps a matrix of pixels to a matrix of labels, where
any pair of locations should have the same label if and
only if their pixels are from the same object.
This is not well-defined in general; for example, in some
circumstances a person and everything they wear should be
considered a single object, in other cases they to clothes
should be separated; every object is composed of smaller
objects, etc.

What is omitted from this formalization?

What other problems are addressed in computer vision that
are relevant?


\subsection{Segregration in computational vision}

In computational/machine vision, the problem of segregation is
formalized as decomposing an image into collections of pixels, where
each collection ideally corresponds to an object.  To make this
decomposition, image features such as color, brightness, motion,
stereoscopic cues, and texture can be used; the basic heuristic
is that by any pair of measurements made within an object should
generally be more similar numerically than a pair of measurements
made on regions belonging to different objects.  Another family
of work concentrates on particular objects rather than the general
case: finding faces, or cars, for example.

The Torralba-led contextual approach.

Give intro to energy formalization, form used
for min-cut approaches in stereo, color, etc?

Active segmentation.

Training for segmentation and recognition.

\subsection{What is the problem}

What is so hard about segregation in the first place?

A basic limitation of many segmentation algorithms is
that they are designed with a shrink-wrapped mentality.


\subsection{Progression}

Early, infants may ignore object (surface?) features for grouping.
Later, infants may assign them high weight. (footnote in
Needham 2001).


\begin{figure}

\centerline{\includegraphics[width=0.5\columnwidth]{fig-pull}}

\caption{
Top: photos of one of Needhan's experiments; a yellow tube is 
pulled away from a blue box with white dots.
Bottom: segmentation of the photos.
}

\end{figure}


\subsection{The references}

Ross; Arsenio.


Michael G. Ross, Leslie Pack Kaelbling. Learning Static Object
Segmentation from Motion Segmentation. Twentieth National Conference
on Artificial Intelligence. July 2005.

\subsection{Possibilities}

``separately moving'' is not totally clear - consider e.g. a 
jacket, can move arms around a certain amount without body.

Bottom up or top down?  Bottom up and top down?  Interaction
with recognition?

Most commonly, but not always, segmentation is implemented
as a precursor to recognition.  Image comes in, gets
segmented, segments are then run through recognizer.

This is problematic, since segmentation without
recognition is relatively brittle -- it is uninformed.
For something like finding faces, it is more common
not to segment first -- but the common trick here
is to try all possible regions.

Good features, a progression.

\cite{swain91color}.

\cite{schiele00recognition}.

\cite{lowe04distinctive}

\cite{felzenszwalb04efficient}

\subsection{Importance}

The importance of object segregation in robotics is 
that it is impossible for a robot to do anything 
useful without it.  

What is available when sitting back and watching; 
what is available when acting.  See Arsenio.



\subsection{Box and tube}



Gestalt principles.

Time course.

Role of experience.

Role of behavior.

Proximity, closure,similarity, good continuation, common fate.

proto-surfaces /superpixels

maturation v experience \cite{quinn05learning}.

ecological statistics \cite{martin04learning} -- note
that object edges are subtle to recognize.

In figure, using \cite{felzenszwalb04efficient}.

There is a very large field of research in computer vision on the
problem of {\it image segmentation}.  This is formalized in various
ways.  Here is a typical formalism.  There is an input image, $X$,
which is a view of the world from a camera, represented
as a matrix of ``pixels'' where each pixel is a simple
real number representing intensity or a vector representing
RGB colors.

There is an output matrix, $Y$, where for each pixel we assign
a label.

There is an energy function that evaluates the output matrix
in terms of the input, and computes a scalar.  This function
is chosen such that choosing $Y$ to minimize it gives a
good segmentation.

$E(X,Y) = E_{smooth}(Y) + E_{data}(X,Y)$

$E_{smooth}$ measures how far the output deviates from being
smooth.  As far as this term is concerned, the smoother the
better -- there is a cost for neighboring pixels being 
assigned different labels.

$E_{data}$ measures conflict between the labelling and
the data.  

For stereo, the labels would correspond to disparity,
and $E_{data}$ can check that with the given disparity
the left and right image match well locally.
