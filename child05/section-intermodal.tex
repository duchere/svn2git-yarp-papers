
Events in the world have complicated effects, and can often have
a detectable impact on many of our senses.  Determining which
components of what we sense are due to the same event can be a 
difficult judgement to make.  It is a useful exercise, though.
(List reasons why)


\cite{lewkowicz00development}
\cite{lewkowicz80crossmodal}
\cite{lewkowicz04learning}
\cite{bahrick04development}
\cite{hernandez01development}
\cite{bahrick03development}
\cite{bahrick00intersensory}
\cite{gibson86ecological}
\cite{prince05synching}

In robotics: and in speech recognition / computer vision, there's
a lot of interest in matching lip movement with speech sounds - 
both to identify which of a set of people is speaking, and to
improve recognition performance by adding extra features.

Lorenzo suggests: tactile discrimination of objects, babybot work,
although not cross-modal.

Most events have components that are accessible through different
senses: A bouncing ball can be seen as well as heard; the position of
the observer's own hand can be seen and felt as it moves through the
visual field.  Although these perceptual experiences are clearly
separate from each other, composing separate `channels', we also
recognize meaningful correspondences between the input from these
channels.  How this is accomplished is not entirely clear.  Different
approaches to the development of intermodal perception posit that
infants' sensory experiences are (a) unified at birth and must be
differentiated from each other over development, or (b) separated at
birth and must be linked through repeated pairings. Although the time
frame over which either of these processes would occur has not been
well defined, research findings do suggest that intermodal
correspondences are detected early in development.

On what basis to infants detect these correspondences?  Some of the
earliest work on this topic revealed that even newborn infants look
for the source of a sound (Butterworth \& Castillo, 1976) and by 4
months of age have specific expectations about what they should see
when they find the source of the sound (Spelke, 1976).  More recent
investigations of infants' auditory-visual correspondences have
identified important roles for synchrony and other amodal properties
of objects -- properties that can be detected across multiple
perceptual modalities.  An impact (e.g., a ball bouncing) provides
amodal information because the sound of the ball hitting the surface
is coincident with a sudden change in direction of the ball's path of
motion.  Some researchers have argued (Bahrick, XXXX) that detection
and use of amodal object properties serves to bootstrap the use of
more idiosyncratic properties (e.g., the kind of sound made by an
object when it hits a surface).

A special case of visual-auditory intermodal perception is speech
perception.  We know that in adult humans, auditory and visual
information combine to create the speech sounds we hear (as evidenced
in the McGurk effect; McGurk \& McDonald (1976)).  Evidence for the
McGurk effect has also been found in infants as young as 5 months of
age (Johnson, Rosenblum, \& Schmuckler (1995).  Young infants are able
to match the visual and auditory components of one particular speech
sound (e.g., they match an open mouthshape to the ``Ah'' sound and a
wide flat mouthshape to the ``Ee'' sound -- Kuhl \& Meltzoff, 1982).
Detection of these multimodal correspondences facilitate infants'
identification of the appropriate speaker visually once the auditory
information is attended.

Movement of an object while a certain vowel sound was presented
facilitated infants' learning of the arbitrary object/sound pairing.
Thus this multimodal information that can be introduced into the
linguistic setting facilitates infants' learning of the arbitrary
connections between sounds and objects.

Multimodal motherese -- Parents tend to use new words in synchrony
with object motion, especially when their infants are younger and may
benefit more from help in attending to or understanding the referent
of the word (Gogate, Bahrick, \& Watson, 2000)



Although more work has been generated by the study of visual-auditory
relations in events, other modalities also offer this redundant
information.

Bushnell magic box study (1985, I think) -- infants reached into a box
that was created in such a way that what they saw inside the box
through a window did not match what they felt when they reached in.
10-month-olds manipulated for longer when there was a mismatch than
when there was a match in vision and haptic



Streri and Spelke did a series of studies a while back in which
infants were familiarized to two rings that were either rigidly
connected with a rod or loosely connected with a string.  Another
source of information available was the similarity in texture between
the two rings.  The studies were set up to see whether infants would
transfer information from the tactile modality (based on the texture
or movement information) with a visual display showing two separate
rings or rings connected as a single object.  Their results showed
that the rigic/nonrigid movements dimension was useful, but the
texture differences were not. Spelke used this as evidence in favor of
her idea that object attributes are not used but physical separations
or movements are used by babies to parse objects.  This idea is not
really endorsed anymore (as our studies and others have shown), but
these studies themselves might be of interest anyway.



Jeff Lockman has done some interesting work on the conditions under
which babies bang objects on surfaces.  He has found that babies are
sensitive to the affordances of the object in relation to the surface.
If th eobject has a rigid side and a nonrigid side, they'll turn the
object so that the rigid side faces the rigid surface and bang away.
The same is true if you put a handle on the object -- they'll match up
the rigid sides for banging.



On a more theoretical level, Bushnell \& Boudreau (1993) put forth the
claim that you can explain changes in infants' perceptual skills by
looking at what they're motorically capable of.  They discuss the
Klatzky \& Lederman Exploration Procedures and a few other examples in
some detail.  Their point about Klatzky \& Lederman was that until
babies/children are capable of producing the hand and arm movements
necessary for producing e.g., the up-and-down movements associated
with ``hefting'' thought to allow for the perception of object weight,
they're not going to perceive weight differences between objects.

Finally, I'm sure you know about Bahrick \& Lickliter's Intermodal (or
Multimodal?) Redundancy Hypothesis.  They've shown in babies and in
young animals (mostly bobwhite quail) that organisms learn better and
faster from multimodal stimulation.



The last thing I was thinking about mentioning is a little something
about affordances -- some people have distinguished between
first-order (or readily apparent, existing more in the external
object) affordances and second-order affordances that depend much more
on the observer's knowledge.  I'm not sure if that distinction would
be useful here or maybe even in the embodiment section, but I thought
I'd mention it.
