\cite{kirkham02visual}
\cite{johnson03building}

\section{Definitions}

Should provide the clearest possible definition of terms,
as a reference.  Human and robot uses of terms could deviate
from this definition if the deviation is also clearly described.

Define a cell as the smallest sensing unit, for whichever sensors are
under consideration (e.g. pixels for images).  Assume there are a
fixed number of cells $c_1...c_{N}$.

There is also an unknown set of causes $s_1...s_{M}$, which for a
bounded period of consideration we will consider fixed.  Each cell
$c_i$ provides varying sensor readings $c_i(t)$.  We would like to
generate assignments $a_j(t)$ which, for each cause, lists all the
cells that can reasonably be attributed to that cause.  Then we choose
our causes to be maximally useful in describing the environment.

It is a little difficult to say exactly what judgement should be
made in complicated cases.  Might be better to give examples,
and underspecify.

Specification for object segregation:

***********

stuff to be integrated [from Lorenzo]:


The paper focuses on objects. Can we say something about the role of the
body? Achieving eye-hand coordination proved to be extremely useful on the
robots we have worked on. Learning to act is necessary to perform active
exploration of objects (poking/pushing/tapping/grasping). During action the
ability to identify the body helps the robot to focus the attention on the
area of the visual space where "things are happening" (i.e. on the hand upon
contact with the object).


***********

Big potential difference between infants and humans: the role
of manipulation in shaping early perception.  Infants can't
act that much to begin with.



