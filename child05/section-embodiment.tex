The study of perception in biological systems cannot neglect the role
of the body and its morphology in the generation of the sensory information reaching the brain. One of the big steps forward of the neurophysiology of the last 20 years of 
the understanding of the functions of the brain 
is related to realizing that the brain controls actions rather than 
movements. That is, the most basic unit of control is not the activation 
of a specific muscle but rather an action unit which includes a goal, a 
motive for acting, specific modes of perception tailored to this goal, and the
recombination of functional modules and synergies of muscles to attain
the goal \cite{vonhoftsen04tics}. This shift was supported by accumulating evidence of the study of the motor system in animals and humans: for a more comprehensive treatment see for example \cite{rizzolatti88book, rizzolatti04review}. 

A modern view of biological motor control considers multiple controllers
which are {\em goal} specific (rather than effector specific) and multiple
homounculi and somatotopies that expand into multiple controllers for 
these goals. This particular type of generalization is, for example, 
crystal clear in one of the premotor areas that is correlated to the act 
of grasping. This area, called simply F5 (frontal area 5), contains neurons 
that are used for grasping with the left hand, the right hand or even with the 
mouth \cite{gallese96brain}. 
 
The next conceptual step in changing our view of the control of movement 
was made by the discovery of sensory neurons (e.g. visual) in this same
premotor cortex, area F5. For example, for what objects are concerned, 
it is now well established that the premotor cortex responds to the sight 
of objects (visual response) and, simultaneously, 
to a grasping action directed at the same object (motoric response) \cite{gallese96brain}. The two 
representations - motoric and visual - coexist in the same brain areas, 
even more so, in the same population of neurons.

Similar responses have been found in the parietal cortex. This forms a 
conspicuous bi-directional connection with the premotor cortex so that it 
is possible to speak of the fronto-parietal system. Parietal neurons have been 
found to respond to geometric global object features (e.g. their orientation 
in 3D) which seem in fact well tuned to the control of action. But clearly 
the fronto-parietal circuitry is active also when an extant movement does 
not become an actual one. The natural question to be posed is then what is 
the purpose of this activation: potential motor action or true object 
recognition?

 Multisensory neurons are just testimonies of how much action 
and perception, body and brain and deeply intertwined in shaping each
other during development and throughout adulthood.

%In primates, for example, the visual information that reaches
%the visual cortex has a non-uniform distribution, due to the peculiar
%arrangement and size of the photoreceptors in the retina. In many
%species the outer ears and the head filter the sound in a way that
%facilitate auditory localization. The intrinsic elasticity of the
%muscles acts as a low pass filter during the interaction between the
%limbs and the environment.

\subsection{Active perception and the body in infants}

Through the body, the brain performs actions to explore the environment
and collect information about its properties and rules. In their
experiments with human subjects, Lederman and Klazky
\cite{lederman87hand} have identified a set of stereotyped hand
movements ({\it exploratory procedures}) that adults use when
haptically exploring objects to determine properties like weight,
shape, texture and temperature. Lederman and Klatzky show that to each
property can be associated a preferential exploratory procedure which
is, if not required, at least optimal for its identification.

These observations support the theory that motor development and the
body play an important role in perceptual development in infancy
\cite{bushnell93motor}. Proper control of at least the head, the arm
and the hand is required before infants can reliably and repetitively
engage in interaction with objects. During the first months of life
the inability of infants to perform skillful movements with the hand
would prevent them from haptically exploring the environment and
perceive properties of objects like weight, volume, hardness and
shape. But, even more surprisingly, motor development could affect the
developmental course of object visual perception (like three
dimensional shape).
%
Further support to this theory comes from the recent experiment by
Needham and colleagues \cite{Needham02apick-me-up}, where the ability
of pre-reaching infants to grasp objects was artificially anticipated
by means of mittens with palms covered with velcro that stuck to some
toys prepared by the experimenters. The results showed that those
infants whose grasping ability had been enhanced by the glove, were
more interested in objects than a reference group of the same age that
developed `normally'. This suggests that, although artificial, the
boost in motor development produced by the glove anticipated the
infants' interest towards objects.

Exploiting actions for learning and perception requires the ability to
match actions with the agents that caused it. The sense of agency
\cite{jeannerod02mechanism} gives humans the sense of ownership of
their actions and implies the existence of an internal representation
of the body. Although some sort of self-recognition is already present
at birth, at least in the form of a simple hand-eye coordination
\cite{meer95thefunctional}, it is during the first months of
development that infants learn to recognize their body as a separate
entity acting in the world \cite{rochat00perceived}. It is believed
that to develop this ability infants exploit correlations across
different sensorial channels (combined double touch/correlation
between proprioception and vision).


\subsection{Active perception and the body in robots}

In robotics we have the possibility to study the link between action
and perception, and its implications on the realization of artificial
systems. Robots, like infants, can exploit the physical interaction with
the environment to enrich and control their sensorial
experience. However these abilities do not come for free. Very much
like an infant, the robot must first learn to identify and control its
body, so that the interaction with the environment is meaningful and,
at least to a certain extent, safe. Indeed, motor control is
challenging especially when it involves the physical interaction
between the robot and the world.

Inspired by the developmental psychology literature, roboticists have
begun to investigate the problem of self-recognition in robotics
\cite{yoshikawa03doestheinvariance,metta03early,natale05exploring,gold05learning}. Although
different in several aspects, in all these work the robot looks for
intermodal similarities and invariances to identify its body from the
rest of the world.  In the work of Yoshikawa
\cite{yoshikawa03doestheinvariance} the rationale is that for any
given posture the body of the robot is invariant with respect to the
rest of the world. The correlation between visual information and
proprioceptive feedback is learned by a neural network which is trained
to predict the position of the arms in the visual field.  Gold and
Scassellati \cite{gold05learning} solve the self-recognition problem
by exploiting knowledge of the time elapsing between the actions of
the robot and the associated sensorial feedback.  In the work of Metta
and Fitzpatrick \cite{metta03early} and Natale et
al. \cite{natale05exploring} actions are instead used to generate
visual motion with a known pattern. Similarities in the proprioceptive
and visual flow are searched to visually identify the hand of the
robot. Periodicity in this case enhances and simplifies the
identification. The robot learns a multimodal representation of its
hand that allows a robust identification in the visual field.

In our experience with robots we identified three scenarios in which
the body proved to be useful in solving perceptual tasks:

\begin{enumerate}

\item {\em direct exploration}: the body in this case is the interface to
   extract information about the objects. For example in
   \cite{natale04learning} haptic information was employed to
   distinguish objects with different shapes, a task that would be much
   more difficult if performed visually. In
   \cite{torres-jara05tapping} the robot learned to recognize a few
   objects by using the sound they generate upon contact with the
   fingers.

\item {\em controlled exploration}: use the body to perform actions to simplify
   perception. The robot can deliberately generate redundant
   information by performing periodic actions in the environment. The robot
   can also initiate actions and wait for the appearance of consequences 
   \cite{fitzpatrick03grounding}.

\item {\em the body as a reference frame}: during action the hand is the place
   where important events are most likely to occur. The ability to
   direct the attention of the robot towards the hand is particularly
   helpful during learning; in \cite{natale05exploring} we show how
   this ability allows the robot to learn a visual model of the
   objects it manages to grasp by simply inspecting the hand when
   touch is detected on the palm (see Figure~\ref{fig:robot}). In
   similar situations the same behavior could allow the robot to
   direct the gaze to the hand if something unexpected touches
   it. Eye-hand coordination seems thus important to establish a link
   between different sensory channels like touch and vision.

\end{enumerate}

%\subsection{Specificity and generalization of knowledge about the body}
% it's a shame we don't have time here to include the experiment
% about hand recognition and a similar work by Hinton based on a motor
% system.
%

%% \begin{figure}[t]

%% \centerline{
%% \includegraphics[width=0.75\columnwidth]{fig-babybot}
%% }

%% \caption{
%% %
%% Manipulation is another opportunity to perform object segregation. In this case the robot explores the visual appearances of an object that has grasped; the information collected in this way is used later on to segment the object \cite{natale05exploring}. The exploration in this case is facilitated by a pre-acquired body-schema that allows the robot to maintain the fixation of the hand. Left: the Babybot, the robotic platform used for this experiment. Right: the object exploration (1-3) and segmentation (4). (consider moving this figure in the segregation-robotics section). 
%% %
%% }

%% \label{fig:babybot}

%% \end{figure}

%%\subsection{more}

%Missing:
%Needham's experiments. 
%\cite{needham01object,needham97object}
%Looking time suggests that experience with objects moving separately
%or together can affect future perception in the right way.
%(also, the ``sticky hands'' work).


