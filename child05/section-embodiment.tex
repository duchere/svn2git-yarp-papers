
Hand regard.  Humans?  Robots?

Lorenzo suggests Babybot work on the visual exploration of objects.

Recent theories agree on the importance of the body to determine the emergence of intelligence and cognition in biological systems. 
From a computational point of view it is impossible to ignore the interaction between the body and the environment that leads to the formation of the sensory information reaching the brain. 
The morphology of the body affects perception in numerous ways. In primates, for example, the visual information that reaches the visual cortex has a non-uniform distribution, due to the peculiar arrangement and size of the photoreceptors in the retina. In many species the outer ears and the head filter the sound in a way that facilitate auditory localization. The intrinsic elasticity of muscles allows for a better adaptation of the limbs during interaction with the environment.
From a developmental point of view the interaction between the body and the environment is considered paramount for the correct perceptual and cognitive development of infants. 
Through the body the brain performs actions aimed to explore the environment and collect information about its properties and rules. For example through manipulation infants have direct control on the exploration of objects. When an infants holds an object, its exploration lasts longer and reveals information about weight and shape through senses like touch and proprioception. More articulated expolarative actions can bring about other properties (the sound generated from collisions, the way the object behaves when pushed and so on).

In robotics we have the possibility to study these aspects and their implications on the realization of artificial systems. Robots like humans can exploit the physical interaction with the environment to enrich and control their sensorial experience. In one experiment for example we have shown that by grasping object a robot can exploit tactile information to distinguish object of different shape, a task that would be much more difficult if performed visually.
However these abilities do not come for free. In order for the exploration of the environment to be meaningful and safe the robot must first learn how to control its body. It is perhaps not a coincidence that, in fact, motor and perceptual abilities develop together in infants. Proper control of at least the head, the arm and the hand is required before infants can reliably and repetitively engage in interaction with objects. Motor control is challenging especially when it involves the physical interaction between the robot and the world.

Exploiting actions for learning and perception requires the ability to match an action with the agent that caused it (sense of agency, Jeannerod 2002). The sense of agency gives humans the sense of ownership of their actions and implies the existence of an internal representation of the body. The experiments by Graziano and Graziano and colleagues (cite) support the existence of a representation of the arm in the premotor cortex of the primate brain. Rochat and Striano (Rochat and Striano 2000) showed that 5 months-old infants are able to recognize their own legs moving on a mirror from those of another infant. It is believed that to develop this ability infants exploit correlations across different sensorial channels (combined double touch/correlation between proprioception and vision), [although some sort of eye-hand coordination is already present at birth (Van der Meer et al. 1995)]. During development infants learn to recognize their body and to distinguish it from other entities in the environment (other people or objects). 

Inspired by these observations, roboticists have begun to investigate the problem of self-recognition in robotics (cite Yoshikawa/Metta Fitzpatrick/Natale). In the work of Yoshikawa (Yoshikawa 2003 check and cite also Yoshikawa 2004) the rationale is that for any given posture the body of the robot is invariant with respect to the rest of the world. Correlation between the visual information and the proprioceptive feedback representing different posture can be learnt by a neural network to predict the position of the arms in the visual field. In the work of Fitzpatrick and Metta, and later Natale actions are instead used to generate visual motion with a known pattern. Similarities in the proprioceptive and visual flow are searched to visually identify the hand of the robot. Periodicity in this case enhance and simplify the identification. As a result of this process the robot learns a multimodal representation of its hand that allows a robust identification in the visual field.

In our experience with robots the body schema (in our case applied only to the arm) proved useful because it allows the robot to identify the hand in the visual field and direct the gaze to fixate it. The ability to direct the attention of the robot towards the hand is particularly helpful during learning. In an experiment we showed how this ability allows the robot to learn a visual model of the object it manages to grasp by simply inspecting the hand when touch is detected on the palm. In other situations the same behavior could allow the robot to direct the gaze to the hand if something unexpected touches it. Eye-hand coordination seems thus important to establish a link between different sensory channels like touch and vision.