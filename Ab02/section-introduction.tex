
\section{Vision, action, and development}

Robots and animals are actors in their environment, not simply passive
observers.  This gives them the potential to examine the world using
causality, by performing probing actions and learning from the
response.  Tracing chains of causality from motor action to perception
(and back again) is important both to understand how the brain deals
with sensorimotor coordination and to implement those same functions
in an artificial system, such as a humanoid robot.

In this paper, we propose that such causal probing can be arranged in
a developmental sequence leading to a manipulation-driven
representation of objects.  We present results for many important steps
along the way, and describe how they fit in a larger scale implementation.
Also, we discuss in what sense our artificial implementation is substantially 
in agreement with neuroscience. 

\begin{figure}[tbh]
\begin{center}
\includegraphics[width=12cm]{mirror-cog.eps}
\caption{ 
\label{fig:mirror-cog}
%
The world is complicated, but contigent.
The ultimate goal of this work is for our robot to follow chains of
causation outwards from its own simple body into the complex world.
%
}
\end{center}
\end{figure}

We wanted to work with the simplest possible form of manipulation.
What is this?  It must at least include contact with objects; one
cannot do much without this.  And in fact, contact of any kind is
sufficient to evoke certain classes of object affordances, such as
rolling.  So we can reasonably use various styles of poking, prodding,
tapping, swiping, ... as basic manipulative gestures.  While cruder
than dextrous manipulation, working with poking allows us to push
much further along developmental, experimental, corrective... UNFINISHED


Table~\ref{tab:causation} shows three levels of causal complexity.
The simplest causal chain that the actor experiences is the
perception of its own actions.  The temporal aspect is immediate:
visual information is tightly synchronized to motor commands.
Once this causal connection is established, we can go further and use
it to active explore the boundaries of objects.  In this case, there
is one more step in the causal chain, and the temporal nature of
the response may be delayed since initiating a reaching movement doesn't
immediately elicit consequences in the environment.  


Finally we argue that extending this causal chain further will allow
the actor to make a connection between her own actions and the actions 
of another. This is reminiscent of what has been observed in the response
of the monkey's premotor cortex. 


\begin{table*}[htbp]
\begin{center}
\begin{tabular}{|p{5.2cm}|p{4.5cm}|p{4.5cm}|}
\hline
{\it type of activity} & {\it nature of causation} &  {\it time profile} \\ \hline\hline
{\bf sensorimotor coordination} & direct causal chain & strict synchrony \\ \hline
{\bf object probing} & one level of indirection & fast onset upon contact, potential for delayed effects\\ \hline
{\bf constructing mirror representation} &  complex causation involving multiple causal chains & arbitrarily delayed onset and effects\\ \hline
\end{tabular}
\caption{
\label{tab:causation}
%
Degrees of causal indirection. There is a natural
trend from simpler to more complicated tasks.  The more time-delayed
an effect, the more difficult it is to model.
%
}
\end{center}
\end{table*}



\ifverbose 
%% Shall we leave this section for the other paper? --paulfitz

\section{The elusive object}

\label{sect:introduction}

Sensory information is intrinsically ambiguous, and very distant from
the world of well-defined objects in which humans believe they live.  
What criterion should be applied to distinguish one object from
another?  How can perception support such a phenomenon as figure-ground
segmentation?  
Consider the example in Figure~\ref{fig:number-cross}.  It is
immediately clear that the drawing on the left is a cross, perhaps
because we already have a criterion, which allows segmenting on the
basis of the intensity difference. It is slightly less clear that the
zeros and ones int the first grid are still a cross. What can we say
about the second grid? If we are not told, and we do not have
the criterion to perform the figure-ground segmentation, we might
think this is just a random collection of numbers. But if we are told
that the criterion is ``prime numbers vs. non-prime'' then a cross can
still be identified.

While we have to be inventive to come up with a segmentation problem
that tests a human, we don't have to go far at all to find something
that baffles our robots.  Figure~\ref{fig:number-cross} (right) shows a
robot's-eye view of a cube sitting on a table.  Simple enough, but
many rules of thumb used in segmentation fail in this particular case.
And even an experienced human observer, diagnosing the cube as a
separate object based on its shadow and subtle differences in the
surface texture of the cube and table, could in fact be mistaken --
perhaps some malicious researcher is up to mischief.  The only way to
find out for sure is to take action, and start poking and prodding.
As early as 1734, Berkeley observed that:
%
\begin{quote}
...objects can only be known by
touch. Vision is subject to illusions, which arise from the
distance-size problem... \cite{berkeley72new}
\end{quote}
%
In this paper, we provide support for a more nuanced proposition: that
in the presence of physical contact, vision becomes more powerful, and many of
its illusions fade away.


%
\begin{figure}[tb]
\begin{center}
\includegraphics[width=8.0cm]{number-cross.eps}
\hspace{2cm}
\includegraphics[width=4cm]{setup-sequence.eps}
\caption{ 
\label{fig:number-cross}
%
On the left are three examples of crosses,
following~\cite{manzotti01coscienza}.  The human ability to segment
objects is not general-purpose, and improves with experience.
On the right is an image of a cube on a table, illustrating the
ambiguities that plague machine vision. 
The edges of the table and cube happen to be
aligned (dashed line), the colors of the cube and table are not well
separated, and the cube has a potentially confusing surface pattern.
%
}
\end{center}
\end{figure}
%
%

\fi



%
%
%
\input{section-bio}






\subsubsection*{A working hypothesis}

%%In particular, for the problem of getting an operational 
%%definition of object, it seems that action is a necessary component.
%%Taken together, these results from neuroscience suggest that relating
%%motor action to visual perception is fruitful.  

Taken together these results from neuroscience suggest a very basic role
for motor action. Certainly vision and action are
intertwined at a very basic level.  While an
experienced adult can interpret visual scenes perfectly well without
acting upon them, linking action and perception seems crucial to the
developmental process that leads to that competence.  We can construct
a working hypothesis: that action is required for object recognition in
cases where an agent has to develop categorization autonomously. 
Of course in standard supervised learning action is not required since
the trainer does the job of pre-segmenting the data by hand.  In an
ecological context, some other mechanism has to be provided.
Ultimately this mechanism is the body itself that through action
(under some suitable developmental rule) generates informative
percepts.

We can distinguish three main conceptual functions (similar to the 
schema of Arbib et al. \cite{arbib-1981}): reaching, grasping (manipulation), and
object recognition. These functions correspond, as mentioned in the 
previous sections, to three levels of causal understanding (see \ref{tab:causation}).
They form also a nice progression of abilities which emerge out
of very few initial assumptions. All that is required is the 
interaction betweeen the actor and the environment, and the abovementioned
developmental rules: i.e. specifying what information is retained during the
interaction, what sort of senory processing, what are the motor
primitives, etc. 

The neuroscience results outlined in the previous section can be streamlined
into a developmental sequence roughly following a dorsal to ventral
gradient. Unfortunately this is a question which was not investigated in detail
by neuroscientists, and there is very little empirical support for this claim
(beside the work of Kovacs et al. \cite{kovacs00human}).

What is certainly true is that the three modules/functions can be 
clearly identified. If our hypothesis is correct then 
the first developmental step has to be that of transporting the hand 
close to the object. In humans, this function is accomplished mostly by the
circuit VIP/LIP/7b-F4-F1. Reaching requires at least the detection of
the object and the transformation of its position into appropriate 
motor commands. Parietal neurons seem to be coding for the spatial
position of the object in non-retinotopic coordinates by taking
into account the position of the eyes with respect to the head. 
According to \cite{pouget-ducom-torri-bavelier-2002} and 
to \cite{flanders-daghestani-berthoz-1999} the 
gaze direction (the eye motor plant) seems to be the privileged
reference system used to code reaching. 
Relating to the description of causality, the link between an executed
motor action and its visual consequences can be easily formed by 
a subsystems that can detect causality in a short time frame (the immediate
aspect).


Once reaching is reliable enough, we can start to move our attention 
outwards onto objects. Area AIP and F5 are involved in the
control of grasping and manipulation. F5 talks to the 
primary motor cortex for the fine control of movement. 
The AIP-F5 system responds to the ``affordances'' of the observed 
object with respect to the
current abilities.
% (for a robotic implementation,
%these may be simply poking and prodding initially). 
Arbib and coworkers \cite{fagg-arbib-1998} proposed 
the FARS model as a possible description of the computation in AIP/F5. 
They did not however consider how affordances can be 
actually learned during the interaction with the environment. 
Learning and understanding affordances requires a slightly longer 
time frame since the initiation of an action (motor command) does not
immediately elicit a sensory consequence. In this example, the initiation
of reaching requires a mechanism to detect when an object is actually 
tocuhed, manipulated, and whether the collision/touch is causal to the
initiation of the movement.

The next step along this hypothetical developmental route is to 
acquire the F5 mirror representation. We might think of canonical neurons as
an association table of grasp/manipulation (action) types with object
(vision) types.  Mirror neurons can then be thought of as a 
second-level associative map which links together the observation of 
a manipulative action performed by somebody else with the neural 
representation of one's own action.
Mirror neurons bring us to an even higher level of causal 
understanding. In this case the action execution has to be associated
to a similar action executed by somebody else. The two events
do not need to be temporally close to each other. Arbitrary time delays
might occur.

The conditions for when this is feasible are a consequence of active
manipulation. During a manipulative act there are a number of
additional constraints that can be factored in to simplify
perception/computation.  For example, detection of useful events is
simplified by information from touch, by timing information 
about when
reaching started, and from a knowledge of the location of the object in
the first place.

The last subsystem to develop is object recognition. Object 
recognition can build on manipulation in finding the boundaries
of objects and segment from the background. More importantly,
once the same object is manipulated many times the brain can
start learning about the criteria to identify the object if 
it happens to see it again. This functions are
carried out by the infero-temporal cortex (IT).
The same considerations apply to the recognition of the 
manipulator (either self or foreign). In fact, STs specialized
to this task. Information about object identity is
also sent to the parietal cortex and contributes to 
the formation of the affordances. For the actual recognition we 
can resort to a fuzzier definition
of causality where mulitple instances of manipulation on a 
certain object need to be grouped together. That is, all the 
information (visual in this case) pertaining to a certain object
has to be grouped (and stored somewhere/somehow) to build a model of some sort
of the object.

\begin{table*}[htbp]
\begin{center}
\begin{tabular}{|p{3.5cm}|p{2.5cm}|p{4.5cm}|}
\hline
{\it nature of causation} & {\it main path} &  {\it function and/or behavior} \\ \hline\hline
{\bf direct causal chain} & VC-VIP/LIP/7b-F4-F1 & reaching\\ \hline
{\bf one level of indirection} & VC-AIP-F5-F1 & poking, prodding, grasping\\ \hline
{\bf complex causation involving multiple causal chains} & VC-AIP-F5-F1+STs+IT & mirror neurons, mimicry\\ \hline
{\bf complex causation involving multiple instances of manipulative acts} & STs+TE-TEO & object recognition\\ \hline
\end{tabular}
\caption{
\label{tab:circuits}
%
Degrees of causal indirection, localization and function in the brain.
%
}
\end{center}
\end{table*}

For the robotic implementation we followed the same developmental
pathway and exploited the same sort of causal links between actions and 
sensory feedback.

