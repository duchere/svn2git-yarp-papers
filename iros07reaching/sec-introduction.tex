\section{Introduction}
Growing evidence in developmental psychology shows the importance 
of motor activity for cognitive development in humans \cite{gallese06mirror}. 
In particular it is through manipulation that infants gain direct access to objects 
and discover properties that otherwise would remain hidden. 
This concerns for example properties like weight, shape, texture and 
softness that are, if not impossible, at least extremely hard to perceive 
by using visual information alone. In adults information originating 
from motor activity and direct contact with the environment, supports 
perception \cite{klatzky87hand}; during development
the physical interaction with the environment provides infants 
with natural invariances that are useful occasions for learning.
Interestingly, motor and perceptual development seem to follow synchronous  
schedules as if new achievements in the motor system were promoting
the development of new perceptual skills \cite{bushnell93motor}.

Research in developmental robotics has demonstrated the importance of
motor activity (in particular manipulation) for visual and haptic 
perception \cite{fitzpatrick07shared}. One of the 
limitations of these approaches is that controlling the interaction between
the robot and the environment is difficult especially when precise models are
not available. Experiments with robots have thus focused on situations
in which the interaction with objects is relatively simple. For these reasons
the investigation of perceptual development in robots requires addressing 
the problem of motor development first and improving how robots interact
with the world.

In this context we focus on reaching, which is clear prerequisite for 
grasping. If vision of the hand and target were available then this problem 
would be easily solved by employing a visual servoing approach (see 
\cite{hutchinson96tutorial} for a review) which is typically based on 
the use of the visuo-motor Jacobian of the arm. The eye-to-hand Jacobian 
transformation is a function of the arm and head joints and includes knowledge 
of the camera parameters; its estimation is in practice a difficult task.
The fundamental advantage of this approach is that even an inaccurate estimation 
of the Jacobian allows reducing the error of the task to zero. But the visual 
servoing approach is not necessarily the best solution. On one hand delays in 
the control loop pose limitations on the speed of the arm, while on the other 
hand it requires that the hand and target are continuously visible for the 
duration of the movement. 
Reaching can also be performed open-loop by directly relating the joint angles of 
the head with those of the arm \cite{blackburn94learning,metta99developmental}. 
The drawback of this approach is that errors because of modeling inaccuracies and 
noise in the sensors, calculations, actuation, cannot be made arbitrarily small 
as in the visual servoing case.

Results in developmental psychology suggest that both solutions might be
adopted by the brain. Clifton et al. \cite{clifton93isvisually} 
tested whether infants require vision of their hand when reaching; they 
found that infants' ability to touch (and grasp) objects is independent 
of whether sight of the hand is available or not. On the other hand 
other experiments \cite{ashmead93visual} show that later on in development 
there is an increase of visual guidance in reaching. Together these results 
suggest the hypothesis that there are two ``distinct'' reaching mechanism: 
one that relies on ``proprioceptive'' information alone and one that uses 
``visual feedback'' to compensate for errors in the visual domain. Further,
there are studies that show the link of the control of the gaze in relation
to the precision of reaching \cite{flanders-daghestani-berthoz-1999}. 
In this paper we integrate the two modes of control with an approach based on
the hypothesis that the target object is fixated. We use the open loop
controller to bring the hand close to the target. The closed loop controller 
is activated when visual feedback from the hand is available. In practice,
we show that the error could be made arbitrarily small. The problem of 
redundancy in solved in the first case by imposing additional constraints to 
the task. Finally, we describe the procedure by which the robot learns all 
the transformations required by the controllers (the open-loop mapping and 
the arm visual Jacobian).