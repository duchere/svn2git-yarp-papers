\section{Interaction}
We describe here a grasping behavior based on the modules described in the previous sections. The interaction with the environment starts when an object is placed in the robot's hand; the robot detects the object by using the tactile sensors on the palm (see picture frame1). When pressure on the palm is detected the fingers close in a steretyped grasping action. The intrinsic elasticity of the hand (see section \ref{sec-platform}) facilitates grasping, because the fingers automatically adapt to the shape of the object. The robot starts the exploration of the object by bringing it close to the cameras in four different positions and orientations (frames 2-6). During the exploration the robot keeps fixation on the object by tracking the hand. At each position a few frames are acquired to train the model of the object. At frame 4 the exploration ends and the object is dropped on the table. The robot exploits now the visual model of the object to search for it in the visual scene. This happens by selecting the blob whose features better match those of the object's main blob and performing a saccade towards it. After the saccade the model of the object is matched against the blob that is being fixated and its surrounding. If the match is positive grasping starts (frame 7-9) otherwise search continues. The disparity map of the segmented object is computed to determine the orientation of the object; two different actions are attempted to maximize the possibility to successfully grasp the object. If the principal axis is oriented horizontally the robot moves the hand above the object, otherwise the hand approaches the object from the side (ADD FRAMES). To determine if the grasping was successful, the robot checks the weight of the object and its consistence in the hand (the shape of the fingers around the object). In case of failure another trial is attempted, otherwise the robot waits for another object.