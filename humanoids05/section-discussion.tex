\section{Discussion}
\label{sect:conclusion}
We have shown results on two phases of the acquisition of sensorimotor coordination in a upper body humanoid robot. The system includes a visual attention system employing top-down and bottom-up information. The former is introduced in the system beforehand, whereas the latter is modulated by the robot's interaction with the environment. 

We have shown the importance of the interaction between the environment and the robot for learning. This was demonstrated indirectly, when the robot exploited self-produced actions to explore its own body, and directly when the robot actively explored the visual properties of the objects it grasped.

In the experiment discussed here, we start to link different actions to different objects to investigate the possibility for the robot to autonomously learn what actions are more suitable for different contexts (different objects or environment). Although far from completed, this is meant to enlighten us about the possibility of autonomously enriching the robot's knowledge of the world. This is not only relevant for action, but also from a perceptual point of view. Indeed, actions establish a link between events and the causes that have generated them. In other words by acting in the world an ``active'' agent can link the actions it performs with their consequences. This link can be used in two ways i) for planning, to select the particular action required to bring about a desired consequence and ii) for interpretation, to understand the meaning of an attended event. In the first case, the advantage to use such a representation is that sometimes it might be convenient to express goals in perceptual terms. For example pushing an object in a particular direction can be represented by means of the resulting visual motion in the image plane \cite{fitzpatrick03learning}.  In the second situation, the only available information is the sensorial experience associated with the event. In this case the robot can search its own experience for an event that closely match what it is observing and select the action(s) that generated them. For example the sound of an object that hits the floor can be associated with the action of dropping it. Either problems, planning and interpretation, are interesting and challenging, luckily the solution to both appears to be tightly intertwined with sensorimotor development.

It is fair to say that the system developed so far, although complex, still manifests a certain degree of brittleness perhaps associated to the amount of ``handcrafted'' components we were nonetheless forced to use to reach this level of functioning in a reasonable amount of time. For instance, the choice of color blobs as features clearly limits the visual system in a way that sometimes prevent the robot from perceiving certain object characteristics. Also, some other times the residual error in reaching goes unnoticed to the robot that eventually fails to grasp the object reliably. On the object recognition side, objects composed of only a few blobs are easily mistaken for other blobs in the background since a single color (or certain blob combinations) is clearly not discriminative enough (the background might present similar combinations). 

We are aware of many of these limitations and, in fact, our ongoing work is exactly aimed to improving the overall performance of the robot both motorically and perceptually with a particular emphasis on the manipulation abilities we deem fundamental for autonomous development.




