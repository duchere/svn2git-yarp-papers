\section{The Body}
Humans become skillful at controlling their own body after a long period of development and probably thousands of trials. As we discussed in the introduction, however, motor development is extremely important and enable the correct perceptual development of the child. For this reason the robot spends the first phase of its artificial development learning how to correcly control the head and the arm to perform various tasks such as visual tracking and reaching of a target.
Control of the body requires implicit knowledge of its structure (e.g. relative position of the limbs, their size) as well as its dynamical caractheristics (e.g. the weight of the body segments). The ensamble of this knowledge is called \emph{body-schema}; experiments in neuroscience have given support to the existence of a body-schema in the primate brain (\cite{graziano99whereis,graziano00coding}). Graziano and co-workers have found neurons in the motor of monkeys which code the position of the hand in the visual field.
On the other hand, developmental psychologist have been trying to understand the mechanisms which allow the brain to acquire such representation. As roboticists we are interested in the same mechanisms as they allow the system (biological or artificial) to autonomously acquire and maintain all parameters required to the control of action and avoid manual estimation and calibration. For this reason the problem has been studied by other authors (\cite{yoshikawa03doestheinvariance,fitzpatrick04feelthebeat,metta03early}.
We follow here an approach similar to the one of \cite{fitzpatrick04feelthebeat,metta03early} where repeated self-generated actions are exploited for learning. We programmed the robot to perform a period movement of the wrist. This motion is detected by the robot visually and \emph{motorically}. In the former case we Visual motion is computed by image differencing with an adaptive model of the background. In the latter case the robot computes the first derivative of the encoder feedback. The period of motion of each pixel in the motion image is compared to that of the encoders. Pixels whose motio is periodic and whose period matches that of the joints are selected and grouped together to form the segmentation of the hand. Figure ~\ref{handsegmentation} shows an example of the result of this procedure.

\begin{figure}
\centering
\includegraphics[width=3in]{handsegmentation}
\caption{Hand segmentation: an example.}
\label{handsegmentation}
\end{figure}

We used this procedure to train a neural network to compute of the hand in the visual field given the current robot posture (arm and head joint configuration). Another neural network learns the approximate shape and orientation of the hand given the same information. Figure ~\ref{handlearning1} show the result of this process. 

\begin{figure}
\centering
\includegraphics[width=3in]{handlearning1}
\caption{Learning to localize the hand in the visual field.}
\label{handlearning1}
\end{figure}

The role of vision during reaching is still debated \cite{saunders03humans}, although experimental results suggest that sight of the hand is not required for children to start reaching for an object \cite{clifton93isvisually,clifton94multimodal} and is used only realatively late in development to actually adjust the trajectory of the hand during action \cite{ashmead93visual}. 
Sight of the hand, however, might be used to acquire eye-hand coordination. By tracking the hand the robot builds a mapping between each position of the arm and the corresponding head configuration when fixation of the head is achieved. The hypoteses is that reaching starts by first fixating the object; in this condition the fixation point coincides with the target and uniquely identifies its position with respect to the body. The arm motor command can be obtained by a transformation between the head and arm joints, that is by mapping motor variables into motor variables:
\begin{equation}q_{arm}=f(q_{head})\end{equation}