\section{Introduction}
\label{sec:intro}

\emph{Multi-modal learning}, that is, learning from sensorial patterns associated with very different kinds of sensors, is paramount for biological systems. Coupled acoustic and visual information is essential, for instance, for animals to determine whether they are facing a predator or a prey, as well as in courtship rituals. From the point of view of artificial intelligence, multi-modal learning is a potentially excellent way of enriching the input space of pattern recognition problems which could be otherwise more difficult.
Indeed, sensorial inputs are available to biological systems in an endless, inextricably mixed flow coming from various sensorial apparatuses. It is not completely clear, then, \emph{how} this information can be used to improve pattern recognition. For example, one could argue that the sight of a certain kind of predator is generally associated with a particular (set of) sound(s) and smell(s), and that animals learn to associate these multi-modal patterns during their infanthood; later on, this fact is employed in recognising the associated danger in a dramatically better way. It seems apparent, then, that there is a mapping among sensorial modalities; e.g., the auditory stimulus corresponding to a predator should be reconstructible from its visual appearance. Therefore, even though not all modalities are always available, it should be possible to recover one from another, to various degrees of precision.\\
In this work we focus upon \emph{active} perception modalities vs. \emph{passive} ones. By active modality we mean perception arising from the \emph{action} an agent performs in its environment; by passive modality, we mean perception of stimuli which are independent from the agent's will. Our paradigmatic case is grasping: objects must be grasped in the right way in order to use them as desired; but visual information only about an object is seldom sufficient to determine what kind of grasp to apply. Therefore the action of grasping must be reconstructed when the object is seen, so that the correct action can be performed.\\
In order to reconstruct actions from perception we draw inspiration from the work on \emph{mirror neurons} \cite{gallese,rizz}. Mirror neurons are clusters of neural cells which will fire if, and only if, an agent grasps an object \emph{or} sees the same object grasped by another agent; they encode the semantics of an action associated to an object, and form the basis of \emph{internal models} of actions, by which animals reconstruct the grasping and can therefore plan the grasp with greater robustness and effectiveness.
Following the path laid out, e.g., in \cite{metta,2007.AR}, where perception-action maps have been built into artificial systems, we hereby propose a theoretical framework for multi-modal learning in which an active modality is reconstructed via statistical regression from a passive modality. In the worked example, visual patterns describing the sight of an object are used to reconstruct the related grasping postures of the hand, with the hope that the use of \emph{two} modalities, one active and one passive, instead of the passive one only, will aid the recognition of the object itself. This framework can be theoretically extended to any such active-passive coupling.

The paper is organized as follows: in Section \ref{sec::framework} we present the framework, discussing motivations and implementation choices; vision issues are tackled in Section \ref{sec::vision} where we deal with objects modelling; the regression techniques used to build the perception-action map are in Section \ref{sec::regression}. In Section \ref{sec::experiments} we describe preliminary experiments that motivate the pertinence of our approach, while the last Section discusses future work.

%Biological systems use multiple modalities to achieve high robustness and cope with uncertainty: humans tipically exploit combinations of external and internal stimulus to solve a number of matters due to interactions with the 3D world.\\
%Among the manifold class of conceivable ways to gather external information, we distinguish between {\it passive} and {\it active} modalities. Passive modality occurs when external stimulus are captured by a human simply because the sense able of detecting it is ready and open. This state accounts for the acquisition of video, audio or smell information, and falls under the  broad concept of {\it perception}.\\
%The second modality, instead, requires not only the ability of undertanding an external stimulus but also the will to cope with the outside. This in turn implies the existence of a target, that we call the {\it embodiment}, to cope with, triggering different capabilities to obtain information that integrates the ones gathered by the plain perception. It is immediate to figure out that the target manipulation (that we can think of as an object) helps to learn not only its 3D shape and geometry, but also the different modalities of interaction. The sense-motor ability, therefore, assumes a key role to answer multiple interesting questions related to an embodiment, concerning the different ways to use and take it: we can view this modality as perception followed by action, that is one notices the presence of something and tries to better understand it by means of appropriate abilities.\\
%Taking inspiration from the biological view-point, multi-modality might be exploited also for artificial systems. This paper deals with the problem of modelling an artificial system equipped with a number of perceptual channels, among which some could be inactive: learning passive and active channels allows to exploit this knowledge to maintain the system running even when one is not active.
%Relative recent studies, indeed, show that by learning a pattern on multi-modal data, the system is able to benefit from the multi-modality even when only one perceptual channel is activated. Rizzolati et al. \cite{gallese,rizz} shows that mirror neurons are responsible of the knowledge captured by humans and primates when acting or observing an action, showing congruence between observed and executed action. This can thus be seen as a practical example of how visual and motor properties are phisically exploited to solve specific tasks \cite{metta}. Another example is the so-called transfer of knowledge across modalities \cite{thrun,malak,barto,caputo} that enables humans to efficiently gather and combine information coming from different channels to improve performances and efficiency even when a small amount of data is available.\\
%In this work we present a theoretical framework for multi-modal learning of visual and motor patterns accounting for [1] acquiring data from perceptive and sensorial channels, [2] modelling multi-modal data, that is turning raw input signals into data suitable for the learning context, and [3] building a proper model of interaction betwwwn the input channels. \\
%The main contribution of the paper includes suggesting a procedural pipeline for the general architecture, where each module acquires a proper role and implementation: in particular, the interaction between data is modelled by means of regression techniques able to build a mapping between them.\\
%Different instances of the architecture can be specified so that many interesting applications take shape: among them, the application we have in mind focuses on methods suitable for medical and robotic problems related to prosthetic automatisms, so that computational issues turn into a fundamental element to cope with. The main idea si to exploit the mechanisms presented so far to jointly recognize objects and grasping actions. \\
%Little work has been done in the learning community on building system for multi-modal data learning \cite{jung,kruger,yang,grans}, so that the research field is quite active and open to new solutions: our work is different since {\bf XXX  in cosa ci distinguiamo XXX}\\
%The remainder of the paper is organized as follows: Sec. \ref{sec::framework} is devoted to present the framework, discussing motivations and implementation choices; vision issues are faced in Sec. \ref{sec::vision} where we deal with objects modelling; the regression techniques used to build the mapping between different classes of data are object of Sec \ref{sec::regression}. In Sec. \ref{sec::experiments} we describe preliminary experiments that motivate the pertinence of our approach, while the last section discusses about future work.  
%
%
%
