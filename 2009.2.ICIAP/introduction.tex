\section{Introduction}
\label{sec:intro}

\emph{Multi-modal learning}, that is, learning from sensorial patterns associated with very different kinds of sensors, is paramount for biological systems. Coupled acoustic and visual information is essential, for instance, for animals to determine whether they are facing a predator or a prey, as well as in courtship rituals. From the point of view of artificial intelligence, multi-modal learning is a potentially excellent way of enriching the input space of pattern recognition problems which could be otherwise more difficult.
Indeed, sensorial inputs are available to biological systems in an endless, inextricably mixed flow coming from various sensorial apparatuses. It is not completely clear, then, \emph{how} this information can be used to improve pattern recognition. For example, one could argue that the sight of a certain kind of predator is generally associated with a particular (set of) sound(s) and smell(s), and that animals learn to associate these multi-modal patterns during their infanthood; later on, this fact is employed in recognising the associated danger in a dramatically better way. It seems apparent, then, that there is a mapping among sensorial modalities; e.g., the auditory stimulus corresponding to a predator should be reconstructible from its visual appearance. Therefore, even though not all modalities are always available, it should be possible to recover one from another, to various degrees of precision.\\
In this work we focus upon \emph{active} perception modalities vs. \emph{passive} ones. By active modality we mean perception arising from the \emph{action} an embodied agent performs in its environment; by passive modality, we mean perception of stimuli which are independent from the agent's will. 
Our paradigmatic case is grasping for an embodied agent: objects must be grasped in the right way in order to use them as desired. 
According to the so-called {\em learning by demonstrations}, that is learning a grasp by observing someone doing it, we build a mapping from the object appearance to the grasping action and assess its ability to accurately describe the grasp type.
In a multimodal setting, the estimated mapping could be used to predict the motor data when the corresponding channel is inactive. 

In order to reconstruct actions from perception we draw inspiration from the work on \emph{mirror neurons} \cite{gallese,rizz}. Mirror neurons are clusters of neural cells which will fire if, and only if, an agent grasps an object \emph{or} sees the same object grasped by another agent; they encode the semantics of an action associated to an object, and form the basis of \emph{internal models} of actions, by which animals reconstruct the grasping and can therefore plan the grasp with greater robustness and effectiveness.
Following the path laid out, e.g., in \cite{metta,2007.AR}, where perception-action maps have been built into artificial systems, we hereby propose a theoretical framework for multi-modal learning in which an active modality is reconstructed via statistical regression from a passive modality. In the worked example, visual patterns describing the sight of an object are used to reconstruct the related grasping postures of the hand, with the hope that the use of \emph{two} modalities, one active and one passive, instead of the passive one only, will aid the recognition of the object itself. This framework can be theoretically extended to any such active-passive coupling.
The paper is organized as follows: in Section \ref{sec::framework} we present the framework, discussing motivations and implementation choices; vision issues are tackled in Section \ref{sec::vision} where we deal with objects modelling; the regression techniques used to build the perception-action map are in Section \ref{sec::regression}. In Section \ref{sec::experiments} we describe preliminary experiments that motivate the pertinence of our approach, while the last Section discusses future work.


