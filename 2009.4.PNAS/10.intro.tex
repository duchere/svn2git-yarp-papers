\section{Introduction/Motivation}
\label{sec:intro}

\dropcap{A}utomatic speech recognition (ASR) is the ability of a machine
to convert human speech, coded as an audio signal, into words.
Potential applications of ASR range from human-computer interfaces
to informatics for the disabled to data mining in large speech corpora.
%Despite decades of research, state-of-the-art ASR
%systems still need to be trained upon very large and heterogeneous data sets
%to account for speech variability.
%, or upon a single speaker's speech in controlled conditions.
%And nevertheless, human beings show an excellent ability
%to understand one another's speech, independently of the speaker, the
%accent, the pitch and speed, noise, etc.
While human beings show an excellent ability to understand one another's speech,
independently of the speaker, the accent, the noise, etc., the robustness to
speech varialbility of state-of-the-art ASR systems is still an active research
topic.

Recent neuroscientific evidence indicates that the brain motor areas responsible for
producing bilabial and dental phonemes are also involved in their perception, at least
when speech is noisy. D'Ausilio et al. \cite{dausilio}
show that in a noisy discrimination task of /b/ and /p/ versus /d/
and /t/, trans-cranial magnetic stimulation of the lips and tongue \emph{motor areas}
improves the \emph{perception} of bilabials, and similarly, stimulation of the tongue
favors dentals. This suggests that motor information may be paramount for
understanding speech in humans.

Inspired by these findings, in this paper we investigate whether knowledge of speech
production in humans, integrated into an automatic phoneme classifier, can improve the
classification of /b/,/p/ versus /d/,/t/, in various conditions of noise and with different
restrictions on the training set.
To this end, we focus on the ``automatic version'' of the problem tackled in D'Ausilio et al.'s
work. For each consonant, a corresponding typical phonetic motor invariant (MI) is identified
according to the basic physiology of speech; e.g., a fast, voiced opening (plosion) of the lips
for /b/ and /p/ and of the tongue against the upper teeth for /d/ and /t/.
MIs are then used to semi-automatically segment the audio/motor data found in a
database of speech/motor trajectories recorded from $6$ subjects.

Subsequently, a simple regression method (namely, a feed-forward neural network) is employed
to build an Audio-Motor Map (AMM), which converts audio features of the isolated segment to
features of the related MI. On an abstract level, an AMM is a mathematical proxy of a mirror
structure \cite{umilta-01}, reconstructing the distal speaker's \emph{speech act} while
listening to the related piece of speech.

To test the approach, we devise three experiments involving a classifier in the form of a
Support Vector Machine \cite{BGV92}. The main question is: can the use of MI-based features,
either those recorded in the database (the ``real'' motor features) or the AMM-reconstructed
ones (a more ecological scenario), improve the classifier's performance?

\subsection{Related Work}

In the ASR community, the combination of explicit speech production knowledge and audio
features has been proposed (see, e.g., \cite{king} for a review) as an alternative to the
classic approach in which speech production variability (e.g., due to speaking rate) and
coarticulation (the phenomenon by which the phonetic realization of a phoneme is affected
by its phonemic context) are directly and implicitly modeled in the acoustic domain.
Here we restrict our investigation to the task of discriminating two bilabial from two
dental consonants, so that we can lift the above working assumptions and technical
difficulties that have so far hampered a satisfactory integration of motor information
into ASR systems.

Additionally, in previous work it is not possible to properly identify which aspects of the
recognition process benefit from motor information. For example, motor knowledge may improve
the modeling (and so the identification) of coarticulation effects that are seen in the training
data set, but not necessarily improve the recognition of phonemes in unseen contexts, i.e., it
may not necessarily improve the generalization ability of the ASR system. On the other hand the
experimental setup we have designed has the main goal of investigating whether motor information
improves the generalization ability of a phoneme classifier.

%Although the integration of speech production knowledge in an ASR system often brings some improvements, %it is commonly held that the potential of speech production knowledge is far from being exhaustively exploited.  

It is known since the Sixties \cite{liberman1} that the audio signal of speech
cannot be effectively segmented down to the level of the single phoneme,
especially as far as stop consonants such as bilabial plosives
are concerned; in particular, their representations in the audio domain are
radically different according to the phoneme which immediately follows.
It remains an open question then, how humans can distinctly perceive a common
phoneme, e.g., /b/, in both /ba/ and /bi/, since they have access to the speaker's
audio signal only. The explanation put forward by the Motor Theory of Speech Perception
(MTS, \cite{liberman2}) is that, while perceiving sounds, humans reconstruct
\emph{phonetic gestures}, the physical acts that produce the phonemes, as they were
trained since birth to associate articulatory gestures to the sounds they heard.

But even ignoring the MTS --- a very controversial theory indeed, recently reviewed and
revised \cite{galant,massaro} --- the use of speech production knowledge in speech recognition
is appealing, in that the coupling of articulatory and audio streams allows for explicit models
of the effects of speech production phenomena (e.g., coarticulation) on the acoustic domain.
In general, these effects cannot be precisely modeled (e.g., when exactly does /a/ affect the phonetic
realization of /b/ in /ba/?) or cannot even be modeled at all (e.g., what happens, in the acoustic domain,
when /o/ is uttered with an exaggeratedly open jaw?) when the phonetic stream is directly mapped
onto the acoustic dimension as in the standard approach to ASR.  

Different solutions have been proposed to integrate speech production knowledge into an ASR system
and different types of speech production information have been used, ranging from articulatory
measurements (see \cite{zlokarnik,stephenson,wrench}, for example) to symbolic non-measured
representations of articulatory gestures that "replicate" a (symbolic) phoneme into all its
possible articulatory configurations
%\footnote{Articulatory configurations are configurations of
%the positions of the phonetic articulators.}
(see \cite{richardson, livescu}, for example).
   
%One possible reason why ASR is so difficult is then that
%machines have in general no access to the motor representation of the
%audio signal they are supposed to understand. We hypothesize that motor 
%information might help ASR, especially when tests on different speakers and different
%coarticulations are performed: for example, when training on subject $A$ and
%testing on subject $B$, or when training on pseudo-words such as /ba/, /bi/,
%/be/ and then testing for the presence of /b/ in /bo/, /bu/ or even /br/.

Although some studies have shown increased word recognition accuracy when including speech
production knowledge in ASR, it is commonly held that the potential of speech production
knowledge is far from being exhaustively exploited. Limits of current approaches include, e.g.,
the use of the phoneme as a basic unit (as opposed to articulatory configuration, for example)
which appears to be too coarse especially in the context of spontaneous spoken speech,
and the lack of a mechanism accounting for the different importance of articulators in the
realization of a given phoneme (e.g., in the production of bilabials the lips are critical whereas
the tongue is not).

As well, the traditional approach in which the speech signal is segmented into equal-length signal
pieces (the "beads on a string" approach \cite{ostendorf}) poses a number of problems to an accurate modeling of
spontaneous speech, in which coarticulation phenomena such as phone deletion or assimilation,
distorting the acoustic appearance of phonemes, are frequent and not always predictable. These problems
call for finer-grained basic units \cite{ostendorf}. To partly make-up for such a limitation we propose
an alternative approach where the audio signal is segmented using phone-specific articulatory patterns,
expectedly more distinctive and stable than acoustic features.

%Concerning the necessity of a phoneme dependent distinction between critical and non-critical articulators we do not 
%Traditionally (e.g., \cite{bourl,salvi}), the audio speech signal is segmented with a
%fixed-length Hamming window, usually 20ms. long. The resulting sequence
%is then analysed in the frequency or cepstral domain and the
%resulting coefficients are used as features for a classification system.
%One negative aspect of this approach is that it
%neglects the qualitative overall characteristics of the
%phoneme being uttered: depending on the speed of the speech, a consonant
%can have different lengths and, by using the above approach, global
%information about it is lost (see \cite{ostendorf}, where this approach is
%dubbed ``beads-on-a-string''). Nevertheless, as far as we know, there is
%so far no widely accepted alternative method for speech segmentation,
%if the audio signal is the only one available. One attempt, but not based
%upon articulatory data atl all, appears in \cite{bourlard}.

During recognition, articulatory gestures have to be recovered 
from audio information as audio is the only signal available.
Reconstruction of articulatory features has been attempted since a long
time, but in most cases it is not derived from articulatory \emph{data}
gathered from human subjects. One pioneering case is that of Papcun
et al. \cite{papcun} where the AMM is carried out by a Multilayer Perceptron.
Our procedure for building the AMM is deeply inspired by this work.
%By using a Multilayer Perceptron we implicitly assume that all articulators have the same importance and that the AMM is a %one-to-one mapping.
%Papcun et al. \cite{papcun} observed that non-critical articulators have higher variance (in terms of position) than critical %articulators.
The Multilayer Perceptron attempts the best recovery of all articulators giving equal importance
to all of them; this could be, in general, problematic, since non-critical articulators will have
high variance during the utterance of unrelated consonants \cite{papcun,rose}.
For example, the tongue position is expected to exhibit high variance while, e.g., velar plosives
such as /k/ and /g/ are uttered. This is the main reason why an AMM is in general a one-to-many mapping: different articulatory configurations result in the same acoustic realization. Solutions to properly address the ill-posedness
of the AMM have been proposed by Richmond et al. \cite{richmond} and Toda et al. \cite{toda}; here we do
not address the issue directly; rather, we consider two articulators only, hopefully alleviating the problem.

%and subsequently Korin Richmond's work
%\cite{richmond2002,richmond2007} who have been able to reconstruct point-by-point
%the trajectories of articulators from the audio signal to a remarkably low
%error rate. The procedure for building the AMM is deeply inspired by their
%work.

Interestingly, the idea of using information about the mechanisms involved in the production of a human action to improve its classification/recognition (in a domain different from the production domain) has not only been applied in the context of speech recognition. For example Metta et al. \cite{metta-06} and Hinton \cite{hinton-2006} 
have shown that articulatory data can improve classification accuracy in automated hand action classification.

%TODO  
% Transferring the method to speech perception seems
% like a natural choice.
