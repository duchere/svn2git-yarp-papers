\section{Introduction/Motivation}
\label{sec:intro}

\dropcap{A}utomated speech recognition (ASR) is the ability of a machine
to convert human speech, coded as an audio signal, into words.
Potential applications of ASR range from human-computer interfaces
to informatics for the disabled to data mining in large speech corpuses.
Despite decades of research, state-of-the-art ASR
systems still need to be trained upon very large and heterogeneous data sets, or upon
a single speaker's speech in controlled conditions.
And nevertheless, human beings show an excellent ability
of understanding one another's speech, independently of the speaker, the
accent, the pitch and speed, noise, etc.

Recent neuroscientific
evidence indicates that the brain motor areas responsible for producing labial
and dental phonemes are also involved in their perception; D'Ausilio et al. \cite{dausilio}
show that in a discrimination task of /b/,/p/,/d/ and /t/, trans-cranial magnetic
stimulation of the lips and tongue \emph{motor areas} creates a bias in favour
of the \emph{perception} of labials, and similarly, stimulation of the tongue
favours dentals. This suggests that motor information may be paramount for
understanding speech in humans.

Inspired by this finding, we investigate whether knowledge about the \emph{production}
of speech in humans, encoded mathematically, would help \emph{recognition}
of speech by a machine. To this end, we have focussed on the automatic version of
the problem tackled in D'Ausilio et al.'s work. For each consonant,
a corresponding typical phonetic motor invariant (MI) was
identified according to basic physiology of speech;
e.g., a a fast, voiced opening (plosion) of the lips for /b/, and so on.
MIs were then used to semi-automatically segment the audio/motor data found in a
database of speech/motor trajectories recorded from $6$ subjects.

Subsequently, a simple regression method (namely, a feed-forward neural network) was employed
to build an Audio-Motor Map (AMM), which converts audio features of the isolated segment to
features of the related MI. On an abstract level, an AMM is a mathematical proxy of a mirror
structure \cite{umilta-01}, reconstructing the distal speaker's speech production act while
listening to the related piece of speech.

To test the approach, we have devised three experiments involving a 
classifier in the form of a Support Vector Machine \cite{BGV92}. We wanted to check whether
the use of MI-based features, either those recorded in the database (the ``real''
motor features) or the AMM-reconstructed ones (a more ecological scenario),
could improve the classifier's performance. Our results show that this is the case,
especially when the classifier is trained on incomplete data sets such as 
per-speaker (e.g., training on speakers $1,2,3$ and testing on $4$) and
per-coarticulation\footnote{Coarticulation is the phenomenon by which a consonant's audio representation
looks different depending of the vowel immediately following it.}
(e.g., training on /ba/, /be/, /bi/, /bo/ and testing on /bu/); or when noise is added,
in which case motor features significantly help classification, even when added to a
state-of-the-art set of audio features about $20$ times larger than that extracted
from the MIs.

\subsection{Related Work}

It is known since the Sixties \cite{liberman1} that the audio signal of speech
cannot be effectively segmented down to the level of the single phoneme,
especially as far as stop consonants such as bilabial plosives
are concerned; in particular, their representations in the audio domain are
radically different according to the phoneme which immediately follows
(\emph{coarticulation}). It remains an open question then, how humans can
distinctly perceive a common phoneme in, e.g., /ba/ and /bi/, since they
apparenty have access to the speaker's audio signal only.

The explanation put forward by the so-called motor theory of speech perception
(MTS, \cite{liberman2,galant,massaro}) is that, while perceiving sounds,
humans reconstruct \emph{phonetic gestures}, the physical acts of
producing the phonemes, as they were trained since birth to associate
articulatory gestures to the sounds they heard. 

One possible reason why ASR is so difficult is then that
machines have in general no access to the motor representation of the
audio signal they are supposed to understand. We hypothesize that motor 
information might help ASR, especially when tests on different speakers and different
coarticulations are performed: for example, when training on subject $A$ and
testing on subject $B$, or when training on pseudo-words such as /ba/, /bi/,
/be/ and then testing for the presence of /b/ in /bo/, /bu/ or even /br/.

Traditionally (e.g., \cite{bourl,salvi}), the audio speech signal is segmented with a
fixed-length Hamming window, usually 20ms. long. The resulting sequence
is then analysed in the frequency or cepstral domain and the
resulting coefficients are used as features for a classification system.
One negative aspect of this approach is that it
neglects the qualitative overall characteristics of the
phoneme being uttered: depending on the speed of the speech, a consonant
can have different lengths and, by using the above approach, global
information about it is lost (see \cite{ostendorf}, where this approach is
dubbed ``beads-on-a-string''). Nevertheless, as far as we know, there is
so far no widely accepted alternative method for speech segmentation,
if the audio signal is the only one available. One attempt, but not based
upon articulatory data al all, appears in \cite{bourlard}.

Reconstruction of articulatory features has been attempted since a long
time, but in most cases it is not derived from articulatory \emph{data}
gathered from human subjects. One pioneeristic case is that of Papcun
et al. \cite{papcun} and subsequently Korin Richmond's work
\cite{richmond2002,richmond2007} who have been able to reconstruct point-by-point
the trajectories of articulators from the audio signal to a remarkably low
error rate. The procedure for building the AMM is deeply inspired by their
work.

On the other hand, the work of Metta et al. \cite{metta-06} and Hinton \cite{hinton-2006} 
has shown that articulatory data can improve classification performance of 
hand actions by a computer. Transferring the method to speech perception seems
like a natural choice.
