\section{Introduction}
\label{sec:intro}

\dropcap{S}peech recognition is the ability for a machine to convert
human speech, coded as an audio signal, into words and commands. The
applications of speech recognition range from human-computer interfaces,
such as the possibility of dictating to a word processor, to informatics
for the disabled, to data mining in large speech corpuses. Despite decades
of research in the field though, state-of-the-art Automated Speech Recognition
(ASR) systems need to be trained upon very large corpuses of data, or upon
a single speaker's speech in controlled conditions of noise, pitch and speed.
Moreover, most ASRs employ a pre-existing vocabulary, which strongly
limits their flexibility.

Inspired by the motor theory of speech perception (MTS, \cite{...}) and by
recent neurological research on speech perception and production in humans, we
hereby investigate the use of \emph{phonetic motor invariants} to overcome
the above limitations of ASRs. A phonetic motor invariant is a recurring
pattern in the motion of phonetic articulators (tongue, lips, teeth, etc.)
associated to a phoneme; for example, bilabial plosive consonants such as
/b/ and /p/ are naturally associated to a fast opening of the lips (a plosion,
that is). It is known since the Sixties \cite{...} that the audio signal of speech
cannot be effectively segmented down to the level of the single phoneme,
especially as far as stop consonants such as bilabial plosives
are concerned; in particular, their representations in the audio domain are
radically different according to the phoneme which immediately follows
(\emph{coarticulation}). It remains an open question then, how humans can
distinctly perceive a common phoneme in, e.g., /ba/ and /bi/, since they
apparenty have access to the speaker's audio signal only.

The explanation put forward by the MTS is that, while perceiving sounds,
humans reconstruct \emph{phonetic gestures}, the physical acts of
producing the phonemes, as they were trained since their birth to associate
them to the sounds they heard. Recent neuroscientific evidence actually
indicates that the brain motor areas responsible
for producing a certain phoneme are inextricably involved in its
perception; D'Ausilio et al., for instance \cite{dausilio}, show that
in a discrimination task of /b/,/p/,/d/ and /t/, TMS of the lips and tongue
motor areas creates a bias in favour of the related consonants. This further
suggests that motor information is paramount for understanding speech in humans.

One possible reason why ASR is so difficult is then that
machines have in general no access to the motor representation of the
audio signal they are supposed to understand. So, motor information should
help ASR, especially when tests on different speakers and different
coarticulations are performed: for example, when training on subject $A$ and
testing on subject $B$, or when training on pseudo-words such as /ba/, /bi/,
/be/ and then testing for the presence of /b/ in /bo/, /bu/ or even /br/.

In this work, we use phonetic motor invariants as mathematical representations
of phonetic gestures and show that, in a simple problem of classification of
four stop consonants /b/, /p/, /d/, /t/ extracted from a dataset of
multi-speaker audio and motor data, they improve dramatically the classification
results, making the system essentially speaker- and coarticulation-independent.
This, despite the small data set (less than $1200$ samples).

First of all, for each consonant we manually identify a motor invariant by
looking at the synchronised signals. The invariants found this way are in
agreement with basic physiology textbooks and with everyday experience,
e.g., /b/ and /p/ are associated to a plosion of the lips. Motor invariants
are then used to semi-automatically segment the audio/motor data and extract
a variable length data sequence for each utterance of each consonant.

We then show that a standard machine learning method, employed to distinguish
(classify) the consonants across speakers and coarticulations, obtains
dramatically better results if features extracted from motor invariants are
available. Three classification experiments are performed: on the whole data set,
per-speaker (training on some speakers and testing on others) and per-coarticulation
(training on some coarticulating phonemes and testing on others). Our results
indicate that on the whole data set motor features are as good as audio features, and that
using them both further improves the classification results. Per-speaker, this gap
is more evident, audio features alone being less and less effective as we make the
problem harder. Per-coarticulation, lastly, audio features become virtually
unusable, while motor features still obtain good results. Interestingly,
using both kinds of features in the per-coarticulation tests makes the
classification \emph{worse} than when using motor features only.
This result agrees with the MTS claim that coarticulation makes
the speech audio signal unsuited for identifying phonemes, and that phonetic
gestures are the true representatives of them.

Lastly, we train another machine learning system on the same data, this time
to obtain a map from audio features to motor features,
in order to check whether motor invariants can be reconstructed
from the audio signal alone. The result is uniformly positive across the three
kinds of analyses, and if we use these reconstructed motor invariants for
classification, we obtain qualitativley similar results, although, as expected,
quantitatively worse. This result seems, in turn, to support the claim
\cite{...} that humans reconstruct distal phonetic gestures when perceiving
speech.
