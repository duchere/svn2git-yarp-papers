% Using motor invariants in automatic phoneme discrimination
% Castellini, Metta, Fadiga, Badino
% submitted to PNAS

\documentclass{pnastwo}

% ---------------------------------- bookkeeping

% remove for submission
\usepackage[dvips]{graphicx}

%\usepackage{pnastwoF}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}

\newcommand{\lio}{\textsf{lio}}
\newcommand{\ttu}{\textsf{ttu}}
\newcommand{\vlio}{\textsf{vlio}}
\newcommand{\vttu}{\textsf{vttu}}
\newcommand{\alio}{\textsf{alio}}
\newcommand{\attu}{\textsf{attu}}

\newcommand{\overall}{\emph{overall}}
\newcommand{\spka}{\emph{spk5vs1}}
\newcommand{\spkb}{\emph{spk3vs3}}
\newcommand{\spkc}{\emph{spk1vs5}}
\newcommand{\coa}{\emph{coart4vs1}}
\newcommand{\cob}{\emph{coart3vs2}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Don't type in anything in the following section:
%%%%%%%%%%%%
%% For PNAS Only:
\contributor{Submitted to Proceedings
of the National Academy of Sciences of the United States of America}
\url{www.pnas.org/cgi/doi/10.1073/pnas.0709640104}
\copyrightyear{2008}
\issuedate{Issue Date}
\volume{Volume}
\issuenumber{Issue Number}
%%%%%%%%%%%%

\begin{document}

% ---------------------------------- frontmatter

\title{Using phonetic motor invariants\\
improves automatic phoneme discrimination}

\author{
Claudio Castellini\affil{1}{German Aerospace Research Center, Oberpfaffenhofen, Germany},
Giorgio Metta\affil{2}{Italian Institute of Technology, Genova, Italy},
Luciano Fadiga\affil{3}{DSBTA, University of Ferrara, Italy} and
Leonardo Badino\affil{2}{}
}

\contributor{Submitted to Proceedings of the National Academy of Sciences
of the United States of America}

\maketitle

\begin{article}

\begin{abstract}

  We hereby investigate the use of phonetic motor invariants (MIs),
  that is, recurring kinematic patterns of the human phonetic articulators,
  to improve automatic phoneme discrimination. Using a multi-subject
  database of synchronized speech and lips/tongue trajectories, we first
  identify MIs commonly associated with bilabial and dental consonants,
  and use them to simultaneously segment speech and motor signals.
  We then build a simple neural network-based regression schema (an Audio-Motor
  Map, AMM) mapping audio features of these segments to the corresponding
  MIs. Experiments show that $(a)$ in ideal (noise-free, multi-subject training set)
  conditions, automatic phoneme discrimination based upon AMM-reconstructed
  MIs is as effective as that based upon standard audio features
  only, and that a simple combination of audio and MI features significantly
  improves the discrimination performance; $(b)$ this gap widens dramatically
  as we restrict the training set on a per-speaker and/or per-coarticulation basis
  (where reconstructed MIs are by far more effective than
  audio features);  and $(c)$ as noise is added the gap persists. These results 
  support some of the claims of the motor theory of speech perception and add 
  experimental evidence of the actual utility of MIs in the more general framework of automated speech recognition.
  
% even if we compare against
%  a state-of-the-art audio feature set which is about $20$ times larger than
 % that extracted from MIs. These results encourage the use of MIs in the more
 % general framework of automated speech recognition. Moreover, our findings 
 % support some of the claims of the motor theory of speech perception.

\end{abstract}

\keywords{motor theory of speech | machine learning | phoneme discrimination |speech recognition}

% ---------------------------------- the paper

\input{10.intro}
\input{20.dataset}
\input{30.rec}
\input{40.class}
\input{60.disc}
\input{70.concl}

\begin{acknowledgments}
  This work is supported by the EU-funded project Contact (NEST 5010) and Poeticon (ICT-FP7-215843).
\end{acknowledgments}

% ---------------------------------- refs

% for the submission: comment out the line below, and then include the .bbl
\bibliographystyle{plain} \bibliography{paper}
%\begin{thebibliography}
%...
%\end{thebibliography}

\end{article}

% ---------------------------------- figures & tables

\begin{figure*}[t]
  \centerline{\includegraphics[width=\textwidth]{figs/figSamples}}
  \caption{the speech signal and motor trajectories of lips opening
    velocity (\vlio) and acceleration (\alio) during utterances containing /b/.
    Left to right: /ba/, subject $5$; /ba/, subject $2$; and /bufalo/, subject $5$.
    The gray zone denotes the detected start and ending of the plosion. All signals
    are normalised over the indicated time frame, for visualisation purposes.}
  \label{fig:isdView}
\end{figure*}

\begin{figure*}[t]
  \centerline{\includegraphics[width=\textwidth]{figs/figAMM}}
  \caption{Quantitative performance of the AMM. For each CV schema and output signal
    we report the NRMSE average value (histogram coloured bars) and standard error of
    the mean (thin error bars). NRMSE is the standard root-mean-square-error, normalised
    as a fraction of the range of each signal.}
  \label{fig:amm_perf}
\end{figure*}

\begin{figure*}[t]
  \centerline{\includegraphics[width=\textwidth]{figs/figAccento_}}
  \caption{Real and AMM-reconstructed MIs for \vlio\ and \vttu\ while uttering
    the /t/ in \emph{accento?} (accent). Notice the apparent gap in the quality
    of the reconstruction, favouring the labiodental trajectory (\vttu).}
  \label{fig:example}
\end{figure*}

\begin{figure*}[t]
  \centerline{\includegraphics[width=\textwidth]{figs/figDD1Vel}}
  \caption{Double dissociation of correlation between real and AMM-reconstructed MI
    (mean and standard error of the mean). Mean coefficients are significantly
    higher for \vlio\ when ``listening'' to labials than dentals ($0.8466 \pm 0.0158$
    versus $0.5196 \pm 0.0281$ with Student's t-test $p<0.01$) and vice-versa
    for \vttu\ ($0.3731 \pm 0.0296$ versus $0.9528 \pm 0.0079$, $p<0.01$). The \overall\ CV
    schema is used.}
  \label{fig:DD}
\end{figure*}

\begin{figure*}[t]
  \centerline{\includegraphics[width=\textwidth]{figs/figClass1}}
  \caption{Classification accuracy (as the percentage of correctly guessed
    /b/, /p/, /d/ and /t/ with respect to the total number of samples) obtained
    in the \overall\ CV schema, using audio features, ``real'' motor features,
    motor features reconstructed by the AMM, and by joining the audio and
    reconstructed-motor models.}
  \label{fig:class1_perf}
\end{figure*}

\begin{figure*}[t]
  \centerline{\includegraphics[width=\textwidth]{figs/figClass2}}
  \caption{Classification accuracy obtained for each CV schema, using the
    four sets of features of Figure \ref{fig:class1_perf}. In the \coa\ and
    \cob\ schemas, the joint features performance is not shown since it is
    worse than that obtained by the reconstructed motor features alone.}
  \label{fig:class2_perf}
\end{figure*}

\begin{figure*}[t]
  \centerline{\includegraphics[width=\textwidth]{figs/figClass3}}
  \caption{Classification accuracy obtained for the \overall\ CV schema
    as noise is added, using the sets of features of Figure \ref{fig:class1_perf}
    but where the $20$-dimensional audio features have been replaced with a
    $351$-dimensional cepstrogram. For the cepstra2KHz and joint features,
    errorbars denoting one standard error of the mean are shown.}
  \label{fig:class3_perf}
\end{figure*}

\end{document}
