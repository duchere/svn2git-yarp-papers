\section{Discussion}
\label{sec:disc}

The experimental results presented in the previous Section clearly prove that,
at least in the case here examined, and with the techniques here exploited,
the answer to the question posed in the Introduction, ``can the use of MI-based
features improve a phoneme classifier's performance?'' is positive.

Overall, $16$ features extracted from motor invariants detected with an articulograph
(what we have called real motor features) exhibit dramatically better error rates
than $390$ state-of-the-art audio features in an automated discrimination task between
two bilabial and two dental consonants. Since the discrimination is performed using an
absolutely standard classifier (and, according to the literature, a good one), that is,
a Support Vector Machine with Gaussian kernel whose hyperparameters are found via
grid-search, this result should have a somehow general validity.
The gap is apparent in all our experiments, and is validated using Student's
t-test all over. It increases as the training sets are restricted, for example
when per-subject (i.e., training on some subjects, testing on the others) or
per-coarticulation (i.e., training on some coarticulating phonemes and testing on
others) tests are conducted. This clearly indicates that MI-based features are
somehow ``more invariant'' than audio-based ones across subjects and regardless
of coarticulation.

Moreover, an Audio-Motor-Map, this one too built using a standard machine learning
method (namely, a feed-forward neural network), is able to reconstruct the MIs
to such a degree of precision that the same features as above, extracted from these
reconstructed trajectories, exhibit comparable error rates as those found with the
audio features when the training sets are restricted (Experiments 1 and 2); and they
boost a largely \emph{better} performance then the audio ones, when noise is added
to the audio signal. This indicates that these features, even when the audio signal
is the only source of information available (a realistic scenario) can improve the
discrimination of phonemes.

Lastly, when audio and reconstructed motor features are joint
using a simple probabilistic schema, the error rates are sometimes better than when
the feature sets are used independently. In some cases a statistically significant
performance increase is noticed (see Experiment 2, CV schemas \spkb\ and \spkc)
even though no clear advantage is seen when using either the audio or the
reconstructed motor features alone. This means that the MI-based models are
correctly classifying some consonants that the audio-based models misclassify,
and that the classification confidence of the MI-based models is higher; and
also that the dual is true. Sometimes the audio features help, sometimes the MI-based
features do.




Experiments
1 and 2 show that 

The experiments presented in this paper were inspired by the intuition that the
proficiency of humans in speech recognition is grounded in the interaction
between production and understanding of speech in the human brain. Alvin
Liberman's motor theory of speech perception, although controversial and
recently reviewed and revised \cite{liberman1,liberman2,galant,massaro},
provides a theoretical framework to this intuition, which recent neurological
evidence \cite{dausilio} supports even further.

Assuming this idea as a working hypothesis, we have first tried to identify
a new, biologically inspired, way to segment the audio signal.
 It is nowadays widely accepted that the traditional segmentation, carried out 
by exclusively looking at the acoustic properties of phonemes, has strong limitations
mainly due to intra-speaker speech variability and to coarticulation.  
%since the same phoneme has different audio characteristics
%depending on the speaker, the voice pitch / speed / accent, and due to the
%phenomenon of coarticulation. 
But physiology tells us that there actually \emph{is} an invariant
over all possible /b/s, for instance, and that is the act that produced it,
that is, a labial plosion. Therefore we propose to segment the audio signal
using the articulators' trajectories to detect
the beginning and end of phonemes. The choice of the articulators and the
conditions on the trajectories are established according to basic rules of
the phonetic production; for example, a /b/ will be identifed from
the beginning to the end of an associated labial plosion. This is possible,
in our case, thanks to a database of synchronised speech / articulographic
data gathered from several human subjects.

With respect to the traditional speech segmentation, 
this approach focuses on the \emph{act} that produced the sound. If it is true that speech acts are invariant across
subjects and words, then it should be possible to extract directly from the motor data
features that are more robust to changes in noise, speakers,
coarticulation, etc., than standard
audio features. We use to this end the coefficients of a cubic fit of the
motor trajectories, so to obtain a qualitative representation of the speech
act.

% Giorgio: the cubic fit is not mentioned earlier in the paper?
% Claudio: Section "Phoneme Discrimination", subsection 1, third paragraph

It is clearly shown by our results (Figures \ref{fig:class1_perf}
and \ref{fig:class2_perf}) that these features perform
better than a set of $20$ mel-cepstral coefficients evaluated across each utterance,
while discriminating /b/, /p/, /d/ or /t/ in the database. This gap is even
more evident if examined from a per-speaker or per-coarticulation perspective.
In these tasks, the audio features show a dramatically decreasing discrimination
ability, which is not the case with the motor features. In short, motor features
are ``more invariant'' than audio features across speakers and coarticulations.
This agrees with the motor theory of speech perception.

Now, speech acts are not available in real life --- according to the mirror
paradigm, human beings reconstruct them from the audio signal, after a long
training which begins in infanthood and probably never ends. The AMM is a
representative of this mechanism. Its numerical evaluation
reveals that the normalised root-mean-square-error obtained is rather
uniform across the reconstructed signals, and is at worst $15.55\%$. These
results are coherent with those presented in \cite{papcun,richmond2002,richmond2007}.

%Leonardo: I removed the paragraph below (but it could be just modified though) as results are just a consequence of the %already known difference of variability between critical and non-critical articulators

%But what is more interesting is that the AMM reconstructs labial trajectories
%better than dental trajectories when presented with labials and viceversa,
%and dually for tongue/dental trajectories and dentals, giving rise to a double
%dissociation. A simple explanation of this fact is that human beings
%(upon whose data the AMM is trained) identify phonetic actuators which produced
%the sound they hear, and neglect the others.


The AMM is used to reconstruct the speech acts from the audio signal; these
reconstructed motor trajectories are then used as input to the phoneme
discrimination classifiers used in the previous experiments. The results
(consider Figures \ref{fig:class1_perf} and \ref{fig:class2_perf} again)
obviously show that they attain lower performances than the features
extracted from the real motor signals, but actually, comparable
to the audio features; and that if we join the audio classification model
to the one obtained for these features (with a trivial probabilistic calculation)
we again obtain a high performance, superior to that of audio features alone
and comparable to that of real motor features. Perhaps the most striking result 
is the one observed in the per-coarticulation setting where the reconstructed motor
signals alone largely outperforms the joint feature set. 


AMM-reconstructed audio features also help in the presence of noise. This time
we evaluate the performance of a $200$ milliseconds wide Mel cepstrogram (a total
of $351$ numbers, each one coding \emph{one} phoneme) as noise is added to the
speech signal; the performance degrades steadily. But again, if we join this
model with one obtained by running the AMM on the noisy speech, we uniformly
and significantly increase the performance (Figure \ref{fig:class3_perf}). The
gap seems to widen as more and more noise is added, although this claim would
require a deeper analysis.

Of course, AMM-reconstructed motor features do not carry more information
(in the sense defined by Shannon) than what is in the speech signal; therefore,
from the point of view of machine learning / speech recognition, one can see
this technique as ``just'' a better way of extracting invariant features from
speech.
