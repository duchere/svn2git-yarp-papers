\section{The Audio-Motor-Map}
\label{sec:rec}

\subsection{Training the AMM}
\label{subsec:amm_setup}

The procedure for building the AMM closely follows that outlined in
\cite{papcun,richmond2002,richmond2007} for a multi-layer perceptron neural network.
For each of the $1157$ audio sequences, the spectrogram is evaluated
over time windows (slices) of $20$ milliseconds, using a $20$ filter
Mel-scale filterbank between $20$Hz and $2$KHz. Each slice overlaps by $10$ milliseconds with
the preceding slice. Each single sample of the velocity and acceleration
of \lio\ and \ttu\ (from now on, in turn, \vlio, \alio, \vttu\ and \attu) is
then associated to $19$ surrounding spectrogram slices, covering
about $200$ milliseconds of speech and centered around the sample itself. With this
"sliding spectrogram window" method, the four trajectories are completely reconstructed.
%Figure \ref{fig:amm} graphically represents the procedure.

About $15000$ samples are extracted from the original $1157$
audio/motor sequences; each input sample consists of $19\cdot 20 = 380$ real
numbers, while the output space is given by the $4$ trajectory points of
the motor signals. A feed-forward neural network is set up in order to
build the AMM, with $380$ input units, one hidden layer with $15$ units and
$4$ output units; the net is trained via the Scaled Conjugate Gradient
Descent method \cite{MOLLER93} and the activation is a logistic sigmoidal function.

Training is done via early stopping on the appropriate validation set (see the previous
Section for the details about CV). This procedure is repeated over $10$ random restarts, and then
the network with best average performance over the $4$ output dimensions is stored.
%We have manually verified that all training sessions end by validation stopping
%(as opposed to bumping into the maximum number of training epochs, which we set
%for safety at a large value, $200$).
The performance measure is Matlab's embedded mean-square-error with regularisation
function, in which after some initial experiments we set the regularisation
parameter at $0.714$. This value, as well as all other parameters, have been found in
an initial experimentation phase, by slightly altering values suggested in literature
and/or in the Matlab manual.

Samples are normalised by subtracting the mean values
and dividing by the standard deviations, dimension-wise; targets are
normalised in order to lie within the range $[0.25,0.75]$, since the
logistic activation function has asymptotic values of $0$ and $1$.

\subsection{Evaluating the AMM}
\label{subsec:amm_results}

Figure \ref{fig:amm_perf} shows a quantitative assessment of the performance
of the AMM. The NRMSE (Normalized Root Mean Square Error) ranges from $15.55\% \pm 1.11\%$ (\vlio, \coa ---
RMSE is normalised against the range of the signals considered) to
$8.07\% \pm 0.5\%$ (\vttu, \spkc). Regression upon \vlio\ shows the largest
errors, over all CV schemas.

Although these figures do not really indicate whether AMM-reconstructed MIs will be
effective in phoneme discrimination, they prove that the error rate in rergession has
limited magnitude and does not differ dramatically across CV schemas and output signals.
Qualitative inspection of the results (one example is given in Figure \ref{fig:example})
is encouraging, showing that the AMM-reconstructed motor signals are on average rather
similar to the real ones.

As a further test of reliability, we checked how good the performance is with respect to the "criticality"
of the articulators. For each utterance,
the correlation coefficient between AMM-reconstructed and real MIs has been collected
according to whether labials (/b/,/p/) or dentals (/d/,/t/) had been presented as input
to the AMM.

As one can see in Figure \ref{fig:DD}, when the \overall\ CV schema is used,
there is a noticeable ``double dissociation''
pattern arising from the comparison of the correlation coefficients when the AMM
reconstructs \vlio\ and \vttu\ from labials or dentals. This pattern repeats to an
almost uniform extent when the other CV schemas are used confirming that the behaviour
of the non-critical articulators is more variable and less predictable.

% Giorgio: should we speculate about the meaning of this for control of articulation?
% minimum variance principle?
%
