\section{Reaching}
\label{sec:reaching}

We describe two approaches to solve the reaching task. A first method 
uses the forward mapping between the arm joint space and the position 
of the hand represented in the head reference frame $\begin{bmatrix} 
\theta_y & \theta_p & \alpha_v^d\end{bmatrix}^\top \in \mathbb R^3$. 
The second method controls the speeds of the arm so to minimize the position 
of the hand in the image plane with respect to the target (usually the 
center of the image plane).

\subsection{Open Loop Reaching}
Suppose the robot is tracking a target using the controlled described in 
\ref{Sec:TrackerController}. In the further assumption of perfect tracking 
(the visual error is zero), the spatial position of the target
in the world can be represented by means of the head joints:
%
\begin{eqnarray*}
\tilde {\mathbf x}_{target}= \mathbf q_{head} =
\begin{bmatrix} \theta_y & \theta_p & \theta_r & \alpha_v^d & \alpha_v^c & \alpha_t^c \end{bmatrix}^\top \in \mathbb R^6.
\end{eqnarray*}
%
Not all components of $\mathbf q_{head}$ are actually needed. During tracking 
$\theta_r$ is maintained stationary to 0, while the head controller 
poses additional contraints on the head joints; in particular we know from section 
\ref{Sec:TrackerController} that the controller minimizes $\alpha_t^c$ and
$\alpha^c_v$ (see equation \ref{Eq:HeadEyeControl}) so that they asymptotically
converge to $\alpha_t^c \rightarrow 0$ and $\alpha_v^c \rightarrow 0$. 

After convergence has been achieved we have:
%
\begin{eqnarray*}
\tilde {\mathbf x}_{target}=
\begin{bmatrix} \theta_y & \theta_p & 0 & \alpha_v^d & 0 & 0 \end{bmatrix}^\top \in \mathbb R^6.
\end{eqnarray*}
%
we define:
%
\begin{eqnarray*}
\mathbf x_{target}=
\begin{bmatrix} \theta_y & \theta_p & \alpha_v^d\end{bmatrix}^\top \in \mathbb R^3.
\end{eqnarray*}
%
which, in our case, uniquely codes the position of the target. This representation is very similar 
to a three dimensional polar representation in which $\theta_y$ and $\theta_p$ code respectively 
azimuth and elevation, while distance is substituted with $\alpha_v$ (\emph{vergence} angle). 

If the robot is tracking the hand, the same subset of the head joint space can be used to code the spatial location of the hand:
%
\begin{eqnarray*}
\mathbf x_{hand}=
\begin{bmatrix} \theta_y & \theta_p & \alpha_v^d\end{bmatrix}^\top \in \mathbb R^3.
\end{eqnarray*}
%
Under these assumptions we can train a neural network to approximate the forward mapping between the arm joint space $\mathbf q_{arm}$ and the position of the hand $\mathbf x_{hand}$:
%
\begin{equation} 
\label{Eq:forward}
\mathbf x_{hand}=f_{arm}(\mathbf q_{arm}), \qquad f_{arm} : \mathbb R^3 \longrightarrow \mathbb R^4.\end{equation}
%
Suppose now that we want to control the robot to reach for a target it is currently 
fixated. Formally the problem can be formulated as determining teh value of $\mathbf q_{arm}$ 
which solves the following minimization:
%
\begin{equation} 
\label{Eq:reaching1}
  \displaystyle\min_{\mathbf q_{arm}}\left(J\right)=\displaystyle\min_{\mathbf q_{arm}}
  \left\|\mathbf x_{hand} - \mathbf x_{target}\right\|^2
\end{equation}
%
where $\mathbf x_{target}$ is measured from the encoders of the head, while 
$\mathbf x_{arm}$ is computed from $\mathbf q_{arm}$ through \ref{Eq:forward}. Given the 
redundancy of the arm kinematics problem \ref{Eq:reaching1} has infinite solutions. To 
overcome this problem we constrained the solution so that one of the joint, for example 
joint number 2, is forced to remain as close as possible to a predefined position $q_{20}$:
%
\begin{equation} 
\label{Eq:reaching2}
  \displaystyle\min_{\mathbf q_{arm}}\left(J_c\right)=\displaystyle\min_{\mathbf q_{arm}}
  \left[
  \left\|\mathbf x_{hand} - \mathbf x_{target}\right\|^2 + \left(q_{arm}(2)-q_{20})\right)^2
  \right]
\end{equation}

\emph{[missing description: numerical recipes algorithm]}

To laern the forward map of equation \ref{Eq:forward} we programmed the robot 
to perform random movement with the arm (chosen to uniformly sample 
a predefined region in the robot workspace). During this ``exploratory'' phase the robot
tracked the hand, to collect samples of the form:
%
\begin{center}
\begin{math}
  \left(\begin{array}{cc}
    \mathbf x_{hand} & \mathbf q_{arm} \end{array}\right)_{0,1\dots,k}
\end{math}
\end{center}
%
A neural network was then trained to learn the relation:
%
\begin{center} 
\begin{math}
  \mathbf x_{hand}=\hat{f}_{arm}\left(\mathbf q_{arm}\right)
\end{math}
\end{center}
%
which approximates equation \ref{Eq:forward}.



























