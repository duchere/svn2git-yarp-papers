\section{Reaching}
\label{sec:reaching}

We describe two approaches to solve the reaching task. A first method 
uses the forward mapping between the arm joint space and the position 
of the hand represented in the head reference frame \begin{bmatrix} 
\theta_y & \theta_p & \alpha_v^d\end{bmatrix}^\top \in \mathbb R^3 \end{bmatrix}. 
The second method controls the speeds of the arm so to minimize the position 
of the hand in the image plane with respect to the target (usually the 
center of the image plane).

\subsection{Open Loop Reaching}
Suppose the robot is tracking a target using the controlled described in 
\ref{Sec:TrackerController}. In the further assumption of perfect tracking 
(the visual error is zero), the spatial position of the target
in the world can be represented by means of the head joints:

\begin{eqnarray*}
\mathbf \tilde{x}_{target}= \mathbf q_{head} =
\begin{bmatrix} \theta_y & \theta_p & \theta_r & \alpha_v^d & \alpha_v^c & \alpha_t^c \end{bmatrix}^\top \in \mathbb R^6.
\end{eqnarray*}

Not all components of $\mathb q_{head}$ are actually needed. During tracking 
$\theta_r$ is maintained stationary to 0, while the head controller 
poses additional contraints on the head joints; in particular we know from section 
\ref{Sec:TrackerController} that the controller minimizes $\alpha_t^c$ and
$\alpha^c_v$ (see equation \ref{Eq:HeadEyeControl}) so that they asymptotically
converge to $\alpha_t^c \rightarrow 0$ and $\alpha_v^c \rightarrow 0$. 

After convergence has been achieved we have:

\begin{eqnarray*}
\tilde {\mathbf x}_{target}=
\begin{bmatrix} \theta_y & \theta_p & 0 & \alpha_v^d & 0 & 0 \end{bmatrix}^\top \in \mathbb R^6.
\end{eqnarray*}

we define:

\begin{eqnarray*}
\mathbf x_{target}=
\begin{bmatrix} \theta_y & \theta_p & \alpha_v^d\end{bmatrix}^\top \in \mathbb R^3.
\end{eqnarray*}

which, in our case, uniquely codes the position of the target. This representation is very close to a three dimensional polar representation in which \theta_y and \theta_p code respectively azimuth and elevation, while distance is substituted with \alpha_v (\emph{vergence} angle). 

If the robot is tracking the hand, the same subset of the head joint space can be used to code the spatial location of the hand:

\begin{eqnarray*}
\mathbf x_{hand}=
\begin{bmatrix} \theta_y & \theta_p & \alpha_v^d\end{bmatrix}^\top \in \mathbb R^3.
\end{eqnarray*}

Under these assumptions we can train a neural network to approximate the forward mapping between the arm joint space $\mathbf q_{arm}$ and the position of the hand $\mathbf x_{hand}$:

\begin{equation} \label{Eq:forward}
\mathbf x_{hand}=f_{forward}}(\mathbf q_{arm}), \qquad f_{ol} : \mathbb R^3 \longrightarrow \mathbb R^4.\end{equation}

Suppose now the robot is tracking














