\section{Results}
\label{sec:results}

In this section we describe the reaching performance obtained with 
the reaching controller described in the previous section. It is
important to underline that a formal definition of the performance 
index is a nontrivial task. 

If we were to consider a robot which operates in an highly structured
environment, the definition of the performance index would have been much easier. 
Specifically, for an industrial robot, the reaching task can be identified 
with the problem of positioning the hand accurately with respect to a world 
reference frame\footnote{Typical industrial robots do not use vision. Their 
grasping movement is the result of 3-D positioning the gripper exactly in the 
position where they know the object to be grasped will be positioned. The problem
of positioning the object is then left to the user.}. In this case, the reaching 
precision can be measured in terms of the distance between the desired and 
actual position of the hand in the world reference frame.
 
In our case, a similar performance index definition cannot be applied. James has not 
been designed to operate in an industrial scenario. Moreover, in our context, the definition of a reference 
frame fixed with respect to the external world does not play the same crucial 
role played in the industrial framework. In our mind, it is more important to 
precisely locate the hand with respect to the object than to precisely locate 
the hand in the external world frame. Therefore, our reaching performance index 
should measure the Cartesian distance between the object to be grasped and the final 
position of the hand. However, measuring this distance is not an easy task. In this section
we approximate the real distance using the stereo cameras image plane distance. Practically speaking, suppose the 
robot sees a target $\mathbf {u_{target}}$ to be grasped. As we have already seen in order to reach the 
target we need to fixate it $\mathbf {u_{target}} = 0$. Using the available sensor (i.e. vision) the best we can do to precisely reach the target is moving the hand to the fixation point as well, i.e. $\mathbf 
{u_{hand}} \longrightarrow 0$. Clearly, the image plane distance $\| \mathbf {u_{hand}} - \mathbf 
{u_{target}}\|$ can be used as a rough estimate for the reaching precision, intended as  the
Cartesian distance between the object to be grasped and the position of the hand. Specifically,
assuming infinite resolution of the camera sensor, if $\| \mathbf {u_{hand}} - \mathbf 
{u_{target}}\| = 0$ then the hand has exactly reached the target. In a more realistic case,
when the image plane distance is null, we can only guarantee that the Cartesian target-hand distance
is upper bounded by a threshold which depends only on the camera intrinsic parameters\footnote{
Computing this upper bound is out of the scope of this paper.}. 

\subsection{Open Loop}
As we have already seen, the first attempt to reach the target consists in using (\ref{Eq:reaching2})
to choose the arm configuration $\q_{arm}$ capable of performing the reaching. Clearly, if the forward 
kinematic function (\ref{Eq:forward}) were perfectly represented and if the target is reachable , we would have $\mathbf x_{hand} =  \mathbf x_{target}$, which implies that the Cartesian distance between the target hand
 is null (see Section \ref{sec:reaching} for details). Therefore, in this ideal case, the open loop 
 strategy already leads to $\| \mathbf {u_{hand}} - \mathbf {u_{target}}\| = 0$. In practice, the model 
 (\ref{Eq:forward}) cannot exactly represent the system kinematic\footnote{Part of the representational 
 errors are related to the way we have chosen the represent the kinematic function, in this case the
 so called Receptive Field Weighted Regression model. Part are due to the mechanical plays of the
 kinematic structure.}. Therefore, even tough we can find $\q_{arm}$ such that $\mathbf x_{hand}=
 \hat f_{arm}(\mathbf q_{arm})$ it is not guaranteed that after the movement execution 
 $\| \mathbf {u_{hand}} - \mathbf {u_{target}}\| = 0$. Figure \ref{Fig:ImagePlaneOpenLoopErrors}
 shows the image plane errors after the execution of the open loop movement. The plot has been obtained
 by fixating a target and performing a series of open loop movements. Each open loop
 movement was different because (\ref{Eq:reaching2}) was solved 
 by changing each time the value $q_{20}$. Let us denote the open loop movements $\q_{arm}^1$, $\dots$, 
 $\q_{arm}^K$ such that:
 
 \begin{eqnarray} \label{Eq:OpenLoopMovements}
 \q_{arm}^k = \arg \min_{\mathbf q_{arm}}
  \left[
  \left\| \hat f_{arm}(\mathbf q_{arm}) - \mathbf x_{target}\right\|^2 + \left(q_{arm,2}-q_{20}^k\right)^2
  \right].
 \end{eqnarray}
 
 The above minimization is such that $\hat f_{arm}(\mathbf q_{arm}^k) = \mathbf x_{target}$. Ideally, in absence
 of modelling errors we would have $ \mathbf x_{hand} = \mathbf x_{target}$ since $\mathbf x_{hand} = f_{arm}(\mathbf q_{arm}^k)$. In practice we have $\mathbf x_{hand} \simeq \hat f_{arm}(\mathbf q_{arm})$ 
 so that we can only achieve an approximate reaching $ \mathbf x_{hand} \simeq \mathbf x_{target}$ 
 which reflects into small image plane errors $\| \mathbf {u_{hand}} - \mathbf {u_{target}}\| \simeq 0$. Different choices 
 of the free variables $q_{20}^1$, $\dots$, $q_{20}^K$ may lead to different image plane errors as shown 
 in Figure \ref{Fig:ImagePlaneOpenLoopErrors}.



\begin{figure}
  % Requires \usepackage{graphicx}
  \begin{center}
	\begin{tabular}{ccc}
	  \parbox{40mm}{\includegraphics[width=40mm]{Figure/LeftEyeOpenLoop.eps}}  & \hspace{2cm} &
	  \parbox{40mm}{\includegraphics[width=40mm]{Figure/RightEyeOpenLoop.eps}}
	  \\
	  Left eye & \hspace{2cm} & Right eye
	  %	  \end{t\\
	  %	Top view & & Lateral view
  \end{tabular}
\end{center}
\caption{The image shows the open loop image plane errors $\mathbf {u_{hand}}$ for different
choices of the redundant variable $q_{20}$. For both pictures, on the horizontal axis 
we have $u_r$ and $u_l$ while on the vertical axis we have $v_r$ and $v_l$ (always in pixels).
In this specific case the target is in the middle of the two image planes 
$[u_{r, target}, v_{r, target}] =[0,0]$ and $[u_{l, target}, v_{l, target}]=[0,0]$.
The hand position in the image plane is instead represented 
by the small clusters.  Each cluster corresponds to the hand position 
after different open loop movements, i.e. different values for the variable $q_{20}$.
The variability within the cluster is due to measurement errors which follows from the hand localization routine.}\label{Fig:ImagePlaneOpenLoopErrors}
  \end{figure}

\section{Closed Loop}

As we have already seen in Section \ref{Eq:ClosedLoop} the residual image plane errors 
due the imperfections in the forward kinamtic model can be reduced by a visual closed loop
control strategy. This control strategy should move the arm so as to progressively drive the
hand image plane positions, ${\mathbf {u_{hand}}}$, to zero. Of course this is guaranteed 
only if the  Jacobian matrix has been learnt in a sufficiently accurate way. Figure 
\ref{Fig:ImagePlaneClosedLoopErrors} and \ref{Fig:TimeResponseClosedLoopErrors} 
show how the hand is actually driven to the 
exact image center in both the image planes. Moreover, it is important to notice the
approximative linearity of the path followed by the hand. This linearity can be shown
to be a consequence of the accuracy of the learnt Jacobian.

\begin{figure}
  % Requires \usepackage{graphicx}
  \begin{center}
	\begin{tabular}{ccc}
	  \parbox{40mm}{\includegraphics[width=40mm]{Figure/LeftEyeClosedLoop.eps}}  & \hspace{2cm} &
	  \parbox{40mm}{\includegraphics[width=40mm]{Figure/RightEyeClosedLoop.eps}}
	  \\
	  Left eye & \hspace{2cm} & Right eye
	  %	  \end{t\\
	  %	Top view & & Lateral view
  \end{tabular}
\end{center}
\caption{The picture shows different visual closed loop control actions. Each trace correspond to a different Cartesian position of the target to be reached (which is always visually at the center of the image planes). The traces start exactly after the execution of the open loop movement so that the initial position corresponds to the initial open loop error. Notice that all the traces ends up in the image center (both left and right image planes) thus indicating that the visual errors are completely eliminated by the closed loop strategy.}\label{Fig:ImagePlaneClosedLoopErrors}
  \end{figure}

\begin{figure}
  % Requires \usepackage{graphicx}
  \begin{center}
	\begin{tabular}{ccc}
	  \parbox{40mm}{\includegraphics[width=40mm]{Figure/TimeReponseLeftClosedLoop.eps}}  & \hspace{2cm} &
	  \parbox{40mm}{\includegraphics[width=40mm]{Figure/TimeReponseRightClosedLoop.eps}}
	  \\
	  Left eye & \hspace{2cm} & Right eye
	  %	  \end{t\\
	  %	Top view & & Lateral view
  \end{tabular}
\end{center}
\caption{The picture shows the time response of the closed loop strategy. The solid lines corresponds to the hand horizontal position in the left ($u_l$) and right ($u_r$) image planes. The dashed lines correspond to the vertical position, $v_l$ and $v_r$. Clearly, the hand is driven to the image center with a null steady state error. Even if velocity was not our primarly concern, the time response is reasonably fast (an error of thirty pixels is eliminated in a little bit more than three seconds). }\label{Fig:TimeResponseClosedLoopErrors}
  \end{figure}
