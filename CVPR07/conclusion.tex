A new method is presented to keep Support Vector Machines small,
called OISVMs (Online Independent Support Vector Machines). OISVMs
avoid adding to the kernel matrix support vectors which are linearly
dependent of previous ones in the feature space --- in other words,
the kernel matrix is always kept at full rank. The primal SVM problem
is then solved via an incremental algorithm which benefits of the
small size of the kernel matrix.

We have tested the method both on a standard set of benchmark
databases and a real-world case study, namely place recognition in an
indoor environment, from sequences acquired by robot platforms under
different weather conditions and across a time span of several months.

The experimental results show that $(i)$ in the case of
finite-dimensional kernels, OISVMs attain the theoretical limit of
linearly independent support vectors allowed by the feature space
without losing any accuracy at all with respect to ordinary SVMs;
$(ii)$ in the case of infinite-dimensional kernels, they dramatically
reduce the number support vectors at the price of a negligible
degradation in the accuracy. In this latter case they can also be used
to obtain full precision, choosing the tolerance threshold to be equal
to machine precision.
