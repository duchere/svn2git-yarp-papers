\subsection{Experiments with Real-world Application}
\label{exp:idol2}

We performed a second series of experiments, namely place recognition in 
an indoor environment, to evaluate our algorithm. 
We considered a realistic scenario where the algorithm had to 
incrementally update the
model to adapt to the variations in an indoor environment 
introduced by human activities
over a long time spans. These variations includes people appearing 
in different rooms during
working time, objects such as cups moved or taken in/out of the drawers, 
pieces of
furniture pushed around, and so forth.   

\begin{figure*}[t]
\centering \footnotesize
\begin{tabular}{@{}c@{\hspace{0.002\linewidth}}c@{\hspace{0.002\linewidth}}c@{\hspace{0.002\linewidth}}c@{\hspace{0.002\linewidth}}c@{\hspace{0.002\linewidth}}c@{\hspace{0.002\linewidth}}c@{\hspace{0.002\linewidth}}c@{}}
% -------------------------------------------------------------------
\includegraphics[width=0.123\linewidth]{figs/idol/bo_cloudy.png} &
\includegraphics[width=0.123\linewidth]{figs/idol/bo_night.png}  &
\includegraphics[width=0.123\linewidth]{figs/idol/bo_sunny.png}  &
\includegraphics[width=0.123\linewidth]{figs/idol/cr_cloudy.png} &
\includegraphics[width=0.123\linewidth]{figs/idol/cr_night.png}  &
\includegraphics[width=0.123\linewidth]{figs/idol/cr_sunny.png} &
\includegraphics[width=0.123\linewidth]{figs/idol/people1.png}  &
\includegraphics[width=0.123\linewidth]{figs/idol/people2.png}  \\

% -------------------------------------------------------------------
\includegraphics[width=0.123\linewidth]{figs/idol/cup1.png}   &
\includegraphics[width=0.123\linewidth]{figs/idol/cup3.png}   &
\includegraphics[width=0.123\linewidth]{figs/idol/chair1.png}   &
\includegraphics[width=0.123\linewidth]{figs/idol/chair3.png} &
\includegraphics[width=0.123\linewidth]{figs/idol/time1.png} &
\includegraphics[width=0.123\linewidth]{figs/idol/time2.png} &
\includegraphics[width=0.123\linewidth]{figs/idol/time3.png}  &
\includegraphics[width=0.123\linewidth]{figs/idol/time4.png}  \\
% -------------------------------------------------------------------
\includegraphics[width=0.123\linewidth]{figs/idol/drive1.png}   &
\includegraphics[width=0.123\linewidth]{figs/idol/drive2.png}   &
\includegraphics[width=0.123\linewidth]{figs/idol/drive3.png}   &
\includegraphics[width=0.123\linewidth]{figs/idol/drive4.png} &
\includegraphics[width=0.123\linewidth]{figs/idol/drive5.png} &
\includegraphics[width=0.123\linewidth]{figs/idol/drive6.png} &
\includegraphics[width=0.123\linewidth]{figs/idol/drive7.png}  &
\includegraphics[width=0.123\linewidth]{figs/idol/drive8.png}   \\
% -------------------------------------------------------------------
\end{tabular}
\caption{Sample images illustrating the variations captured in the IDOL2 database.
Images in the top row show the variability introduced by changes in illumination
for two rooms (first six images) as well as people appearing in the environment.
The middle row shows the influence of people's everyday activity (first four images)
as well as larger variations which happened over a time span of 6 months. Finally, the
bottom row illustrates the changes in viewpoint observed for a series of images acquired 
one after another in 1.6 second.}
\label{fig:idol}
\end{figure*}

For experiments, we used a newly introduced database called IDOL2 
(Image Database for rObot Localization 2, \cite{luo:idol2}), 
which contains 24 image sequences acquired using a perspective
camera mounted on two mobile robot platforms The acquisition was
performed within an indoor laboratory environment consisting of five 
rooms of different functionality. The sequences were acquired under
various weather and illumination conditions (sunny, cloudy, and night)
and across a time span of six months. Thus, this data capture
natural variability that occurs in real-world environments because of both 
natural changes in the illumination and human activity. Fig. \ref{fig:idol} 
shows some sample images from the database, illustrate these variations.
The image sequences in the database are divided as
follows: for each robot platform and for each type of illumination conditions, 
there were
four sequences recorded. Of these four sequences, the first two were 
acquired six months
before the last two. This means that, for each robot and for every 
illumination condition,
there are always two sequences acquired under similar conditions, and two 
sequences acquired
under very different conditions. It makes the database suitable for 
different kinds of
evaluation on the adaptability of an incremental algorithm. 
For further details about the
database, we refer the readers to \cite{luo:idol2}.

The evaluation was performed using composed receptive field histograms (CRFH)
\cite{Linde:Lindeberg:ICPR04} as global image features and SIFT \cite{lowe99object}
for extracting local features. In the experiments, we consider both $\chi^2$ kernel
for SVM (when use CRFH), and local kernels \cite{wallraven:iccv03} (SIFT).
Similarly to the previous set of experiments, we run the OISVM using 
different values of %\eta%, for both kernels.
We benchmarked  OISVM against two approximate incremental SVM extensions:
the standard fixed-partition technique \cite{ijcai99} and the 
memory-controlled incremental
SVM \cite{luo:icra07}, a recently introduced version of incremental SVM, 
 inspired by the work of Downs et~al. \cite{DownsGM01}. 
We used a similar experimental
method as the one presented in \cite{luo:icra07}. 
The algorithm was trained incrementally on
three sequences from IDOL2 acquired under similar illumination conditions 
with the same robot
platform; the fourth sequence was used for testing. In order to test the 
various properties of
interest of the incremental algorithms, we need a reasonable number of 
incremental steps.
Thus, every sequence was splitted into 5 subsequences, so that each subset 
contained one of the
five images acquired by the robot every second (image sequences were acquired 
at a rate of
5fps). Since during acquisition the camera's viewpoint continuously changes 
\cite{luo:icra07},
the subsequences could be considered as recorded separately in a static 
environment but for
varying pose. This setup allows us to examine how the algorithms perform 
on data with less
variations. As a result, training on each sequence was performed in 5 steps, 
using one subsequence
at a time, resulting in 15 steps in total. In total, we considered 36 
different permutations
of training and test sequences for $chi^2$ kernel and 12 permutations for 
local kernel
\footnote{Upon acceptance of the paper, we will report results on 36 permutations for the local kernel as well (experiments currently running).}; here
we report average results. Fig. \ref{fig:chi},top, shows the recognition rates 
of $chi^2$ kernel experiments
We have run the OISVM with $\eta$ at different values, similar to previous setup.

We benchmarked our OISVM with two approximate incremental SVM extensions:
the standard fixed-partition technique \cite{ijcai99} and the memory-controlled incremental
SVM \cite{luo:icra07}, a recently introduced version of incremental SVM, 
 inspired from Downs et~al. \cite{DownsGM01}. We used a similar experimental
method as the one presented in \cite{luo:icra07}. The algorithm was trained incrementally on
three sequences from IDOL2 acquired under similar illumination conditions with the same robot
platform; the fourth sequence was used for testing. In order to test the various properties of
interest of the incremental algorithms, we need a reasonable number of incremental steps.
Thus, every sequence was splitted into 5 subsequences, so that each subset contained one of the
five images acquired by the robot every second (image sequences were acquired at a rate of
5fps). Since during acquisition the camera's viewpoint continuously changes \cite{luo:icra07},
the subsequences could be considered as recorded separately in a static environment but for
varying pose. In order to get a feeling of the variations of the frame images in a sequence,
bottom row of Fig. \ref{fig:idol} shows some sample images acquired within a time span of 1.6 sec.
This setup allows us to examine how the algorithms perform on data with less
variations. As a result, training on each sequence was performed in 5 steps, using one subsequence
at a time, resulting in 15 steps in total. In total, we considered 36 different permutations
of training and test sequences for $\chi^2$ kernel and 12 permutations for local kernel; here
we report average results. Fig. \ref{fig:chi},top, shows the recognition rates of $\chi^2$ kernel experiments
obtained at each step using the OISVM (with three different $\eta$ value), fixed-partition and
memory-controlled. Fig. \ref{fig:chi}, bottom, reports number of support vectors stored in the model of
each step. \textbf{FIXME: here comments with numbers}

\begin{figure}[t]
\footnotesize
  \begin{tabular}{@{}c@{}}
  \includegraphics[width=0.95\linewidth]{figs/results/chi_cr}\\
  (a)~Classification rate at each incremental step.\vspace{0.1cm}\\
  \includegraphics[width=0.95\linewidth]{figs/results/chi_sv}\\
  (b)~Number of support vectors at each incremental step.\\
  \end{tabular}
\caption{Average results obtained for experiment performed for
         OISVM using three different $\eta$ values as well as
         fixed-partition and memory-controlled method. }
\label{fig:chi}
\end{figure}
