Due to space limitations, this is a very quick account of SVMs --- the
interested reader is referred to \cite{Burges98,SmolaTut2004} for a
tutorial, and to \cite{Cristianini00} for a comprehensive introduction
to the subject. Assume $\{\xx_i,y_i\}_{i=1}^l$, with $\xx_i \in \RR^m$
and $y_i \in \{-1,1\}$, is a set of samples and labels drawn from an
unknown probability distribution. The problem is to approximate the
distribution in order to classify more data coming from the same
source to the best extent possible. In the most general setting, a
\emph{separating hyperplane} in $\RR^m$ is sought for,

\begin{equation} \label{eqn:sol}
  f(\xx) = sgn \left( \sum_{i=1}^l \alpha_i y_i K(\xx,\xx_i) + b \right)
\end{equation}

\noindent where $\alpha_i,b \in \RR$ and $K(\xx_1,\xx_2) = \Phi(\xx_1)
\cdot \Phi(\xx_2)$, the \emph{kernel function}, evaluates dot
products between elements of $\RR^m$ in the feature space --- that is,
through a non-linear mapping $\Phi(\xx)$. This enables SVMs to work
with highly non-separable data, gifted with non-linear features.

The coefficients $\alpha_i$ are found by minimisation of the
associated \emph{Lagrangian primal problem} (training phase):

\begin{equation} \label{eqn:svm_primal}
  \min_{\ww} \left( ||\ww||^2 + C \sum_{i=1}^l \xi_i \right)
\end{equation}

\noindent subject to the constraints

\begin{eqnarray} \label{eqn:svm_constr}
  y_i (\ww\cdot\xx_i + b) & \geq & 1-\xi_i \\
                    \xi_i & \geq & 0 \nonumber
\end{eqnarray}

\noindent where $C \in \RR$ is a positive weight coefficient. The main
element is the so-called \emph{kernel matrix} $K$, where $K_{ij} =
K(\xx_i,\xx_j)$. Notice that in general the size of $K$ is $l \times
l$, but that in most practical applications, most of the $\alpha_i$
are found to be zero after training and therefore can be neglected in
evaluating the solution. In fact, those $\xx_i$s for which this does
\emph{not} hold are somehow crucial to the solution and are called
\emph{support vectors}. This phenomenon is known as \emph{sparseness}
of the solution, meaning that only a subset of the training data is
usually really needed to build it.
