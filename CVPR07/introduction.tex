Introduced in the early 90s by Boser, Guyon and Vapnik \cite{BGV92},
\emph{Support Vector Machines} (SVMs) are a class of machine learning
algorithms deeply rooted in Statistical Learning Theory
\cite{v-edbed-82}, able to classify data taken from an unknown
probability distribution, given a set of training examples. As opposed
to analogous methods such as, e.g., artificial neural networks, they
have the main advantages that $(a)$ training is guaranteed to end up
in a global minimum, $(b)$ their generalization power is theoretically
well founded, $(c)$ they can easily work with highly dimensional,
non-linear data, and $(d)$ the solution achieved is sparse. Due to
these good properties, they have been now extensively used in, e.g.,
speech recognition, object classification and function approximation
\cite{Cristianini00}. On the other hand, one of their main drawbacks
is their alleged inability to cope with large datasets
\cite{KeerthiCDC06}.

Yet, in most real-life applications, datasets \emph{are} large, for
example when online learning must be performed. Online learning is a
scenario in which training data is provided one example at a time, as
opposed to the batch mode in which all examples are available at once
(see \cite{Laskov2006} and citations therein). In the case of, e.g.,
non-stationary data, online algorithms will generally perform better
since ambiguous information (i.e., whose distribution varies over
time) is present, and couldn't possibly be taken into account by the
batch algorithm. Online algorithms allow to incorporate additional
training data, when it is available, without re-training from scratch.

In an online setting there is no guarantee that the flow of data will
\emph{ever} cease; therefore, applying SVMs here looks appealing but
we need a way to cope with large datasets. One of the keys to the
problem seems to lie in  the sparseness of their solution. That an
SVM solution is \emph{sparse} means that usually just a few samples
account for its complexity; in fact, SVMs can be seen as a way of
compressing data by selecting ``the most important'' samples
(\emph{support vectors}, SV) among those in the training set. Keeping
the number of SVs small without losing accuracy is therefore a major
challenge, also since a recent result \cite{Steinwart03} shows that
their number grows indefinitely with (namely, proportionally to) the
number of training samples.

In this paper we propose a method of incrementally selecting SVs based
upon \emph{linear independence in the feature space}: SVs which are
linearly dependent on already stored ones are rejected, and a smart,
incremental minimization algorithm is employed to find the new minimum
of the cost function. The size of the kernel matrix (the core of an
SVM and its major bottleneck) is therefore kept small. Our experiments
indicate that SVMs employing this idea, that we call
\emph{Online Independent Support Vector Machines} (OISVMs), do not
grow linearly with the training set, as it was the case in
\cite{Steinwart03}, but reach a limit size and then stop growing
\cite{engel2004}. In the case of finite-dimensional feature spaces
they also \emph{keep the full accuracy of standard SVMs}; whereas in
the infinite-dimensional case, at the price of a negligible loss in
accuracy, one can tune the growing rate of the machine.

To support this claim, we show an extensive set of experimental
results obtained by comparing SVMs and OISVMs on standard benchmark
databases as well as on a real-world, online application coming from
robotic vision: place recognition in an indoor environment, from
sequences acquired by robot platforms under different weather
conditions and across a time span of several months. Our results show
that, on standard benchmarks, the accuracy of OISVMs can be up to only
$0.063\%$ worse than SVMs, with less than $5\%$ of the support
vectors; whereas, on the real-world application, we get as few as one
third of the SVs required by an online approximated method, while
retaining essentially the same accuracy.

The paper is structured as follows: after a review of the relevant
literature, Section \ref{sec:bg} gives an overview of background
mathematics proper to SVMs; in Section \ref{sec:opt} OISVMs are
described.  Section \ref{sec:exp}  shows the experimental results
and lastly, in Section \ref{sec:concl}, conclusions are drawn and future
work is outlined.
