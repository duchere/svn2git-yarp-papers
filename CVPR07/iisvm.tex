Following Keerthi et al. \cite{KeerthiCDC06} then, and inspired by the
above considerations, we explicitly choose a subset of the support
vectors to form a ``basis'' for the solution. In that paper, two
heuristics are proposed to select an appropriate subset of support
vectors; we hereby propose to incrementally select a set of support
vectors that are linearly independent in the feature space. The
solution found this way is the same as if using all the training
samples as basis set, that is the classical SVM formulation. No
approximation whatsoever is involved, unless one gives it up in order
to obtain even less support vectors. See below, especially Section
\ref{sec:exp}, for a discussion on this point.

In the following, the notation $A_{IJ}$ and $\mathbf{v}_I$, where $A$
is a matrix, $\mathbf{v}$ is a vector and $I,J \subset \NN$ denote in
turn the sub-matrix and the sub-vector obtained from $A$ and
$\mathbf{v}$ by taking the indexes in $I$ and $J$. We assume that a
set of $l$ training samples is available and that the machine has been
trained on them. The indexes of the vectors in the current basis are
denoted by $\b$, and $\xx_{l+1}$ denotes the new sample under
judgement. Since the procedure is incremental, we also assume that the
vectors indexed by $\b$ are linearly independent in the feature space,
that is, that $K_{\b\b}$ has full rank. The question is: should the
new sample be added to the basis? The algorithm can then be summed up
as follows:

\begin{itemize}

  \item check whether $\xx_{l+1}$ is linearly independent in the
        feature space with respect to the basis. If it is, add it to
        the basis (otherwise, leave the basis unchanged);

  \item incrementally re-train the machine.

\end{itemize}

The next two Subsections detail the linear independence test and the
training method.

\subsection{Linear independence}

In general, checking linear independence in a matrix is done via some
decomposition, or by looking at the eigenvalues of the matrix; but
here we want to check whether a \emph{single} vector is linearly
independent from a set of vectors. Inspired by the definition of
linear independence \cite{EngelMM02sparse}, we check how well the
vector can be approximated by a linear combination of the vectors in
the set. Let $d_j \in \RR$ with $j \in \b$; then let

\begin{equation} \label{eqn:ald1}
  \Delta = \min_\dd \left|\left|\sum_{j \in \b} d_j \phi(\xx_j) - \phi(\xx_{l+1}) \right|\right|^2
\end{equation}

If $\Delta > 0$ then $\xx_{l+1}$ is linearly independent with respect
to the basis, and $l+1$ is added to $\b$. In practice, we check
whether $\Delta \leq \eta$ where $\eta > 0$ is a tolerance factor, and
we expect that larger values of $\eta$ lead to worse accuracy, but also
to smaller bases. As a matter of fact, if $\eta$ is set at machine
precision, IISVMs retain the exact accuracy of SVMs.

Expanding equation (\ref{eqn:ald1}) we get

\begin{eqnarray} \label{eqn:ald2}
  \Delta = \min_{\dd} (
      \sum_{i,j \in \b} d_j d_i \phi(\xx_j) \cdot \phi(\xx_i) \\
    - 2\sum_{j \in \b} d_j \phi(\xx_j) \cdot \phi(\xx_{l+1}) \nonumber \\
    + \phi(\xx_{l+1}) \cdot \phi(\xx_{l+1}) ) \nonumber
\end{eqnarray}

that is, applying the kernel trick,

\begin{equation} \label{eqn:ald3}
  \Delta = \min_{\dd} \left(
      \dd^T K_{\b\b}\dd
    - 2 \dd^T \mathbf{k}
    + K(\xx_{l+1},\xx_{l+1})
  \right)
\end{equation}

where $k_i = K(\xx_i,\xx_{l+1})$ with $i \in \b$.
%It is
%apparent from Equation (\ref{eqn:ald3}) that the range of $\eta$ is
%related to the kernel used; for example for Gaussian kernels $\D \leq
%1$ and hence good values of $\eta$ range in $\{0,1\}$.

Solving (\ref{eqn:ald3}), that is, applying the extremum conditions
with respect to $\dd$, we obtain

\begin{equation} \label{eqn:ald4}
  \mdd = K_{\b\b}^{-1} \mathbf{k} \\
\end{equation}

and, by replacing (\ref{eqn:ald4}) in (\ref{eqn:ald3}) once,

\begin{equation} \label{eqn:ald5}
  \Delta = K(\xx_{l+1},\xx_{l+1}) - \mathbf{k}^T \mdd
\end{equation}

Note that $\b$ can be safely inverted since, by incremental
construction, it is full-rank. An efficient way to do it, exploiting
the incrementality of the approach, is that of updating it
recursively:

\begin{equation} \label{eqn:inv_upd}
  K_{\b\b}^{-1} ~~~ \leftarrow ~~~
  \left[\begin{array}{cccc}
       &               &   & 0 \\
       & K_{\b\b}^{-1} &   & \vdots \\
       &               &   & 0 \\
     0 &       \cdots  & 0 & 0
  \end{array}\right]
  +
  \frac{1}{\Delta}
  \left[\begin{array}{c}
    \mdd \\
    -1
  \end{array}\right]
  \left[\begin{array}{cc}
    \mdd^T & -1
  \end{array}\right]
\end{equation}

where $\mdd$ and $\Delta$ are already evaluated during the test. This
method matches the one used in Cauwenberghs and Poggio's incremental
algorithm \cite{CauwenberghsP00}, in turn similar to on-line recursive
estimation of the covariance of sparsified Gaussian processes
\cite{csat'o01sparse}.

\subsection{Training the machine}

The training method largely follows Keerthi et
al. \cite{KeerthiDC05,KeerthiCDC06}. We consider the norm-2
formulation of (\ref{eqn:svm_primal}), e.g., $\xi_i$ is squared in the
sum, and transform it to an unconstrained problem. Let $\d \subset
\{1,\ldots,l\}$; then the unconstrained problem is

\begin{equation} \label{eqn:primal}
  \min_{\bb} \left( 
      \frac{1}{2} \bb^T K_{\d\d} \bb
    + \frac{1}{2} C \sum_{i=1}^l max \left(0,1-y_i K_{i,\d} \bb \right)^2
  \right)
\end{equation}

Then, we explicitly set $\d = \b$, assuring thus that the solution to
the problem is unique, since $K_{\b\b}$ is full rank by
construction. Newton's method as modified by Keerthi et
al. \cite{KeerthiDC05,KeerthiCDC06} can then be used to solve
(\ref{eqn:primal}). The steps are:

\begin{enumerate}

   \item use the current value of $\boldsymbol{\beta}$ as as starting
     vector;

   \item let $\mathcal{I} = \{ i: 1-y_i o_i<0 \}$ where $o_i =
     K_{i,\b} \bb$ is the output of the $i$-th training sample;

   \item update $\bb$ with a Newton step:
     $\bb - \gamma \mathbf{P}^{-1}\mathbf{g} \rightarrow \bb$ where
     $\mathbf{P} = K_{\b\b} + C K_{\b\mathcal{I}} K_{\b\mathcal{I}}^T$ and
     $\mathbf{g} = K_{\b\b} \bb - C K_{\b\mathcal{I}}
        \left( \mathbf{y}_{\mathcal{I}}-\mathbf{o}_{\mathcal{I}}\right)$;

   \item if $\bb$ is the optimal solution stop; otherwise go to step
     2.

\end{enumerate}

In Step $3$ above, $\gamma$ is set to one. In order to speed up the
algorithm, we maintain an updated Cholesky decomposition of
$\mathbf{P}$. The space complexity of the algorithm is
$O(|\b|^2+|\b|l)$.
