The time required by an SVM to train and predict is, in turn, cubic
and linear in the number of support vectors \cite{KeerthiCDC06}. In
general, the possibility to obtain an alternative, possibly more
compact representation of the SVM solution follows from the fact that
the solution of a SVM problem is not unique if the kernel matrix $K$
does not have full rank \cite{Burges98}, which is equivalent to some
of the support vectors being linearly dependent on the others \emph{in
the feature space} (this is the core of Downs et al.'s
\cite{DownsGM01} original idea).

Following Keerthi et al. \cite{KeerthiCDC06} then, and inspired by the
above considerations, we explicitly choose a subset of the support
vectors to form a ``basis'' for the solution. In that paper, two
heuristics are proposed to select an appropriate subset of support
vectors; we hereby propose to incrementally add a support vector if it
is linearly independent in the feature space with respect to the
vectors already present in the basis. This makes our algorithm
suitable for online learning. The solution found this way is the same
as if using all the training samples as basis set, that is the
classical SVM formulation; therefore, no approximation whatsoever is
involved, unless one gives it up in order to obtain even less support
vectors. See below, especially Section \ref{sec:exp}, for a discussion
on this point.

In the following, the notations $A_{IJ}$ and $\mathbf{v}_I$, where $A$
is a matrix, $\mathbf{v}$ is a vector and $I,J \subset \NN$ denote in
turn the sub-matrix and the sub-vector obtained from $A$ and
$\mathbf{v}$ by taking the indexes in $I$ and $J$. We assume that a
set of $l$ training samples is available and that the machine has been
trained on them. The indexes of the vectors in the current basis are
denoted by $\b$, and $\xx_{l+1}$ denotes the new sample under
judgement. Since the procedure is incremental, we also assume that the
vectors indexed by $\b$ are linearly independent in the feature space,
that is, that $K_{\b\b}$ has full rank. The question is: should the
new sample be added to the basis? The algorithm can then be summed up
as follows:

\begin{itemize}

  \item check whether $\xx_{l+1}$ is linearly independent in the
        feature space with respect to the basis. If it is, add it to
        the basis (otherwise, leave the basis unchanged);

  \item incrementally re-train the machine.

\end{itemize}

The next two Subsections detail the linear independence test and the
training method.

\subsection*{Linear independence}

In general, checking whether a matrix has full rank is done via some
decomposition, or by looking at the eigenvalues of the matrix; but
here we want to check whether a \emph{single} vector is linearly
independent from a matrix which is already known to be
full-rank. Inspired by the definition of linear independence
\cite{EngelMM02sparse}, we check how well the vector can be
approximated by a linear combination of the vectors in the set. Let
$d_j \in \RR$ with $j \in \b$; then let

\begin{equation} \label{eqn:ald1}
  \Delta = \min_\dd \left|\left|\sum_{j \in \b} d_j \phi(\xx_j) - \phi(\xx_{l+1}) \right|\right|^2
\end{equation}

If $\Delta > 0$ then $\xx_{l+1}$ is linearly independent with respect
to the basis, and $l+1$ is added to $\b$. In practice, we check
whether $\Delta \leq \eta$ where $\eta > 0$ is a tolerance factor, and
expect that larger values of $\eta$ lead to worse accuracy, but also
to smaller bases. As a matter of fact, if $\eta$ is set at machine
precision, IISVMs retain the exact accuracy of SVMs. Notice also that
if the feature space, i.e., the space in which the $\Phi(\xx)$ live,
has finite dimension $n$, then no more than $n$ linearly independent
vectors can be found, and therefore by using this test $K$'s size can
be at most $n \times n$.

Expanding equation (\ref{eqn:ald1}) we get

\begin{eqnarray} \label{eqn:ald2}
  \Delta = \min_{\dd} (
      \sum_{i,j \in \b} d_j d_i \phi(\xx_j) \cdot \phi(\xx_i) \\
    - 2\sum_{j \in \b} d_j \phi(\xx_j) \cdot \phi(\xx_{l+1}) \nonumber \\
    + \phi(\xx_{l+1}) \cdot \phi(\xx_{l+1}) ) \nonumber
\end{eqnarray}

\noindent that is, applying the kernel trick,

\begin{equation} \label{eqn:ald3}
  \Delta = \min_{\dd} \left(
      \dd^T K_{\b\b}\dd
    - 2 \dd^T \mathbf{k}
    + K(\xx_{l+1},\xx_{l+1})
  \right)
\end{equation}

\noindent where $k_i = K(\xx_i,\xx_{l+1})$ with $i \in \b$. Solving
(\ref{eqn:ald3}), that is, applying the extremum conditions with
respect to $\dd$, we obtain

\begin{equation} \label{eqn:ald4}
  \mdd = K_{\b\b}^{-1} \mathbf{k} \\
\end{equation}

\noindent and, by replacing (\ref{eqn:ald4}) in (\ref{eqn:ald3}) once,

\begin{equation} \label{eqn:ald5}
  \Delta = K(\xx_{l+1},\xx_{l+1}) - \mathbf{k}^T \mdd
\end{equation}

Note that $K_{\b\b}$ can be safely inverted since, by incremental
construction, it is full-rank. An efficient way to do it, exploiting
the incrementality of the approach, is that of updating it
recursively: after the addition of a new sample, the new
$K_{\b\b}^{-1}$ then becomes

\begin{equation} \label{eqn:inv_upd}
  \left[\begin{array}{cccc}
       &               &   & 0 \\
       & K_{\b\b}^{-1} &   & \vdots \\
       &               &   & 0 \\
     0 &       \cdots  & 0 & 0
  \end{array}\right]
  +
  \frac{1}{\Delta}
  \left[\begin{array}{c}
    \mdd \\
    -1
  \end{array}\right]
  \left[\begin{array}{cc}
    \mdd^T & -1
  \end{array}\right]
\end{equation}

\noindent where $\mdd$ and $\Delta$ are already evaluated during the test. This
method matches the one used in Cauwenberghs and Poggio's incremental
algorithm \cite{CauwenberghsP00}, in turn similar to on-line recursive
estimation of the covariance of sparsified Gaussian processes
\cite{csat'o01sparse}.

\subsection*{Training the machine}

The training method largely follows Keerthi et
al. \cite{KeerthiDC05,KeerthiCDC06}. We consider the norm-2
formulation of (\ref{eqn:svm_primal}), e.g., $\xi_i$ is squared in the
sum, and transform it to an unconstrained problem. Let $\d \subset
\{1,\ldots,l\}$; then the unconstrained problem is

\begin{equation} \label{eqn:primal}
  \min_{\bb} \left( 
      \frac{1}{2} \bb^T K_{\d\d} \bb
    + \frac{1}{2} C \sum_{i=1}^l max \left(0,1-y_i K_{i,\d} \bb \right)^2
  \right)
\end{equation}

Then, we explicitly set $\d = \b$, assuring thus that the solution to
the problem is unique, since $K_{\b\b}$ is full rank by
construction. Newton's method as modified by Keerthi et
al. \cite{KeerthiDC05,KeerthiCDC06} can then be used to solve
(\ref{eqn:primal}). The steps are:

\begin{enumerate}

   \item use the current value of $\boldsymbol{\beta}$ as as starting
     vector;

   \item let $\mathcal{I} = \{ i: 1-y_i o_i<0 \}$ where $o_i =
     K_{i,\b} \bb$ is the output of the $i$-th training sample;

   \item update $\bb$ with a Newton step:
     $\bb - \gamma \mathbf{P}^{-1}\mathbf{g} \rightarrow \bb$ where
     $\mathbf{P} = K_{\b\b} + C K_{\b\mathcal{I}} K_{\b\mathcal{I}}^T$ and
     $\mathbf{g} = K_{\b\b} \bb - C K_{\b\mathcal{I}}
        \left( \mathbf{y}_{\mathcal{I}}-\mathbf{o}_{\mathcal{I}}\right)$;

   \item if $\bb$ is the optimal solution stop; otherwise go to step
     2.

\end{enumerate}

In Step $3$ above, $\gamma$ is set to one. In order to speed up the
algorithm, we maintain an updated Cholesky decomposition of
$\mathbf{P}$. The space complexity of the algorithm is
$O(|\b|^2+|\b|l)$.
