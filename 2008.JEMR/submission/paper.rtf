{\rtf1\ansi\ansicpg1252\uc1\deff0\stshfdbch0\stshfloch0\stshfhich0\stshfbi0\deflang2057\deflangfe2057{\fonttbl{\f0\froman\fcharset0\fprq2{\*\panose 02020603050405020304}Times New Roman;}
{\f2\fmodern\fcharset0\fprq1{\*\panose 02070309020205020404}Courier New;}{\f36\froman\fcharset238\fprq2 Times New Roman CE;}{\f37\froman\fcharset204\fprq2 Times New Roman Cyr;}{\f39\froman\fcharset161\fprq2 Times New Roman Greek;}
{\f40\froman\fcharset162\fprq2 Times New Roman Tur;}{\f41\froman\fcharset177\fprq2 Times New Roman (Hebrew);}{\f42\froman\fcharset178\fprq2 Times New Roman (Arabic);}{\f43\froman\fcharset186\fprq2 Times New Roman Baltic;}
{\f44\froman\fcharset163\fprq2 Times New Roman (Vietnamese);}{\f56\fmodern\fcharset238\fprq1 Courier New CE;}{\f57\fmodern\fcharset204\fprq1 Courier New Cyr;}{\f59\fmodern\fcharset161\fprq1 Courier New Greek;}
{\f60\fmodern\fcharset162\fprq1 Courier New Tur;}{\f61\fmodern\fcharset177\fprq1 Courier New (Hebrew);}{\f62\fmodern\fcharset178\fprq1 Courier New (Arabic);}{\f63\fmodern\fcharset186\fprq1 Courier New Baltic;}
{\f64\fmodern\fcharset163\fprq1 Courier New (Vietnamese);}}{\colortbl;\red0\green0\blue0;\red0\green0\blue255;\red0\green255\blue255;\red0\green255\blue0;\red255\green0\blue255;\red255\green0\blue0;\red255\green255\blue0;\red255\green255\blue255;
\red0\green0\blue128;\red0\green128\blue128;\red0\green128\blue0;\red128\green0\blue128;\red128\green0\blue0;\red128\green128\blue0;\red128\green128\blue128;\red192\green192\blue192;}{\stylesheet{
\ql \li0\ri0\widctlpar\wrapdefault\aspalpha\aspnum\faauto\adjustright\rin0\lin0\itap0 \fs24\lang1024\langfe1024\cgrid\noproof\langnp1040\langfenp2057 \snext0 Normal;}{\*\cs10 \additive \ssemihidden Default Paragraph Font;}{\*
\ts11\tsrowd\trftsWidthB3\trpaddl108\trpaddr108\trpaddfl3\trpaddft3\trpaddfb3\trpaddfr3\trcbpat1\trcfpat1\tblind0\tblindtype3\tscellwidthfts0\tsvertalt\tsbrdrt\tsbrdrl\tsbrdrb\tsbrdrr\tsbrdrdgl\tsbrdrdgr\tsbrdrh\tsbrdrv 
\ql \li0\ri0\widctlpar\wrapdefault\aspalpha\aspnum\faauto\adjustright\rin0\lin0\itap0 \fs20\lang1024\langfe1024\cgrid\langnp1024\langfenp1024 \snext11 \ssemihidden Normal Table;}{
\s15\ql \li0\ri0\widctlpar\wrapdefault\aspalpha\aspnum\faauto\adjustright\rin0\lin0\itap0 \f2\fs20\lang1024\langfe1024\cgrid\noproof\langnp1040\langfenp2057 \sbasedon0 \snext15 \styrsid16407836 Plain Text;}}{\*\rsidtbl \rsid66679\rsid678333\rsid808761
\rsid2173926\rsid2184191\rsid3307153\rsid3960257\rsid4478300\rsid4981357\rsid5329716\rsid5524173\rsid6565395\rsid6845808\rsid7105999\rsid7630495\rsid8070965\rsid8151142\rsid8615838\rsid9783263\rsid10769687\rsid12609387\rsid14369561\rsid14881444
\rsid15414259\rsid16126363\rsid16407836\rsid16646278}{\*\generator Microsoft Word 10.0.6835;}{\info{\title Gaze Tracking in Semi-Autonomous Grasping}{\author Claudio Castellini}{\operator Claudio Castellini}{\creatim\yr2007\mo11\dy26\hr16\min11}
{\revtim\yr2007\mo11\dy26\hr19\min42}{\version12}{\edmins164}{\nofpages8}{\nofwords3555}{\nofchars20266}{\*\company University of Genova, Italy}{\nofcharsws23774}{\vern16393}{\*\password 00000000}}{\*\xmlnstbl }
\paperw12240\paperh15840\margl1319\margr1319\margt1440\margb1440\gutter0 \widowctrl\ftnbj\aenddoc\grfdocevents0\noxlattoyen\expshrtn\noultrlspc\dntblnsbdb\nospaceforul\formshade\horzdoc\dgmargin\dghspace180\dgvspace180\dghorigin1319\dgvorigin1440\dghshow1
\dgvshow1\jexpand\viewkind4\viewscale100\pgbrdrhead\pgbrdrfoot\splytwnine\ftnlytwnine\htmautsp\nolnhtadjtbl\useltbaln\alntblind\lytcalctblwd\lyttblrtgr\lnbrkrule\nobrkwrptbl\snaptogridincell\allowfieldendsel\wrppunct\asianbrkrule\rsidroot10769687 \fet0
{\*\wgrffmtfilter 013f}\sectd \linex0\headery708\footery708\colsx708\endnhere\sectlinegrid360\sectdefaultcl\sectrsid16407836\sftnbj {\*\pnseclvl1\pnucrm\pnstart1\pnindent720\pnhang {\pntxta .}}{\*\pnseclvl2\pnucltr\pnstart1\pnindent720\pnhang {\pntxta .}}
{\*\pnseclvl3\pndec\pnstart1\pnindent720\pnhang {\pntxta .}}{\*\pnseclvl4\pnlcltr\pnstart1\pnindent720\pnhang {\pntxta )}}{\*\pnseclvl5\pndec\pnstart1\pnindent720\pnhang {\pntxtb (}{\pntxta )}}{\*\pnseclvl6\pnlcltr\pnstart1\pnindent720\pnhang {\pntxtb (}
{\pntxta )}}{\*\pnseclvl7\pnlcrm\pnstart1\pnindent720\pnhang {\pntxtb (}{\pntxta )}}{\*\pnseclvl8\pnlcltr\pnstart1\pnindent720\pnhang {\pntxtb (}{\pntxta )}}{\*\pnseclvl9\pnlcrm\pnstart1\pnindent720\pnhang {\pntxtb (}{\pntxta )}}\pard\plain 
\s15\ql \li0\ri0\widctlpar\wrapdefault\aspalpha\aspnum\faauto\adjustright\rin0\lin0\itap0\pararsid16407836 \f2\fs20\lang1024\langfe1024\cgrid\noproof\langnp1040\langfenp2057 {\insrsid16646278\charrsid16407836 Gaze Track}{\insrsid12609387 
ing in Semi-Autonomous Grasping}{\insrsid16646278\charrsid16407836 
\par }{\insrsid12609387 
\par Claudio Castellini}{\insrsid16646278\charrsid16407836 
\par 
\par }{\insrsid7630495 ABSTRACT}{\insrsid16646278\charrsid16407836 
\par 
\par In critical human/robotic interactions such as, e.g., teleoperation by}{\insrsid4981357  }{\insrsid16646278\charrsid16407836 a disabled master or with insufficient bandwidth, it is highly}{\insrsid4981357  }{\insrsid16646278\charrsid16407836 
desirable to have }{\i\insrsid16646278\charrsid12609387 semi-autonomous}{\insrsid16646278\charrsid16407836  robotic artifacts interact}{\insrsid4981357  }{\insrsid16646278\charrsid16407836 with a human being. Semi-autonomous grasping, for instance, c
onsists}{\insrsid4981357  }{\insrsid16646278\charrsid16407836 of having a smart slave able to guess the master's intentions and}{\insrsid4981357  }{\insrsid16646278\charrsid16407836 initiating a grasping sequence whenever the master wants to grasp an}{
\insrsid4981357  }{\insrsid16646278\charrsid16407836 object in the slave's workspace.
\par 
\par In this paper we investigate the possibility of building such an}{\insrsid4981357  }{\insrsid16646278\charrsid16407836 intelligent robotic artifact by training a machine learning system on}{\insrsid4981357  }{\insrsid16646278\charrsid16407836 
data gathered from several human subjects while trying to grasp}{\insrsid4981357  }{\insrsid16646278\charrsid16407836 objects in a teleoperation setup. In particular, we investigate the}{\insrsid4981357  }{\insrsid16646278\charrsid16407836 
usefulness of gaze tracking in such a scenario. The resulting system}{\insrsid4981357  }{\insrsid16646278\charrsid16407836 must be light enough to be usable on-line and flexible enough to adapt}{\insrsid4981357  }{\insrsid16646278\charrsid16407836 
to different masters, e.g., elderly and/or slow.
\par 
\par The outcome of the experiment is that such a system, based upon}{\insrsid4981357  }{\insrsid16646278\charrsid16407836 Support Vector Machines, meets a}{\insrsid12609387 ll the requirements, being }{\i\insrsid12609387\charrsid12609387 (a)}{
\i\insrsid4981357  }{\insrsid16646278\charrsid16407836 highly accurate, }{\i\insrsid16646278\charrsid12609387 (b)}{\insrsid16646278\charrsid16407836  compact and fast, and }{\i\insrsid16646278\charrsid12609387 (c)}{\insrsid16646278\charrsid16407836 
 largely unaffected}{\insrsid4981357  }{\insrsid16646278\charrsid16407836 by the subjects' diversity. It is also clearly shown that gaze}{\insrsid4981357  }{\insrsid16646278\charrsid16407836 
tracking significantly improves both the accuracy and compactness of}{\insrsid4981357  }{\insrsid16646278\charrsid16407836 the obtained models, if compared with the use of the hand position}{\insrsid4981357  }{\insrsid16646278\charrsid16407836 
alone. The system can be trained }{\insrsid4981357 with something like 3.5}{\insrsid16646278\charrsid16407836  minutes of
\par human data in the worst case.
\par 
\par }{\insrsid12609387 INTRODUCTION}{\insrsid16646278\charrsid16407836 
\par 
\par Semi-autonomous teleoperation consists of having a robotic setup (the}{\insrsid16126363  }{\i\insrsid16646278\charrsid12609387 slave}{\insrsid16646278\charrsid16407836 ) work in a remote environment, guided by a human user}{\insrsid16126363  }{
\insrsid16646278\charrsid16407836 (the }{\i\insrsid16646278\charrsid12609387 master}{\insrsid16646278\charrsid16407836 ). In a basic setting, the cooperation between}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
master and slave is realised in such a way that the actions performed}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 by the master can be precisely and timely replicated by the slave. In}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
order to convey a feeling of telepresence, in particular, a high
\par bandwidth is required for the slave-to-master sensorial feedback}{\insrsid16126363  }{\insrsid678333 [BTR00]}{\insrsid16646278\charrsid16407836 . But, when any of the above conditions fails,}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
teleoperation must be somehow augmented. The slave could consist of a}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 set of surgical tools }{\insrsid678333 [Oka04]}{\insrsid16646278\charrsid16407836  not immediately related to the}{
\insrsid16126363  }{\insrsid16646278\charrsid16407836 human fingers; or, the master could be a disabled person; or, lastly,}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 the master/slave communication bandwidth could be insufficient for a}{
\insrsid16126363  }{\insrsid16646278\charrsid16407836 timely and accurate transmission of sensorial feedback.
\par 
\par One of the possibilities to overcome these problems is that of making}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 the slave more intelligent by buildin}{\insrsid4478300 g into it }{\i\insrsid4478300\charrsid4478300 internal models}{
\i\insrsid16126363  }{\insrsid16646278\charrsid16407836 of the required actions }{\insrsid678333 [Kaw99]}{\insrsid16646278\charrsid16407836 . It is envisioned, in this}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
scenario, that the master should first train the slave to perform the}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 actions required, in a safe and controlled environment; and that this}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
acquired knowledge should then be used by the slave in real}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 situations, whenever the environment or the master's abilities do not
\par allow direct control. Upon detecting the master's intention to, e.g.,}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 grasp an object in its own workspace, the slave should take control}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
over, initiate and complete a grasping action possibly modelled upon}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 the user's style, and then return the control to the master. This is}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
what we call}{\insrsid4478300  }{\i\insrsid4478300\charrsid4478300 semi-autonomous grasping}{\insrsid16646278\charrsid16407836 .
\par 
\par As a minimum set of requirements, such a model should be}{\insrsid16126363  }{\i\insrsid16646278\charrsid4478300 accurate}{\insrsid16646278\charrsid16407836  (it should be able to tell exactly when to start an}{\insrsid16126363  }{
\insrsid16646278\charrsid16407836 autonomous grasp, avoiding doing it when not required), }{\i\insrsid16646278\charrsid4478300 fast}{\i\insrsid4478300 ,}{\i\insrsid16126363  }{\insrsid16646278\charrsid16407836 since it must be used in real-time, and }{
\i\insrsid16646278\charrsid4478300 flexible: }{\insrsid16646278\charrsid16407836 it must adapt}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 well to different subjects' parameters (speed of reach, direction of}{\insrsid16126363  }{
\insrsid16646278\charrsid16407836 motion), abilities and intentions, and it must be trained in a}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 reasonably short amount of time.
\par 
\par In this paper we investigate the possibility of building such an}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 internal model, employing a machine learning system based upon a}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
Support Vector Machine (SVM) }{\insrsid678333 [BGV92]}{\insrsid16646278\charrsid16407836 . In }{\insrsid678333 [CS07]}{\insrsid16646278 ,}{\insrsid16646278\charrsid16407836  we}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
already showed some promising results related to the problem; here we}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 refine the data analysis and show the results of an experiment in}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
which seven subjects, of different ages and with slightly different}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 movement and gaze abilities, were placed in a real teleoperation}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
scenario; the subjects would then teach a SVM to recognise when they}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 wanted to grasp an object in the slave setup by simply and naturally}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
fixating the object, reaching for it and closing their hand. Here we}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 carry the data analysis to the end.
\par 
\par Data was gathered from the subjects using a magnetic tracker for the}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 position of the hand, and a gaze tracker in order to understand}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
whether the subject was fixating a particular object. One interesting}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 question is whether gaze tracking can improve the situation. The}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
outcome of the experiment is that such a model can actually be built,}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 and that it fulfills all the requirements enumerated above: it is}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
highly accurate; the solution achieved is extremely compact; and these}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 characteristics are largely independent of the subjects'}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
abilities. The training phase i}{\insrsid4478300 s accomplished using about 3.5}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 minutes of data gathered in real-time from each subject, in the worst}{\insrsid16126363  }{
\insrsid16646278\charrsid16407836 case. The use of gaze tracking significantly improves the obtained}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 models, both as far as the accuracy and the size of the models is}{\insrsid16126363  }{
\insrsid16646278\charrsid16407836 concerned.
\par 
\par %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\par 
\par }{\insrsid4478300 MATERIALS AND METHODS}{\insrsid16646278\charrsid16407836 
\par 
\par }{\insrsid4478300 Subjects}{\insrsid16646278\charrsid16407836 
\par 
\par Seven subjects, four women }{\insrsid4478300 and three men, aged 30 to 73}{\insrsid16646278\charrsid16407836 ,}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 volunteered to join the experiment. They were all right-handed and}{\insrsid16126363  }{
\insrsid16646278\charrsid16407836 fully able-bodied, and were given no knowledge of the aim of the}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 experiment. Four of the subjects were slightly visually}{\insrsid16126363  }{
\insrsid16646278\charrsid16407836 impaired.
\par 
\par }{\insrsid4478300 Setup and devices}{\insrsid16646278\charrsid16407836 
\par 
\par The subjects were asked to sit confortably in front of a clean}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 workspace, and a flat 17 inches color monitor was placed in front of}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
them at a distance of about half a meter. They wore an Immersion}{\insrsid16126363  }{\i\insrsid4478300\charrsid4478300 CyberGlove}{\insrsid16646278\charrsid16407836  data glove }{\insrsid678333 [Vir98]}{\insrsid16646278\charrsid16407836 
 on their right hand,}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 and an Ascension }{\i\insrsid16646278\charrsid4478300 Flock-of-Birds}{\insrsid16646278\charrsid16407836  (FoB) }{\insrsid678333 [Asc99]}{\insrsid16646278\charrsid16407836 
 magnetic}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 tracker was firmly mounted on top of their wrist. Lastly, an ASL}{\insrsid16126363  }{\i\insrsid4478300\charrsid4478300 E504}{\insrsid16646278\charrsid16407836  gaze tracker }{
\insrsid678333 [App01]}{\insrsid16646278\charrsid16407836  was placed on the left hand side}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 of the monitor. Figure }{\insrsid6845808 1}{\insrsid16646278\charrsid16407836  shows the devices and setup.

\par }{\insrsid16646278 
\par }{\insrsid66679 --------------------------------------}{\insrsid16646278\charrsid16407836 
\par }{\insrsid8151142 Figure 1: }{\insrsid16646278\charrsid16407836 The devices and setup used for the experiment: (a) the}{\insrsid8151142  }{\insrsid16646278\charrsid16407836 Immersion CyberGlove with the Flock-of-Birds sensor just above the}{
\insrsid8151142  w}{\insrsid16646278\charrsid16407836 rist; (b) the ASL E504 gaze tracker (pan/tilt near-infrared}{\insrsid8151142  }{\insrsid16646278\charrsid16407836 camera); (c) the whole setup.}{\insrsid16646278  }{\insrsid16646278\charrsid16407836 

\par }\pard \s15\ql \li0\ri0\widctlpar\wrapdefault\aspalpha\aspnum\faauto\adjustright\rin0\lin0\itap0\pararsid66679 {\insrsid66679 --------------------------------------}{\insrsid66679\charrsid16407836 
\par }\pard \s15\ql \li0\ri0\widctlpar\wrapdefault\aspalpha\aspnum\faauto\adjustright\rin0\lin0\itap0\pararsid16407836 {\insrsid16646278\charrsid16407836 
\par The FoB returns six double-precision numbers describing the position}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 (x, y and z in inches) and rotation (azimuth, elevation and roll}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
in degrees) of the sensor with respect to a magnetic basis mounted}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 about one meter away from the subject. Its resolution is 0.1 inches}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
and 0.5 deg}{\insrsid6845808 rees. The E504, after a careful }{\insrsid16646278\charrsid16407836 calibration phase,}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 returns one true/false value, denoting validity of the gaze}{\insrsid16126363  }{
\insrsid16646278\charrsid16407836 coordinates, and two double-precision numbers indicating the}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 coordinates of the subject's gaze with respect to the monitor. The}{\insrsid16126363  }{
\insrsid16646278\charrsid16407836 CyberGlove was used as an on-off switch, to detect when the subject's}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 hand would close, by monitoring one of its sensors via a threshold.}{\insrsid6845808  }{
\insrsid16646278\charrsid16407836 The monitor showed the slave's workspace; the slave is the humanoid}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 platform Babybot }{\insrsid678333 [NOB+05]}{\insrsid16646278\charrsid16407836 
. For the experiment we only}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 employed one of its colour cameras.
\par 
\par All data were collected, synchronised, and saved in real time at a}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 frequency of about 47Hz.
\par 
\par }{\insrsid4478300 Method}{\insrsid16646278\charrsid16407836 
\par 
\par The subjects were asked to initially keep their right hand and arm in}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 a resting position. The monitor showed the slave's workspace, in which}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
several objects could be clearly seen, and a moving red cross}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 corresponding to the detected subject's gaze. The subjects were then}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
instructed, upon a request by the experimenter, to look at one of the}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 objects on the monitor and then to move their hand as if to reach and}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
grasp it, signalling the act of grasping by closing their right}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 hand. The red cross on the screen turned green when the hand was}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
closed, to confirm the grasping.
\par 
\par This fake grasping act was repeated for 15 to 21 times, each time}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 with a different object (therefore, toward a different position) seen}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
on the monitor. The maximum duration of the whole experiment for a}{\insrsid16126363  }{\insrsid4478300 single subject was about 3.5}{\insrsid16646278\charrsid16407836  minutes, resulting in no tiredness.
\par 
\par }{\insrsid4478300 Building the data set}{\insrsid16646278\charrsid16407836 
\par 
\par The first question was what to monitor from the subjects'}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 data. A few intuitive considerations led us to consider (a) the}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 average of th}{
\insrsid4478300 e subjects' hand velocity, (b)}{\insrsid16646278\charrsid16407836  the variance of the}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 subj}{\insrsid4478300 ects' gaze coordinates and (c)}{\insrsid16646278\charrsid16407836 
 the information whether the}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 subjects' right hand was open or closed. When the subject wants to}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 grasp an object, he/she first }{
\i\insrsid16646278\charrsid4478300 fixates}{\insrsid16646278\charrsid16407836  the desired object and}{\insrsid16126363  }{\insrsid4478300 then }{\i\insrsid4478300\charrsid4478300 reaches}{\i\insrsid16646278\charrsid4478300  }{\insrsid678333 
for it (see, e.g., [JWBF01]}{\insrsid16646278\charrsid16407836 ); lastly, at}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 some time after the beginning of the reaching movement, he/she will}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
close the right hand as if to grasp, as instructed.
\par 
\par We then expect, while fixating, the gaze coordinates to hover around}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 the point on the screen where the desired object is seen, that is,}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
their standard deviation o}{\insrsid4478300 ver some time to be }{\i\insrsid4478300\charrsid4478300 small}{\i\insrsid16646278\charrsid4478300 ; }{\insrsid16646278\charrsid16407836 also we}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
expect, while reaching, the hand to move toward the object on the}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 screen, that is, the hand velocity components to be on average}{\insrsid16126363  }{\i\insrsid4478300\charrsid4478300 large}{
\i\insrsid16646278\charrsid4478300 . }{\insrsid16646278\charrsid16407836 The instants in which the hand is closed will signal the
\par intention to grasp, whereas those when the hand is open will be taken}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 as negative examples. Data (a) were easily obtained by}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
differentiating in time the hand position x,y,z coordinates obtained}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 fro}{\insrsid4478300 m the FoB, while (b) and (c)}{\insrsid16646278\charrsid16407836  were obtained straight from the}{
\insrsid16126363  }{\insrsid16646278\charrsid16407836 E504 and the CyberGlove. (The samples corresponding to negative values}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 of the E504 validity flag were ignored, manually verifying that this}{
\insrsid16126363  }{\insrsid16646278\charrsid16407836 would not hamper the overall statistics.)
\par 
\par From each subje}{\insrsid4478300 ct we obtained a sequence of 6}{\insrsid16646278\charrsid16407836 -tuples (the three hand}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 velocity coordinates, the two gaze coordinates and the open/closed}{
\insrsid16126363  }{\insrsid16646278\charrsid16407836 hand flag). The above considerations should be valid }{\i\insrsid16646278\charrsid2173926 over a}{\i\insrsid16126363\charrsid2173926  }{\i\insrsid16646278\charrsid2173926 certain time window}{
\insrsid16646278 ,}{\insrsid16646278\charrsid16407836  characteristic of the fixation/reaching}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 operations --- call it }{\f59\insrsid3960257 \'f4}{\insrsid16646278\charrsid16407836 
; and in general each subject will have a}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 different }{\f59\insrsid3960257 \'f4}{\insrsid16646278\charrsid16407836 (i), i=1,}{\insrsid3960257 ...,7}{\insrsid16646278\charrsid16407836 
. Driven by this, we then decided to}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 feed the learning system the following data: for each user i (and}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
therefore for each sequence) and for a range of different values T}{\sub\insrsid3960257\charrsid3960257 c}{\sub\insrsid16126363  }{\insrsid16646278\charrsid16407836 attributed to }{\f59\insrsid3960257 \'f4}{\insrsid16646278\charrsid16407836 (i), the }{
\i\insrsid16646278\charrsid3960257 hand velocity average values}{\insrsid16646278\charrsid16407836  over}{\insrsid16126363  }{\insrsid3960257\charrsid16407836 T}{\sub\insrsid3960257\charrsid3960257 c}{\insrsid16646278\charrsid16407836 
 (three real numbers) and the }{\i\insrsid16646278\charrsid3960257 gaze position standard}{\i\insrsid16126363  }{\i\insrsid16646278\charrsid3960257 deviations}{\insrsid16646278\charrsid16407836  over }{\insrsid3960257\charrsid16407836 T}{
\sub\insrsid3960257\charrsid3960257 c}{\insrsid16646278\charrsid16407836  (two real numbers). Training was enforced by}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 requiring that the the system could guess, instant by instant, whether}{
\insrsid16126363  }{\insrsid16646278\charrsid16407836 the hand was closed or not. This was represented as an integer value,}{\insrsid16126363  }{\insrsid3960257 in turn 1 or -1}{\insrsid16646278\charrsid16407836 
. The problem of guessing when the subject wants to}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 grasp was thus turned into a typical supervised learning problem.
\par 
\par }{\insrsid3960257 Grasping speed}{\insrsid16646278\charrsid16407836 
\par 
\par In choosing the range for }{\insrsid3960257\charrsid16407836 T}{\sub\insrsid3960257\charrsid3960257 c}{\insrsid16646278\charrsid16407836 , we were driven by the main}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
consideration that a moving time window should not be longer then the}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 interval of time between one grasping attempt and the following}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
one. In fact, a longer time window could trick the system into}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 considering data obtained during two or more independent grasping}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 attempts.
\par 
\par By examining all sequences we found out that the interval between one}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 grasping attempt and the following one}{\insrsid15414259  lasted on average 7.1\'b1}{\insrsid3960257 1.8}{\insrsid16126363  }{
\insrsid16646278\charrsid16407836 seconds. We then decided to let }{\insrsid3960257\charrsid16407836 T}{\sub\insrsid3960257\charrsid3960257 c}{\insrsid16646278\charrsid16407836  range in the interval}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
0.1,}{\insrsid3960257 ...}{\insrsid16646278\charrsid16407836 ,5 seconds.
\par 
\par In general, we expected to fin}{\insrsid3960257 d a best minimum value for }{\insrsid3960257\charrsid16407836 T}{\sub\insrsid3960257\charrsid3960257 c}{\insrsid16646278\charrsid16407836 , which}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 would 
then be the required }{\f59\insrsid3960257 \'f4}{\insrsid3960257\charrsid16407836 (i)}{\insrsid16646278\charrsid16407836  for each user, figuring out that}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
shorter values would convey too little information about the ongoing}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 movement, and that for longer ones, the moving averages would reach a}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
plateau effect, tending to the overall average values of the hand}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 velocity and gaze standard deviations.
\par 
\par }{\insrsid16646278\charrsid3960257 Support Vector Machines
\par }{\insrsid16646278\charrsid16407836 
\par Our machine learning system is based upon Support Vector Machines}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 (SVMs). Introduced in the early 90s by Boser, Guyon and Vapnik}{\insrsid16126363  }{\insrsid678333 [BGV92]}{\insrsid16646278 ,}{
\insrsid16646278\charrsid16407836  SVMs are a class of kernel-based learning algorithms}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 deeply rooted in Statistical Learning Theory }{\insrsid678333 [Vap98]}{\insrsid16646278 ,}{
\insrsid16646278\charrsid16407836  now}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 extensively used in, e.g., speech recognition, object classification}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
and function approximation with good results }{\insrsid678333 [CST00]}{\insrsid16646278\charrsid16407836 . For}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 an extensive introduction to the subject, see, e.g., }{\insrsid678333 [Bur98]}{
\insrsid16646278\charrsid16407836 .
\par 
\par }{\insrsid7630495 In a SVM, b}{\insrsid16646278\charrsid16407836 oth the training time}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 (i.e., the time required by the training phase) and the testing time}{\insrsid16126363  }{
\insrsid16646278\charrsid16407836 (i.e., the time required to find the value of a point not }{\insrsid7630495 previously seen}{\insrsid16646278\charrsid16407836 )}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
crucially depend on the total number of support vectors; therefore,}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 this number is an indicator of how hard the problem is. Since the}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
number of support vectors is proportional to the sample set}{\insrsid16126363  }{\insrsid678333 [Ste03]}{\insrsid16646278 ,}{\insrsid16646278\charrsid16407836  an even better indicator of the hardness of the}{\insrsid16126363  }{
\insrsid16646278\charrsid16407836 problem is the percentage of support vectors with respect to the}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 sample set size. We will call }{\insrsid7630495 this quantitity the }{
\i\insrsid16646278\charrsid7630495 size}{\insrsid16646278  }{\insrsid16646278\charrsid16407836 of the related model. Willing to implement the}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
system on-line, one has to choose models with the smallest possible}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 size.
\par 
\par There are two parameters to be tuned in our setting}{\insrsid7630495 , called }{\insrsid16646278\charrsid16407836 C}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 and }{\f59\insrsid7630495 \'f3}{\insrsid16646278\charrsid16407836 
. In all our tests we found the optimal values of C and}{\insrsid16126363  }{\f59\insrsid7630495 \'f3}{\insrsid16646278\charrsid16407836  by grid search with }{\insrsid7630495 5}{\insrsid16646278\charrsid16407836 -fold cross-validation. This ensures}{
\insrsid16126363  }{\insrsid16646278\charrsid16407836 that the obtained models will have a high generalisation power.
\par 
\par We have employed LIBSVM v2.82 }{\insrsid678333 [CL01]}{\insrsid16646278 ,}{\insrsid16646278\charrsid16407836  a standard, efficient}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 implementation of SVMs.
\par 
\par According to the procedure described in the previous parts of this}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 Section, we decided to set up a SVM for each user and value of the}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
time window }{\insrsid6845808\charrsid16407836 T}{\sub\insrsid6845808\charrsid3960257 c}{\insrsid16646278\charrsid16407836 . Willing to compare the performance with and without}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
the use of the gaze signal, we defined }{\insrsid7630495 R}{\super\insrsid16646278\charrsid7630495 3}{\insrsid16646278\charrsid16407836  as the input space in}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 the case of
 not using the gaze (the 3 numbers representing the hand}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 average velocity over }{\insrsid6845808\charrsid16407836 T}{\sub\insrsid6845808\charrsid3960257 c}{\insrsid16646278\charrsid16407836 ), and }{
\insrsid7630495 R}{\super\insrsid7630495\charrsid7630495 5}{\insrsid16646278\charrsid16407836  in the case of using the}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 gaze (the 5 numbers representing the hand velocity average and gaze}{
\insrsid16126363  }{\insrsid16646278\charrsid16407836 position standard deviation over }{\insrsid6845808\charrsid16407836 T}{\sub\insrsid6845808\charrsid3960257 c}{\insrsid16646278\charrsid16407836 ).
\par 
\par %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\par 
\par }{\insrsid16126363 EXPERIMENTAL RESULTS}{\insrsid16646278\charrsid16407836 
\par 
\par In }{\insrsid678333 [CS07]}{\insrsid16646278 ,}{\insrsid16646278\charrsid16407836  we already showed that SVMs clearly outperform a}{\insrsid808761  }{\insrsid16646278\charrsid16407836 simple decision tree in solving the problem; so we will be using SVMs}
{\insrsid808761  }{\insrsid16646278\charrsid16407836 alone in this paper. Morover, it was therein noted that the standard}{\insrsid808761  }{\insrsid16646278\charrsid16407836 measure of performance for a SVM classifier (that is, the fraction of}{
\insrsid808761  }{\insrsid16646278\charrsid16407836 correctly guessed labels over the total number of samples) is biased,}{\insrsid808761  }{\insrsid16646278\charrsid16407836 at least in two ways: firstly, the number of 1 labels is much}{\insrsid808761  }
{\insrsid16646278\charrsid16407836 smaller than that of -1 labels; as a consequence, a dumb predictor}{\insrsid808761  }{\insrsid16646278\charrsid16407836 which guessed -1 identically would achieve an average accuracy of
\par about 83%. Therefore we adopt a weigthed measure of accuracy,}{\insrsid808761  }{\insrsid7630495 in which every correctly guessed label is weighted inversely to the number of times it occurs in the test set.}{\insrsid7630495\charrsid16407836  }{
\insrsid16646278\charrsid16407836 
\par 
\par Secondly, we are looking for both accurate and small}{\insrsid16646278  }{\insrsid16646278\charrsid16407836 models;}{\insrsid808761  }{\insrsid16646278\charrsid16407836 this means that looking for the most accurate models could lead to}{\insrsid808761  }{
\insrsid16646278\charrsid16407836 unnecessarily large models. Therefore, we adopt here an index of}{\insrsid808761  }{\insrsid16646278\charrsid16407836 performance given by the ratio of the weighted accuracy detailed above}{\insrsid808761  }{
\insrsid16646278\charrsid16407836 and the percentage of support vectors in the obtained model. This}{\insrsid808761  }{\insrsid16646278\charrsid16407836 index was used for grid }{\insrsid7630495 searching the best values of C}{
\insrsid16646278\charrsid16407836  and }{\f59\insrsid7630495 \'f3}{\insrsid16646278\charrsid16407836 .
\par 
\par Figure }{\insrsid66679 2}{\insrsid16646278\charrsid16407836  shows the weighted accuracy and size of}{\insrsid808761  }{\insrsid16646278\charrsid16407836 the obtained models as }{\insrsid6845808\charrsid16407836 T}{\sub\insrsid6845808\charrsid3960257 c}{
\insrsid16646278\charrsid16407836  is increased, both when using and not}{\insrsid808761  }{\insrsid16646278\charrsid16407836 using the gaze.
\par 
\par }\pard \s15\ql \li0\ri0\widctlpar\wrapdefault\aspalpha\aspnum\faauto\adjustright\rin0\lin0\itap0\pararsid66679 {\insrsid66679 --------------------------------------}{\insrsid66679\charrsid16407836 
\par }\pard \s15\ql \li0\ri0\widctlpar\wrapdefault\aspalpha\aspnum\faauto\adjustright\rin0\lin0\itap0\pararsid16407836 {\insrsid66679 Figure 2:
\par }\pard \s15\ql \li0\ri0\widctlpar\wrapdefault\aspalpha\aspnum\faauto\adjustright\rin0\lin0\itap0\pararsid66679 {\insrsid16646278\charrsid16407836 Comparison between not using the gaze (red curve) and}{\insrsid66679  }{\insrsid16646278\charrsid16407836 
using the gaze (black one) as }{\insrsid6845808\charrsid16407836 T}{\sub\insrsid6845808\charrsid3960257 c}{\insrsid16646278\charrsid16407836 
 increases. Top panel: weighted accuracy. Bottom panel: size of the models, expressed as the fraction of Support Vectors. Dots are the mean values over all}{\insrsid66679  }{\insrsid16646278\charrsid16407836 subjects, whereas the errorbars denote}{
\insrsid15414259  plus/minus }{\insrsid16646278\charrsid16407836 one standard deviation.}{\insrsid66679 
\par --------------------------------------}{\insrsid66679\charrsid16407836 
\par }\pard \s15\ql \li0\ri0\widctlpar\wrapdefault\aspalpha\aspnum\faauto\adjustright\rin0\lin0\itap0\pararsid16407836 {\insrsid16646278\charrsid16407836 
\par First of all, the effect predicted in the previous Section is present:}{\insrsid6845808  }{\insrsid16646278\charrsid16407836 both the accuracy and size of the models become better as }{\insrsid6845808\charrsid16407836 T}{
\sub\insrsid6845808\charrsid3960257 c}{\insrsid16646278\charrsid16407836  is}{\insrsid6845808  }{\insrsid16646278\charrsid16407836 increased, and reach a maximum around }{\insrsid6845808\charrsid16407836 T}{\sub\insrsid6845808\charrsid3960257 c}{
\insrsid6845808 =}{\insrsid7630495 2}{\insrsid16646278\charrsid16407836  seconds; then they}{\insrsid6845808  }{\insrsid16646278\charrsid16407836 remain essentially constant. Secondly, notice that the use of the gaze}{\insrsid6845808  }{
\insrsid16646278\charrsid16407836 uniformly and consistently improves both the accuracy of the models}{\insrsid6845808  }{\insrsid16646278\charrsid16407836 and their size, the black curve being systematically higher than the}{\insrsid6845808  }{
\insrsid16646278\charrsid16407836 red one, as far as the accuracy is concerned, and lower in the case of}{\insrsid6845808  }{\insrsid16646278\charrsid16407836 the model size. For }{\insrsid6845808\charrsid16407836 T}{\sub\insrsid6845808\charrsid3960257 c}
{\insrsid6845808 >2}{\insrsid16646278\charrsid16407836 , there is little overlap among the}{\insrsid6845808  }{\insrsid16646278\charrsid16407836 errorbars, indicating a stastically significant improvement. Thirdly,}{\insrsid6845808  }{
\insrsid16646278\charrsid16407836 it seems that such a value of }{\insrsid6845808\charrsid16407836 T}{\sub\insrsid6845808\charrsid3960257 c}{\insrsid16646278\charrsid16407836  is just about right for all}{\insrsid6845808  }{
\insrsid16646278\charrsid16407836 subjects, notwithstanding their }{\insrsid808761 d}{\insrsid16646278\charrsid16407836 ifferences; we then conclude that}{\insrsid6845808  }{\f59\insrsid7630495 \'f4(i)}{\insrsid16646278\charrsid16407836 
 is essentially the same for all subjects. This could}{\insrsid6845808  }{\insrsid16646278\charrsid16407836 dramatically reduce the setup time in a real setting.
\par 
\par For }{\insrsid808761\charrsid16407836 T}{\sub\insrsid808761\charrsid3960257 c}{\insrsid808761 =2}{\insrsid16646278\charrsid16407836  seconds, the accuracy is 96.79}{\insrsid8615838 %}{\insrsid15414259 \'b1}{\insrsid16646278\charrsid16407836 1.7}{
\insrsid8615838 %}{\insrsid16646278\charrsid16407836  using the}{\insrsid808761  }{\insrsid16646278\charrsid16407836 gaze, as opposed to 93.69}{\insrsid8615838 %}{\insrsid15414259 \'b1}{\insrsid16646278\charrsid16407836 3.24}{\insrsid8615838 %}{
\insrsid16646278\charrsid16407836  when not using it; the model}{\insrsid808761  }{\insrsid16646278\charrsid16407836 size is 5.27}{\insrsid8615838 %}{\insrsid15414259 \'b1}{\insrsid16646278\charrsid16407836 2.12}{\insrsid8615838 %}{
\insrsid16646278\charrsid16407836  as opposed to 8.88}{\insrsid8615838 %}{\insrsid15414259 \'b1}{\insrsid16646278\charrsid16407836 2.73}{\insrsid8615838 %}{\insrsid16646278\charrsid16407836 . The}{\insrsid808761  }{\insrsid16646278\charrsid16407836 
mean values are better, and the standard deviations are smaller when}{\insrsid808761  }{\insrsid16646278\charrsid16407836 using the gaze, denoting better performance and higher robustness with}{\insrsid808761  }{\insrsid16646278\charrsid16407836 
respect to the diversity of the subjects.}{\insrsid808761  }{\insrsid16646278\charrsid16407836 
\par Let us then turn to the experiment with the gaze, and analyse the}{\insrsid808761  }{\insrsid16646278\charrsid16407836 performance in deeper detail for the single subjects. Figure}{\insrsid808761  }{\insrsid66679 3}{\insrsid16646278\charrsid16407836 
 shows the same results as the black curves of Figure}{\insrsid808761  }{\insrsid66679 2,}{\insrsid16646278\charrsid16407836  but for each subject.
\par 
\par }\pard \s15\ql \li0\ri0\widctlpar\wrapdefault\aspalpha\aspnum\faauto\adjustright\rin0\lin0\itap0\pararsid66679 {\insrsid66679 --------------------------------------}{\insrsid66679\charrsid16407836 
\par }\pard \s15\ql \li0\ri0\widctlpar\wrapdefault\aspalpha\aspnum\faauto\adjustright\rin0\lin0\itap0\pararsid16407836 {\insrsid66679 Figure 3:
\par }{\insrsid16646278\charrsid16407836 Accuracy (a) and size (b) of the models for each single subject,}{\insrsid66679  }{\insrsid16646278\charrsid16407836 using the gaze.}{\insrsid16646278  }{\insrsid16646278\charrsid16407836 
\par }\pard \s15\ql \li0\ri0\widctlpar\wrapdefault\aspalpha\aspnum\faauto\adjustright\rin0\lin0\itap0\pararsid66679 {\insrsid66679 --------------------------------------}{\insrsid66679\charrsid16407836 
\par }\pard \s15\ql \li0\ri0\widctlpar\wrapdefault\aspalpha\aspnum\faauto\adjustright\rin0\lin0\itap0\pararsid16407836 {\insrsid16646278\charrsid16407836 
\par Consider the Figure, pane (a): it is apparent that the worst subject}{\insrsid5329716  }{\insrsid7630495 is number 5}{\insrsid16646278\charrsid16407836 , a 73-years old woman who has undergone in}{\insrsid5329716  }{\insrsid16646278\charrsid16407836 
the past a cataract surgical operation. It is not surprising that this}{\insrsid5329716  }{\insrsid16646278\charrsid16407836 is the hardest subject; still, for }{\insrsid7630495\charrsid16407836 T}{\sub\insrsid7630495\charrsid3960257 c}{\insrsid7630495 =2
}{\insrsid16646278\charrsid16407836 , the model has a remarkable}{\insrsid5329716  }{\insrsid16646278\charrsid16407836 accuracy of 93.03}{\insrsid8615838 %}{\insrsid16646278\charrsid16407836 . All other subjects reach an accuracy of}{\insrsid5329716  }{
\insrsid16646278\charrsid16407836 96.75}{\insrsid8615838 %}{\insrsid16646278\charrsid16407836  and more for the same value of }{\insrsid6845808\charrsid16407836 T}{\sub\insrsid6845808\charrsid3960257 c}{\insrsid16646278\charrsid16407836 . Analogous}{
\insrsid5329716  }{\insrsid16646278\charrsid16407836 considerations apply as far as the mod}{\insrsid7630495 el size is concerned (pane (b)}{\insrsid5329716  }{\insrsid16646278\charrsid16407836 
of the same Figure). It is interesting to note that, even for such a}{\insrsid5329716  }{\insrsid16646278\charrsid16407836 hard subject as subject 5, the use of the gaze signal greatly}{\insrsid5329716  }{\insrsid16646278\charrsid16407836 
improves the accuracy of the}{\insrsid66679  model (see Figure 4}{\insrsid16646278\charrsid16407836 ).
\par 
\par }\pard \s15\ql \li0\ri0\widctlpar\wrapdefault\aspalpha\aspnum\faauto\adjustright\rin0\lin0\itap0\pararsid66679 {\insrsid66679 --------------------------------------}{\insrsid66679\charrsid16407836 
\par }\pard \s15\ql \li0\ri0\widctlpar\wrapdefault\aspalpha\aspnum\faauto\adjustright\rin0\lin0\itap0\pararsid16407836 {\insrsid66679 Figure 4:
\par }{\insrsid16646278\charrsid16407836 Comparison between not using the gaze (red curve) and using the gaze (black one) as }{\insrsid6845808\charrsid16407836 T}{\sub\insrsid6845808\charrsid3960257 c}{\insrsid16646278\charrsid16407836 
 increases, for subject 5.}{\insrsid16646278  }{\insrsid16646278\charrsid16407836 
\par }\pard \s15\ql \li0\ri0\widctlpar\wrapdefault\aspalpha\aspnum\faauto\adjustright\rin0\lin0\itap0\pararsid66679 {\insrsid66679 --------------------------------------}{\insrsid66679\charrsid16407836 
\par }\pard \s15\ql \li0\ri0\widctlpar\wrapdefault\aspalpha\aspnum\faauto\adjustright\rin0\lin0\itap0\pararsid16407836 {\insrsid16646278\charrsid16407836 
\par Cross-subject analysis}{\insrsid16646278  }{\insrsid16646278\charrsid16407836 
\par 
\par One further interesting point is: how well can these models be}{\insrsid5329716  }{\insrsid16646278\charrsid16407836 transferred across subjects? In other words: are models trained on a}{\insrsid5329716  }{\insrsid7630495 certain subject i}{
\insrsid16646278\charrsid16407836  good for predicting the intention of another}{\insrsid5329716  }{\insrsid16646278\charrsid16407836 subject j? Does gaze improve the situation? And, what are the}{\insrsid5329716  }{\insrsid7630495 \'93}{
\insrsid16646278\charrsid16407836 hardest}{\insrsid7630495 \'94}{\insrsid16646278\charrsid16407836  subjects to be modelled? In order to answer these}{\insrsid5329716  }{\insrsid16646278\charrsid16407836 questions we have tested each model for }{
\insrsid7105999\charrsid16407836 T}{\sub\insrsid7105999\charrsid3960257 c}{\insrsid7105999 =2}{\insrsid16646278\charrsid16407836  on data gathered from}{\insrsid5329716  }{\insrsid16646278\charrsid16407836 
all subjects, both when using and not using the gaze. Figure}{\insrsid5329716  }{\insrsid66679 5}{\insrsid16646278\charrsid16407836  shows the results as cross-accuracy matrices: in each}{\insrsid5329716  }{\insrsid7105999 matrix A, the entry A}{
\sub\insrsid16646278\charrsid7105999 ij}{\insrsid16646278\charrsid16407836  represents in colour the accuracy}{\insrsid5329716  }{\insrsid16646278\charrsid16407836 attained by }{\insrsid7105999 the model trained on subject i}{
\insrsid16646278\charrsid16407836  when tested on data}{\insrsid5329716  }{\insrsid7105999 gathered from subject j}{\insrsid16646278\charrsid16407836 .
\par 
\par }\pard \s15\ql \li0\ri0\widctlpar\wrapdefault\aspalpha\aspnum\faauto\adjustright\rin0\lin0\itap0\pararsid66679 {\insrsid66679 --------------------------------------}{\insrsid66679\charrsid16407836 
\par }\pard \s15\ql \li0\ri0\widctlpar\wrapdefault\aspalpha\aspnum\faauto\adjustright\rin0\lin0\itap0\pararsid16407836 {\insrsid66679 Figure 5:
\par }{\insrsid16646278\charrsid16407836 Cross-subject accuracy, not using the gaze (left pane) and using the gaze (right pane).}{\insrsid16646278  }{\insrsid16646278\charrsid16407836 
\par }\pard \s15\ql \li0\ri0\widctlpar\wrapdefault\aspalpha\aspnum\faauto\adjustright\rin0\lin0\itap0\pararsid66679 {\insrsid66679 --------------------------------------}{\insrsid66679\charrsid16407836 
\par }\pard \s15\ql \li0\ri0\widctlpar\wrapdefault\aspalpha\aspnum\faauto\adjustright\rin0\lin0\itap0\pararsid16407836 {\insrsid16646278\charrsid16407836 
\par As is apparent, using the gaze improves the situation: the overall}{\insrsid5329716  }{\insrsid16646278\charrsid16407836 accuracy (not considering the diagonal elements of the matrices, of}{\insrsid5329716  }{\insrsid16646278\charrsid16407836 
course) is 55.97}{\insrsid8615838 %}{\insrsid15414259 \'b1}{\insrsid16646278\charrsid16407836 8.6}{\insrsid8615838 %}{\insrsid16646278\charrsid16407836  when not using the gaze, as opposed to}{\insrsid5329716  }{\insrsid16646278\charrsid16407836 61.32}{
\insrsid8615838 %}{\insrsid15414259 \'b1}{\insrsid16646278\charrsid16407836 11.77}{\insrsid8615838 %}{\insrsid16646278\charrsid16407836  when using the gaze. In general these figures}{\insrsid5329716  }{\insrsid16646278\charrsid16407836 
are not very good, meaning that there is little chance that models can}{\insrsid5329716  }{\insrsid16646278\charrsid16407836 be transferred across subjects, even when using the gaze.
\par 
\par Let us now restrict to the experiment with the gaze (right pane of the}{\insrsid5329716  }{\insrsid16646278\charrsid16407836 Figure). The worst model (row of the matrix) is, unsurprisingly,}{\insrsid5329716  }{\insrsid16646278\charrsid16407836 
obtained from subject number 5, with an accuracy of 49.31}{\insrsid8615838 %}{\insrsid15414259 \'b1}{\insrsid16646278\charrsid16407836 2.8}{\insrsid8615838 %}{\insrsid16646278\charrsid16407836 ; as well, the hardest subject (column of the matrix) is again
}{\insrsid5329716  }{\insrsid7105999 number 5}{\insrsid16646278\charrsid16407836 , with an accuracy of }{\insrsid7105999 56.70}{\insrsid8615838 %}{\insrsid15414259 \'b1}{\insrsid7105999 9.09}{\insrsid8615838 %}{\insrsid7105999 . Subject 5}{
\insrsid16646278\charrsid16407836  has}{\insrsid5329716  }{\insrsid16646278\charrsid16407836 to be treated on her own. But as well, again, let us remark that the}{\insrsid5329716  }{\insrsid16646278\charrsid16407836 
two best models, obtained from subjects 2 (73.99}{\insrsid8615838 %}{\insrsid15414259 \'b1}{\insrsid16646278\charrsid16407836 7.89}{\insrsid8615838 %}{\insrsid16646278\charrsid16407836 ) and}{\insrsid5329716  }{\insrsid16646278\charrsid16407836 6 (70.32}{
\insrsid8615838 %}{\insrsid15414259 \'b1}{\insrsid16646278\charrsid16407836 6.61}{\insrsid8615838 %}{\insrsid16646278\charrsid16407836 ) still show a poor trasnferability as well}{\insrsid5329716  }{\insrsid16646278\charrsid16407836 
--- compare these figures with those of the previous Subsection.
\par 
\par %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\par 
\par }{\insrsid16126363 DISCUSSION AND CONCLUSIONS}{\insrsid16646278\charrsid16407836 
\par 
\par }{\insrsid14369561 
\par }{\insrsid16646278\charrsid16407836 In this paper we have shown that good artificial internal models of}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 when to grasp can be obtained by applying Support Vector Machines to}{\insrsid16126363  }{
\insrsid16646278\charrsid16407836 data gathered from diverse human subjects, engaged in a simple}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 grasping experiment in a teleoperation scenario. The data consisted of}{\insrsid16126363  }{
\insrsid16646278\charrsid16407836 the hand velocity and the gaze signal, which was proved to be crucial}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 in improving the performance of the models, both in terms of accuracy}{\insrsid16126363  }{
\insrsid16646278\charrsid16407836 and size.
\par 
\par Interestingly, about the same characteristic time value for }{\insrsid6845808\charrsid16407836 T}{\sub\insrsid6845808\charrsid3960257 c}{\insrsid16646278\charrsid16407836  can}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
be found for all subjects, the best models being found for }{\insrsid6845808\charrsid16407836 T}{\sub\insrsid6845808\charrsid3960257 c}{\insrsid6845808 =2}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
seconds. The system greatly benefits from the use of gaze tracking,}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 also in case the subjects are}{\insrsid7105999  visually impaired (subject 5}{\insrsid16646278\charrsid16407836 ).
\par 
\par The models obtained for each subject are (a) h}{\insrsid16126363 ig}{\insrsid16646278\charrsid16407836 hly accurate, giving}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 the correct guess in 96.79}{\insrsid8615838 %}{\insrsid15414259 \'b1}{
\insrsid16646278\charrsid16407836 1.7}{\insrsid8615838 %}{\insrsid16646278\charrsid16407836  of the cases; and (b)}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 small, and therefore fast and usable in an on-line environment, the}{
\insrsid16126363  }{\insrsid16646278\charrsid16407836 percentage of support vectors for each model being 5.27}{\insrsid8615838 %}{\insrsid15414259 \'b1}{\insrsid16646278\charrsid16407836 2.12}{\insrsid8615838 %}{\insrsid16646278\charrsid16407836 
. Analogous figures when the gaze is not used are sensibly}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 worse. The accuracy and size of the best models for each subject are}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
remarkable, meaning that the system is also very flexible and}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 unhindered by the subjects' diverse abilities and visual impairments.
\par 
\par Cross-subject transferability of models is, on the other hand, so far}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 unfeasible, the figures showing that models trained on a subject}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
cannot in general obtain good accuracy values when applied to data}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 gathered from different subjects. But this does not seem of any}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
hindrance to the approach, since the models can be trained, in the}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 worst case, us}{\insrsid16126363 ing 3.5 minutes of user data. I}{\insrsid16646278\charrsid16407836 ndependent training for}{
\insrsid16126363  }{\insrsid16646278\charrsid16407836 each subject can then be performed with a reasonable effort.
\par 
\par %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\par }{\insrsid16126363 
\par ACKNOWLEDGMENTS}{\insrsid16646278\charrsid16407836 
\par 
\par The work is supported by the EU-funded project NEURObotics}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 (FP6-IST-001917). We thank Giulio Sandini and Giorgio Metta of the}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 
Italian Institute of Technology and Francesco Orabona of IDIAP}{\insrsid16126363  }{\insrsid16646278\charrsid16407836 Switzerland for their support.
\par }{\insrsid16407836 
\par }\pard \s15\ql \li0\ri0\widctlpar\wrapdefault\aspalpha\aspnum\faauto\adjustright\rin0\lin0\itap0\pararsid16646278 {\insrsid16646278\charrsid16407836 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\par }{\insrsid16646278 
\par }\pard \s15\ql \li0\ri0\widctlpar\wrapdefault\aspalpha\aspnum\faauto\adjustright\rin0\lin0\itap0\pararsid16407836 {\insrsid16646278 BIBLIOGRAPHY
\par 
\par }\pard \s15\ql \li0\ri0\widctlpar\wrapdefault\aspalpha\aspnum\faauto\adjustright\rin0\lin0\itap0\pararsid16646278 {\insrsid16646278 [App01]}{\insrsid14881444  }{\insrsid16646278 Applied Science Laboratories, 175 Middlesex Turnpike, Bed
ford, MA 01730. Eye Tracking System Instruction Manual --- Model 504 Pan/Tilt Optics --- v2.4, 2001.
\par [Asc99]}{\insrsid14881444  }{\insrsid16646278 Ascension Technology Corporation, PO Box 527, Burlington (VT), USA. The Flock of Birds --- Installation and operation guide, January 1999.
\par [BGV92]}{\insrsid14881444  }{\insrsid16646278 
B. E. Boser, I. M. Guyon, and V. N. Vapnik. A training algorithm for optimal margin classifiers. In D. Haussler, editor, Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory, pages 144--152. ACM press, 1992.
\par [BTR00] Pattaraphol Batsomboon, Sabri Tosunoglu, and Daniel W. Repperger. A survey of telesensation and teleoperation technology with virtual reality and force reflection capabilities. International Journal of Modeling and Simulation, 20(1):79---88, 2000.

\par [Bur98]}{\insrsid14881444  }{\insrsid16646278 Christopher J. C. Burges. A tutorial on support vector machines for pattern recognition. Knowledge Discovery and Data Mining, 2(2), 1998.
\par [CL01]}{\insrsid14881444  }{\insrsid16646278 Chih-Chung Chang and Chih-Jen Lin. LIBSVM: a library for support vector machines, 2001. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm.
\par [CS07]}{\insrsid14881444  }{\insrsid16646278 
Claudio Castellini and Giulio Sandini. Learning when to grasp. Invited paper at Concept Learning for Embodied Agents, a workshop of  ICRA 2007, Rome, Italy. Available at http://people.liralab.it/drwho/publications/2007.CLEA.pdf, 2007.
\par [CST00]}{\insrsid14881444  }{\insrsid16646278 N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines (and Other Kernel-Based Learning Methods). CUP, 2000.
\par [JWBF01]}{\insrsid14881444  }{\insrsid16646278 R. S. Johansson, G. Westling, A. Baeckstroem, and J. R. Flanagan. Eye-hand coordination in object manipulation. J. Neurosci., 21:6917---6932, 2001.
\par [Kaw99]}{\insrsid14881444  }{\insrsid16646278 M. Kawato. Internal models for motor control and trajectory planning. Current Opinion in Neurobiology, 9:718--727, 1999.
\par [NOB05]}{\insrsid14881444  }{\insrsid16646278 L. Natale, F. Orabona, F. Berton, G. Metta, and G. Sandini. From sensorimotor development to object perception. In  Proceedings of the International Conference on Humanoid Robotics, Humanoids 2005, 2005.

\par [Oka04]}{\insrsid14881444  }{\insrsid16646278 A. M. Okamura. Methods for haptic feedback in teleoperated robot-assisted surgery. Industrial robot, 31(6):499---508, 2004.
\par [Ste03]}{\insrsid14881444  }{\insrsid16646278 I. Steinwart. Sparseness of support vector machines. Journal of Machine Learning Research, 4:1071--1105, 2003.
\par [Vap98]}{\insrsid14881444  }{\insrsid16646278 Vladimir N. Vapnik. Statistical Learning Theory. John Wiley and Sons, New York, 1998.
\par [Vir98]}{\insrsid14881444  }{\insrsid16646278 Virtual Technologies, Inc., 2175 Park Blvd., Palo Alto (CA), USA. CyberGlove Reference Manual, August 1998.
\par }}