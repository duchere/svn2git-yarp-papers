In this paper we have shown that good artificial internal models of
when to grasp can be obtained by applying Support Vector Machines to
data gathered from diverse human subjects, engaged in a simple
grasping experiment in a teleoperation scenario. The data consisted of
the hand velocity and the gaze signal, which was proved to be crucial
in improving the performance of the models, both in terms of accuracy
and size.

Interestingly, about the same characteristic time value for $T_c$ can
be found for all subjects, the best models being found for $T_c=2$
seconds. The system greatly benefits from the use of gaze tracking,
also in case the subjects are visually impaired (subject $5$).

The models obtained for each subject are $(a)$ highly accurate, giving
the correct guess in $96.79\% \pm 1.7\%$ of the cases; and $(b)$
small, and therefore fast and usable in an on-line environment, the
percentage of support vectors for each model being $5.27\% \pm
2.12\%$. Analogous figures when the gaze is not used are sensibly
worse. The accuracy and size of the best models for each subject are
remarkable, meaning that the system is also very flexible and
unhindered by the subjects' diverse abilities and visual impairments.

Cross-subject transferability of models is, on the other hand, so far
unfeasible, the figures showing that models trained on a subject
cannot in general obtain good accuracy values when applied to data
gathered from different subjects. But this does not seem of any
hindrance to the approach, since the models can be trained, in the
worst case, using $3.5$ minutes of user data. Independent training for
each subject can then be performed with a reasonable effort.
