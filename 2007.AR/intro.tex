One of the most distinguishing features of cognitive systems is the
ability to \emph{learn to predict} the future course of actions and
the results of ongoing behaviours, and in general to plan actions well
in advance. It is now relatively well understood that the brain builds
models of the physical world through learning. These models are
sometimes called ``internal models'', meaning that they are the
internal rehearsal (or simulation) of the world enacted by the brain
\cite{kawato-99}.

Interestingly, these models are built not only because they are
required to control movements, but also, as it has been determined
more recently, to interpret the movements of others \cite{wolpert-03,
mussaivaldi-00, lackner-98}. There is now a large body of literature
that links the observation of actions to action execution, like for
example the study of the motor system conducted by Rizzolatti and
colleagues in relatively recent years
\cite{rizzolatti-04,gallese-96,rizzolatti-01}. It seems then that
building predictions of the future course of action is a key feature 
of intelligent living systems.

Moreover, it has been shown in the context of object grasping, that
the efficiency of a model of grasping can be improved by using
knowledge on the object to be grasped as priors \cite{lopes-05,
metta-06}, i.e., the presence of a target object and its geometrical
properties strongly constrain the type of grasp and the approach to
the object, and, as a consequence, the brain might need to include
this information when planning an appropriate course of action
\cite{sakata-95}.

In this paper we set forth to investigate whether a computer, equipped
with enough sensory information about human movements, namely
grasping, could acquire a specialized model using machine
learning methods. In particular we ask $(a)$ whether the final
configuration of the hand, i.e., at the very moment an object is
grasped, could be predicted from the initial part of the movement; and
$(b)$ whether the knowledge of the object to be grasped could improve
the model efficiency, leading to a smaller error in prediction.
It is worth noting that we do not want to necessarily mimic the structure
of the brain, but rather more simply analyze the human movement data 
with the best possible algorithm available.

To shed light on these questions, we have set up an experiment in
which several able-bodied subjects have performed a highly repetitive
grasping task on various daily life objects, and we have collected
data about their hand position, orientation and posture. Then we have
tried to put a computer in the same situation a human observer would
be if he were to see only the initial part of a grasping action, the
final part being occluded by a screen: a sub-sequence of each grasping
sequence, namely the initial segment a human observer would be able to
see, was used to train an efficient machine learning system based on
Support Vector Machines.

We have then analyzed the error in predicting the final hand
configuration; and we have analyzed whether the \emph{a priori}
knowledge of the grasped object makes a difference in performance as
it should intuitively do. The results we present here, albeit still in
a preliminary form, indicate that the machine is able to predict
reasonably well human reaching and grasping, and that prior knowledge
of the object to be grasped improves the performance of the machine,
while keeping the same computational cost.

Once actually realised, optimised and implemented, such 
models could potentially be used in various ways including the control
of semi-autonomous teleoperated / prosthetic robotic artifacts, the
interpretation and possibly mimicry of human movements
\cite{wolpert-01}. For example, in controlling or teleoperating an
anthropomorphic robotic platform, such models would be able to guess
the user intention and ask the robot to complete the action
autonomously. Predicting the user intention finds its natural role in
building man-machine interfaces and possibly into the control of
prosthetic devices.

The paper is structured as follows: after a brief review of related
work, we describe the methods and the experimental setup in section
\ref{sec:exp_desc} and the results obtained in section
\ref{sec:exp_res}; lastly we discuss them and comment on future
development in section \ref{sec:Conclusions}.

\subsection{Related work}

In the monkey, premotor area F5 has been particularly well studied and
it is in fact the location where ``mirror neurons'' were first
identified. In this respect, mirror neurons are the quintessential
correlate of internal models since they are
activated both when executing a specific grasping action and when
observing a congruent action being executed by another individual (or
the experimenter) \cite{fadiga-00}.

In a study by Umilt\`a et al. \cite{umilta-01} the response of mirror
neurons to the observation of actions that terminate behind a screen
has been investigated. In this case, the authors analyzed mirror
neurons in situations where the final part of the trajectory of the hand was occluded
by an opaque screen with the monkey knowing of the presence/absence of
an object to be grasped. As long as the object was shown to the
monkey, the brain could easily supply the missing visual information
by rehearsing the model of the action. The control
experiment, in this case, was that of an identical hand kinematics, an
identical screen but the absence of the target object, that is,
identical visual stimulation apart from the knowledge of the presence
of the object. Elsewhere it has been also shown that the presence of
an object is required to elicit the mirror neurons response in the
monkey \cite{gallese-96}.

{\em A posteriori}, given these results, it is easy to see how the
presence of a target object and its geometrical properties strongly
constrain the type of grasp and the approach to the object, and that,
as a consequence, the brain might need to include this information
when planning an appropriate course of action. In the monkey these
constraints are so strong that mirror neurons do not fire unless the
goal of the action is clearly perceivable. The brain codes for the
object-motor identity in part via another class of F5 visuomotor
neurons called ``canonical neurons'' (for a discussion see for example
\cite{metta-06}). To complete the picture, the work of Graziano,
Hu, and Gross \cite{graziano-97} has shown that the presence of
objects is coded in the ventral premotor cortex and maintained even
when the object is no longer visible as long as there is evidence for
its presence at a particular location.

Relevant to this discussion, the work of Fogassi et
al. \cite{fogassi-05} contributed to the identification of mirror
neurons in the parietal cortex (inferior parietal lobule), which are
thought to be related to the decoding of the intentions of
others. Contextual information which links the enacted action to its
final goal seems to be implicated in this type of neural response. The
presence of objects is a clear contextual cue. In humans, it has been
demonstrated that the activation of brain areas correlated to action
observation is not simply a perceptual effect but rather the
activation of a precise sensorimotor model which includes for example
the hand kinematics \cite{pozzo-06} and a precise muscular pattern of
activations \cite{borroni-05}.
 
Accordingly, Fadiga et al. \cite{fadiga-99,vargas-04} have shown that
motor imagery changes the excitability of the cortico-spinal
connections specifically to the imagined action, that is, imagining a
motor task causes the under-threshold activation of the same neural
pathways required to execute the task. This under-threshold activation
was revealed by transcranial magnetic stimulation. In a conceptually
similar experiment \cite{fadiga-05}, the excitability of
cortico-spinal pathways was also examined as a consequence of the
actual sensory input. In summary, the motor system is similarly
activated when acting in first person, when imagining an action, or
when watching somebody else's action. Jeannerod \cite{jeannerod-88},
for example, goes to a great length in showing how plausible is the
fact that mental imagery uses the same internal models used by actual
action generation.  It is known in this respect that the time required
to simulate an action is the same that is required to execute that
action \cite{sirigu-96}. For a review refer to \cite{jeannerod-99}.

As far as gesture / hand configuration recognition is concerned, in a
previous experiment we have analyzed the problem of recognizing hand
gestures visually by incorporating a generative approach that used
motor information explicitly \cite{lopes-05,metta-06}. In that case we
showed that an action recognition system that uses motor information
in a preprocessing step can perform better ($97\%$ recognition rate
versus $80\%$ on the test set) than a traditional classifier built
directly in terms of visual information. This justifies the fact that
as a preprocessing step we can consider a visuo to motor mapping that
transforms the available visual information into motor data. This
procedure is consistent in that it can be trained through
self-observation. We can imagine that the brain can exercise its
control and simultaneously acquire both the motor commands and the
corresponding visual information and learn such a mapping. In the
following, we will only consider motor information since we can safely
assume that the visuo-motor map can be always incorporated in the global
model.

Lastly, machine learning has already been used to \emph{classify} the
types of grasps (e.g., in \cite{degranville,heumer,ekvall}); but, to
the best of our knowledge, nobody has applied it in order to perform
\emph{regression} on the position and configuration of the hand.
