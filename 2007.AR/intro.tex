It is reasonably well established that the brain stores ``internal
models'' of actions either because they are required to control
movements or, as it has been determined more recently, to interpret
the movements of others \cite{kawato-99, wolpert-03, mussaivaldi-00,
lackner-98}. There is a large body of literature that links the
observation of actions to action execution, like for example the study
of the motor system conducted by Rizzolatti and colleagues in
relatively recent years
\cite{rizzolatti-04,gallese-96,rizzolatti-01}. In the monkey, premotor
area F5 has been particularly well studied and it is in fact the
location where ``mirror neurons'' were first identified. In this
respect, mirror neurons are the quintessential correlate of internal
models since they are activated both when executing a specific
grasping action and when observing a congruent action being executed
by another individual (or the experimenter)
\cite{fadiga-00}.

In a study by Umilt\`a et al. \cite{umilta-01} the response of mirror
neurons to the observation of actions that terminate behind a screen
has been investigated. In this case, the authors analyzed mirror
neurons in situations where the final part of the action was occluded
by an opaque screen with the monkey knowing of the presence/absence of
an object to be grasped. As long as the object was shown to the
monkey, the brain could easily supply the missing visual information
by rehearsing the internal model of the action. The control
experiment, in this case, was that of an identical hand kinematics, an
identical screen but the absence of the target object, that is,
identical visual stimulation apart from the knowledge of the presence
of the object. Elsewhere it has been also shown that the presence of
an object is required to elicit the mirror neurons response in the
monkey \cite{gallese-96}.

{\em A posteriori}, given these results, it is easy to see how the
presence of a target object and its geometrical properties strongly
constrain the type of grasp and the approach to the object, and that,
as a consequence, the brain might need to include this information
when planning an appropriate course of action. In the monkey these
constraints are so strong that mirror neurons do not fire unless the
goal of the action is clearly perceivable. The brain codes for the
object-motor identity in part via another class of F5 visuomotor
neurons called ``canonical neurons'' (for a discussion see for example
\cite{metta-06}). To complete the picture, the work of Graziano,
Hu, and Gross \cite{graziano-97} has shown that the presence of
objects is coded in the ventral premotor cortex and maintained even
when the object is no longer visible as long as there is evidence for
its presence at a particular location.

Relevant to this discussion, the work of Fogassi et
al. \cite{fogassi-05} contributed to the identification of mirror
neurons in the parietal cortex (inferior parietal lobule), which are
thought to be related to the decoding of the intentions of
others. Contextual information which links the enacted action to its
final goal seems to be implicated in this type of neural response. The
presence of objects is a clear contextual cue. In humans, it has been
demonstrated that the activation of brain areas correlated to action
observation is not simply a perceptual effect but rather the
activation of a precise sensorimotor model which includes for example
the hand kinematics \cite{pozzo-06}.
 
Accordingly, Fadiga et al. \cite{fadiga-99,vargas-04} have shown that
motor imagery changes the excitability of the cortico-spinal
connections specifically to the imagined action, that is, imagining a
motor task causes the under-threshold activation of the same neural
pathways required to execute the task. This under-threshold activation
was revealed by transcranial magnetic stimulation. In a conceptually
similar experiment \cite{fadiga-05}, the excitability of
cortico-spinal pathways was also examined as a consequence of the
actual sensory input. In summary, the motor system is similarly
activated when acting in first person, when imagining an action, or
when watching somebody else's action. Jeannerod \cite{jeannerod-88},
for example, goes to a great length in showing how plausible is the
fact that mental imagery uses the same internal models used by actual
action generation.  It is known in this respect that the time required
to simulate an action is the same that is required to execute that
action \cite{sirigu-96}. For a review refer to \cite{jeannerod-99}.

In this paper we set forth to investigate whether a machine equipped
with enough sensory information about human movement could acquire a
similar ``internal model'' via machine learning methods. In
particular, we investigated whether the final configuration of the arm
and hand can be predicted from the initial part of the movement by
imagining that the final part of the action is occluded by a
screen. We analyzed how much error we make in predicting the final
position of the fingers joint angles and the configuration of the
wrist in space: i.e., position and orientation of a coordinate system
fixed with respect to the palm of the hand. We also analyzed whether
the knowledge of the grasped object makes a difference in performance
as it should intuitively do.

In a previous experiment, we analyzed the problem of recognizing hand
gestures visually by incorporating a generative approach that used
motor information explicitly \cite{lopes-05,metta-06}. In that case we
showed that an action recognition system that uses motor information
in a preprocessing step can perform better ($97\%$ recognition rate
versus $80\%$ on the test set) than a traditional classifier built
directly in terms of visual information. This justifies the fact that
as a preprocessing step we can consider a visuo to motor mapping that
transforms the available visual information into motor data. This
procedure is consistent in that it can be trained through 
self-observation. We can imagine that the brain can exercise its
control and simultaneously acquire both the motor commands and the
corresponding visual information and learn such a mapping. In the
following, we will only consider motor information since we can safely
assume that the visuo-motor map can always incorporated in the global
internal model.

If actually realised, such internal models could potentially be used
in various ways including the control of semi-autonomous teleoperated
/ prosthetic robotic artifacts, the interpretation of human movements,
and clearly they can be used to reproduce and mimic human movements
\cite{wolpert-01}. For example, in controlling or teleoperating an
anthropomorphic robotic platform, we can guess the user intention and
ask the robot to complete the action autonomously. Predicting the user
intention finds its natural role in building man-machine interfaces
and possibly into the control of prosthetic devices.

The paper is structured as follows: we describe the methods and the
experimental setup in section \ref{sec:exp_desc} and the results
obtained in section \ref{sec:exp_res}; lastly we discuss them and
comment on future development in section \ref{sec:Conclusions}.
