Our goal in classification is to demonstrate that the motor information is
useful in object learning and recognition. Specifically,  we want 
to show that integrating it with the visual information can 
produce a better performance, namely higher classificaton 
rate and robusteness.

To this end we consider both the visual and the motor features
labelled in terms of objects. The idea is that a classifier 
should predict which is the inspected object when the input is
 visual, motor or the combination 
of the two.
Algorithmically, this implies building a classifier over multiple cues.

In the computer vision and pattern recognition literature 
some authors have suggested different methods to combine
multiple cues. They can be all reconducted to one of the 
following three approaches: low-level, mid-level and high-level
integration \cite{Polikar2006,sanderson2004}. 
In the low-level case the features are concatenated
to define a single vector. In the mid-level approach
the different features descriptor are kept separated 
but they are integrated in a single classifier generating the
final hypothesis. The high-level method starts from
the output of different classifiers each dealing with one feature: 
the hypotheses produced are then combined together to
achieve a consensus decision.

To learn the Visuo-Motor Classifier here we decided to implement these three strategies in an SVM-based framework, and to evaluate
experimentally their suitability for the task. Specifically, 
we used the
Discriminative Accumulation Scheme (DAS, \cite{DAS}) for
the high-level, and the Multi-Cue Kernel (MCK, \cite{MCK}) for the
mid-level integration.
As already mentioned, the low-level integration consisted only in the feature concatenation, with the new vector fed to a standard SVM.


\vspace{0.5cm}

\noindent\textbf{DAS.} It is based on a weak coupling method called accumulation. Its main 
idea is that information from different cues can be summed together.

Suppose we are given $M$ object classes and for each class, a set
of $N_{j}$ training data $\{I^{j}_{i}\}_{i=1}^{N_{j}}$,
$j=1,\ldots M$. For each, we have a set of $P$ different
features so that for an object $j$ we have $P$ new training sets.
We train an SVM on every set. Kernel functions may differ from cue to
cue and model parameters can be estimated during the training step
via cross validation. Given a test image $\hat{I}$ and assuming
$M\geq2$, for each single-cue SVM we compute the distance from the
separating hyperplane $D_{j}(p)$, $p=1\ldots P$.
After collecting all the distances $\{D_{j}(p)\}_{p=1}^{P}$ for
all the $M$ objects  and the $P$ cues, we classify the image
$\hat{I}$ using the linear combination:
\begin{equation}
j^{*}=\argmax_{j=1}^{M} \left \{\sum_{p=1}^{P}a_{p}D_{j(p)} \right \}, \quad
\sum_{p=1}^{P}a_{p}=1. \label{eq:DAS}
\end{equation}
The coefficients $\{a_{p}\}_{p=1}^{P}\in \Re^{+}$ are determined
via cross validation during the training step.

\vspace{0.5cm}

\noindent\textbf{MCK.} The Multi Cue Kernel is positively
weighted linear combination of Mercer kernels, thus a Mercer kernel itself:
\begin{equation}
K_{MC}({\{T_{p}(I_{i})\}}_{p},{\{T_{p}(I)\}}_{p})=\sum_{p=1}^{P}a_{p}K_{p}(T_{p}(I_{i}),T_{p}(I)),
\; \sum_{p=1}^{P}a_{p}=1.\label{eq:MCK}
\end{equation}
In this way it is possible to perform only one classification
step, identifying the best weighting factors $a_{p}\in \Re^{+}$
through cross validation while
determining the optimal separating hyperplane. This means that the
coefficients $a_{p}$ are guaranteed to be optimal. 
 


%Here we take a mid-level approach, and we propose to use the Multi Cue Kernel
%within an SVM classifier \cite{MCK}. This method has been shown to outperform state of the art
%high- and low-level integration methods.
%
%Suppose we are given $M$ object classes and for each class, a set
%of $N_{j}$ training data $\{I^{j}_{i}\}_{i=1}^{N_{j}}$,
%$j=1,\ldots M$. For each, we have a set of $P$ different
%features so that for an object $j$ we have $P$ new training sets.
%We train an SVM on the whole set, using the Multi cue Kernel which is defined as:
%\begin{equation}
%K_{MC}({\{T_{p}(I_{i})\}}_{p},{\{T_{p}(I)\}}_{p})=\sum_{p=1}^{P}a_{p}K_{p}(T_{p}(I_{i}),T_{p}(I)),
%\; \sum_{p=1}^{P}a_{p}=1.\label{eq:MCK}
%\end{equation}
%The Multi Cue Kernel is a positively
%weighted linear combination of Mercer kernels, thus a Mercer kernel itself.
%By using this mid-level integration strategy, 
%it is possible to perform only one classification
%step, identifying the best weighting factors $a_{p}\in \Re^{+}$
%through cross validation while
%determining the optimal separating hyperplane. This means that the
%coefficients $a_{p}$ are guaranteed to be optimal.
%
%To assess our results, we implemented also a low-level and a high-level, SVM-based approaches.
%For the low-level case, we simply concatenated the features to form a single vector, and fed it as input to an SVM. 
%For the high-level case, we chose the Discriminative Accumulation Scheme (DAS, \cite{DAS}) that showed very high 
%performances on several object
%recognition and robot vision applications \cite{DAS, pronobis_etal_icra2008}.
%
%


