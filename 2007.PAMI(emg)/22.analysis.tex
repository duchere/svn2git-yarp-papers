The gathered data was analysed both for classification and
regression. \emph{Classification} is the process by which one wants to
assign a label to each sample in the input space, whereas in
\emph{regression} the target is a real-valued function of the values
of the input samples. All machine learning methods we examined need to
be first trained on a set of points in the input space for which the
target (label or force value) is known; this set will be called
\emph{training set}. Then, in order to verify that the obtained models
are good, they are tested on a separate set of points, the
\emph{testing set}. In all our experiments, the training and testing
sets were disjoint, and $5$-fold cross-validation was employed to
check the generalisation power of the methods.

Taking into account the considerations of the previous Section, we set
the input space to be $\RR^{10}$, that is, one coordinate for each EMG
electrode; therefore, $\xx_i \in \RR^{10}, i=1,\ldots,l$. In the case
of classification, each category representing a grasping type would be
represented as an integer value, that is, $y_i \in \{0,\ldots,4\}
\subset \NN, i=1,\ldots,l$. In the case of regression, the force value
would be directly encoded as a real number, that is, $y_i \in \RR,
i=1,\ldots,l$. Before training, all samples in the training set were
normalised, as is customary, by subtracting the mean values and
dividing by the standard deviation, for each input space
dimension. Testing was done by normalising with the mean and standard
deviations found during training, as one should not assume to know any
a-priori statistics whatsoever about the testing samples. No filtering
whatsoever was applied to the input signal, in order to have a more
realistic, delay-free result.

\subsubsection{Neural Networks}

Feed-Forward Artificial Neural Networks (FFN; see, e.g.,
\cite{bishop} for a comprehensive introduction) are probably the most
popular machine learning algorithm nowadays available for both
classification and regression. %An ANN is a directed graph in which,
%for every node, the weighted sum of the input values is evaluated;
%this sum is then used as the argument of an \emph{activation function}
%to determine the output of the node. The nodes fed the input values to
%the network form the \emph{input layer}, and the nodes whose output is
%taken as the output of the network are called \emph{output
%layer}.
%
%An ANN is initialised with random weights; then, for every sample in
%the training set, the network output is evaluated and its error with
%respect to the target is considered. A minimisation algorithm is then
%employed to change the weights of the network, until the desired
%precision is reached.
For our experiment we strived to keep the FFN as
simple as possible. With 10 inputs, we chose a feed-forward neural network with
one hidden layer with $10$ units with
hyperbolic tangent (sigmoidal) activation function.  $5$ Outputs are
used for classification, each unit representing one category,
and one unit in the output layer for regression, the unit representing
the target force value. The network was trained using gradient
descent, with backpropagation being enforced via the quasi-Newton
algorithm (for classification) and the Levenberg-Marquardt algorithm
(for regression). 
The training phase was stopped either after $100$ epochs
or when the validation set would show an increasing error. For each
experiment, we repeated the training phase $10$ times and then
gathered the best model found, in order to overcome the well-known
problem of local minima. Validation was done on a disjoint subset of
the training samples.

\subsubsection{Support Vector Machines}

Support Vector Machines (SVMs; see, e.g.,
\cite{BGV92,Burges98,Cristianini00}) are a machine learning method
able to determine the best candidate function for a classification or
regression problem, drawn from a functional space induced by the
choice of a binary function between points in the input space,
$K(\xx_1,\xx_2)$, with $\xx_1, \xx_2 \in \RR^{10}$ in this case. $K$
is called \emph{kernel}. In the most general setting, the function
found is

\begin{equation} \label{eqn:sol}
  f(\xx) = \sum_{i=1}^l \alpha_i y_i K(\xx,\xx_i) + b
\end{equation}

\noindent where $b \in \RR$, whereas the $\alpha_i \in \RR$s are
Lagrangian coefficients obtained by solving a minimisation problem
whose cost functional is guaranteed to be convex. Because of this,
SVMs do not suffer from the problem of local minima. We employed a
well-known freely available SVM package, \emph{libsvm} v2.83
\cite{ChangL01}, in the Matlab wrapped flavour.


\textbf{Claudio: Please add some details about your SVM, rather than general text.  E.g., the C and sigma parameters? Indeed remove much of the }

\subsubsection{Locally Weighted Projection Regression}

Locally Weighted Projection Regression (LWPR---see \cite{lwpr}) is a
regression method especially targeted for high-dimensional
spaces with redundant and irrelevant input dimensions. It employs
locally linear models, each of which performs univariate regressions
in selected directions in the input space. It has a computational
complexity that is linear in the number of inputs, but due to its
incrementality it can take long time to train (as we verified it was
the case). Therefore we used it for regression only, and trained it
with the uniformisation procedure.

We used the latest stable C version of LWPR, kindly made available by
Stefan Klanke, wrapped in a Matlab command interface. We chose to use
the Radial Basis Function kernel and meta-learning, and then
performed $5$-fold cross-validation and found the initial values of
the distance metric for receptive fields by grid search.
