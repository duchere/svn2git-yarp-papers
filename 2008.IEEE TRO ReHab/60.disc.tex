The results hereby presented clearly show that the plain, old EMG
signal can be used to drive a mechanical hand / prosthetic hand in a
radically new way: finer, force-controlling, more dexterous. Our
results indicate that a machine learning approach such as Support
Vector Machines will effectively detect well-separated grasping
patterns in real time, as required by an amputee; at the same time,
the system will be able to detect how much force is involved in the
grasp. If the prosthesis has a sufficient number of DOFs, that is, it
can be position-controlled to mimic the required grasps, and if it can
be force-controlled, then our system will be able to control it in a
totally natural way, that is, according to what the patient wants it
to do.

The positioning of the electrodes, at least in the case of the three
patients who joined the experiment, is uniform and not related to any
anatomical consideration. Since the results we have obtained are
uniformly good for all patients, it is probably possible to claim that
careful positioning, as well as medical control to identify the best
working remaining muscles in the stump, is not required. This would
simplify the whole procedure.

The proposed analysis of SVM hyperparameters and percentage of Support
Vectors with respect to the total number of training samples (see
Section \ref{sec:exp}) indicates that the problem is rather easy, if
considered form the pure point of view of machine learning. This lets
us hope that, in case SVMs proved too hard to implement in practice,
even a less sophisticated method such as, e.g., nearest neighbours,
could obtain acceptable performance values. Anyway, our realistic
implementation analysis shows that there should be no problem in
miniaturising the system, and embedding it in a prosthesis, even a
commercially available one.

As far as training is concerned, it is worth noting that soon on-line
learning could be needed, especially since patients are not supposed
to remain in controlled conditions while wearing the prosthesis. In
previous or submitted work, we have already shown that the framework
is theoretically perfectly able to work in non-controlled conditions
(the so-called Daily-Life Activity analysis). As well, it has been
shown that a sparsification strategy such as \emph{uniformisation} can
enforce on-line learning of new situations at the price of degrading
the performance reasonably. It remains to adapt the proposed PC /
microcontroller architecture to such a continual retraining schema,
but this seems no big problem.

Lastly, a few notes from a rather medical point of view. In this work
we are making \emph{no statement at all} about the physiological
resemblance of the patterns we deal with, with respect to the muscular
patterns elicited for the same grasps in a healthy arm. Actually, we
have not investigated what the patient's stump muscles do when the
patient is asked to imagine, e.g., a pinch grip --- this is indeed a
very interesting issue, but is not the focus of this work. Since each
and every amputation is different from one another, and therefore each
stump has different muscular conditions, what we can hope for is that
the system works fine for a reasonable range of amputees and
stumps. Although we have only $3$ patients here, their diversity as
far as age, age of operation and type of amputation lets us hope for
the best. In the end, as long as for each patient, each grasp type or
posture corresponds to a different pattern, then we can detect it and
send the appropriate command to the prosthesis. The patient will then
be able to elicit from the prosthesis, say, a $5$N pinch grip, or a
$30$N power grasp, just by ``desiring'' so. This is what we mean by a
better quality of life for amputees and a shorter training time. The
keyword \emph{adaptive prosthetics} is meant, here, in the
machine-learning sense, that is: the prosthesis is trained upon the
patient's data and therefore adapts to her/him.

We have actually found it surprising that so much fine muscular
activity can be elicited from elderly patients, such as our subjects
$1$ and $2$, who, by the way, have been operated \emph{decades}
ago. Notwithstanding this, they are still perfectly able to produce,
e.g., two very different patterns for similar grips such as the index
/ thumb pinch grip and the tripodal grip. This lets us hope that
possibly even a simpler method than SVMs could be used without
degrading the performance too much, in case the miniaturisation of
this method proves too hard in practice. But actually, the analysis
performed in Section \ref{sec:impl} is very promising.

The use of three different modalities has proven effective in finding
the best way for each single patient to train the system. Actually, as
one can see by considering Table \ref{tab:results} again, subject $1$
obtains better results if he trains the system in the teacher
imitation modality, whereas subjects $2$ and $3$ make it work better
in the bilateral action and mirror-box modalities. It is unclear why
this is the case, and anyway a broader range of patients is required
to observe any stastically meaningful phenomenon, in order to possibly
relate the type of amputation to the preferred modality --- a
relationship which would be of great help when a new patient has to
train the system. Here we can note that subject $1$ is the one left
with the shortest stump. One possibility why he performs best in the
teacher imitation modality is then\footnote{personal communication
with Benoni B. Edin of the Ume\o a University, Sweden.\textbf{FIXME}}
that he presents the ``highest degree'' of de-afferentiation in his
forearm, since he misses most of it, as opposed to subjects $2$ and
$3$. Literature on de-afferentiated patients (see, e.g.,
\cite{...}) indicates that such a condition, at least in neuropathic
patients, eases tasks in which motor and visual perception are
de-coupled or contradictory; and this seems to be the case, since
subject $1$ obtains better results when he has to \emph{imitate
someone else} rather than figuring out his own movements.
