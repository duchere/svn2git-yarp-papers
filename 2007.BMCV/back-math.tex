Due to space limitations, this is a very quick account of SVMs --- the
interested reader is referred to \cite{Burges98} for a tutorial, and
to \cite{Cristianini00} for a comprehensive introduction to the
subject. Assume $\{\xx_i,y_i\}_{i=1}^l$, with $\xx_i \in \RR^m$ and
$y_i \in \{-1,1\}$, is a set of samples and labels drawn from an
unknown probability distribution; we want to find a function $f(\xx)$
such that $sign(f(\xx))$ best determines the category of any future
sample $\xx$. In the most general setting,

\begin{equation} \label{eqn:sol}
  f(\xx) = \sum_{i=1}^l \alpha_i y_i K(\xx,\xx_i) + b
\end{equation}

\noindent where $b \in \RR$ and $K(\xx_1,\xx_2) = \Phi(\xx_1)
\cdot \Phi(\xx_2)$, the \emph{kernel function}, evaluates inner
products between images of the samples through a non-linear mapping
$\Phi$. The $\alpha_i$s are Lagrangian coefficients obtained by
solving (the dual Lagrangian form of) the problem

\begin{eqnarray} \label{eqn:svm_primal}
  \min_{\ww,b}      & \frac{1}{2} ||\ww||^2 + C \sum_{i=1}^l \xi_i^p            \\
  \mbox{subject to} & y_i (\ww\cdot\xx_i + b) \geq 1-\xi_i            \nonumber \\
                    & \xi_i \geq 0                                    \nonumber
\end{eqnarray}

\noindent where $\ww$ defines a separating hyperplane
in the \emph{feature space}, i.e., the space where $\Phi$ lives,
whereas $\xi_i \in \RR$ are slack variables, $C \in \RR^+$ is an error
penalty coefficient and $p$ is usually $1$ or $2$. In practice, most
of the $\alpha_i$ are found to be zero after training; the vectors
with an associated $\alpha_i$ different from zero are called
\emph{support vectors}. Notice that, from (\ref{eqn:sol}), the testing
time of a new point is proportional to the number of SVs, hence
reducing the number of SVs implies reducing the testing time.

In the following, the term \emph{kernel dimension} will refer, as is
customary, to the dimension of the feature space. The kernel dimension
is related to the generalization power of the machine, and it depends
on the choice of the kernel itself. Widely used kernels include the
\emph{polynomial} one (finite-dimensional) and the \emph{Gaussian} one
(infinite-dimensional).
