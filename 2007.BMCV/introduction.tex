Place recognition is an open and highly challenging problem in
computer vision, especially when applied to mobile robotics in indoor
environments. Simply stated, the problem is that of determining what
room of a house or office a mobile robot is in, based upon what the
robot sees through one or more cameras. The problem is made very
difficult by at least three factors: $(a)$ the input space is huge,
since we deal with images, usually at a reasonable resolution and in
color; $(b)$ images of the same place can be quite different as
illumination conditions change and moving obstacles get in the way;
and $(c)$, recognition must be done on-line in real time, as the robot
is moving around.
The topic is  widely researched, but
incremental learning approaches have been so far mostly used for constructing the
geometrical map, or the environment representation, online\cite{emma:irca05, ljubjiana:icra02}.
Robustness to illumination changes, and more generally to realistic
visual variations in time, has been addresses in \cite{pronobis:iros06}, where it was showed that
a pure learning approach can be very effective 
%Quite clearly, only a machine learning approach can potentially solve
%the problem, given the amazing variability of the input data. A
%learning machine is adaptive in nature and, if properly trained, can
%tackle the variability of the input. Nevertheless, machine learning
%usually suffers when confronted with massive amounts of data and when
%asked to work on-line.
%A striking example of this is represented by Support Vector Machines
%(SVM), one of the most promising machine learning approaches
%nowadays. Introduced in the early 90s by Boser, Guyon and Vapnik
%\cite{BGV92}, SVMs are able to classify data drawn from an unknown
%probability distribution, given a set of training examples. As opposed
%to analogous methods such as, e.g., artificial neural networks, they
%have the main advantages that $(a)$ training is guaranteed to end up
%in a global minimum, $(b)$ their generalization power is theoretically
%well founded, $(c)$ they can easily work with highly dimensional,
%non-linear data, and $(d)$ the solution achieved is sparse. Due to
%these good properties, they have been now extensively used in, e.g.,
%speech recognition, object classification and function approximation
%\cite{Cristianini00}. On the other hand they have the disadvantage to
%``grow'' for ever \cite{Steinwart03}, that is 
%Recent work \cite{pronobis:iros06} has showed that a pure machine
%learning approach can be very effective 
for tackling the first two
issues: indeed it was demonstrated that an approach based upon Support
Vector Machines (SVM, see, e.g., \cite{BGV92}) \emph{in batch mode}
could achieve a remarkable robustness to illumination changes and
variability due to the normal use of the environments. At the same time
the work elicited the problem of the growth of the testing time when a bigger
training set was used, to have better recognition performances.
In fact, as far as the third issue is concerned, it is well known that
both the training and testing time of an SVM crucially depend on the number of samples
considered \cite{KeerthiCDC06}; as well, the number of Support Vectors
(SVs) found, which determine the complexity of the solution to the
problem, grows proportionally with respect to the number of samples
\cite{Steinwart03}. This makes the approach unsuitable, at least so
far, for on-line learning, where a potentially endless flow of data is
acquired by the machine. SVMs can be up to $50$ times slower than
other specialized approaches with similar performances~\cite{BurgesS96}.
Several exact and approximate approaches have been proposed so
far for simplifying the SVM decision function: see for instance
\cite{DownsGM01}, based upon linear independence of the SVs in the
feature space performed after training, and other after-training simplification methods (e.g. 
chapter 18.3 in \cite{SmolaS02} and \cite{nguyen2005}).
The exact
solution to online SVM learning was given by Cauwenberghs and Poggio
in 2000 \cite{CauwenberghsP00}, but their algorithm cannot be used to
reduce the number of SVs. In \cite{syed99incremental} and
\cite{pronobis:icvs06} approximate incremental
versions of the SVM are proposed, that also achieve a reduction of the
number of SVs with small degradation of their performances.

In this paper we propose an improvement to SVMs that we call Online
Independent Support Vector Machines (OISVMs). OISVMs incrementally
select ``basis vectors'' that are used to build the solution of the
SVM training problem, based upon \emph{linear independence in the
feature space}: vectors which are linearly dependent on already stored
ones are rejected, and a smart, incremental minimization algorithm is
employed to find the new minimum of the cost function. This keeps the
number of SVs much smaller than usual, reducing the complexity of the
solution and therefore both the training and testing time. 
Unsupervised rank reduction methods have been proposed
\cite{Baudat03} as well as supervised ones \cite{BachJordan2005}
that achieve the same goals, but
no application of these ideas appears so far, to the best of our
knowledge, in online settings.
This is particularly important since it
has been shown \cite{Steinwart03} that the SV set grows linearly with
the sample set; therefore, in an online setting, the size of a SVM
would grow indefinitely, and so would the testing time. 
Our experiments instead indicate that the number of basis
vectors of OISVMs does not grow linearly with the training set, but
reaches a limit and then stops growing. This result is theoretically
confirmed, e.g., in \cite{engel2004}, even in the case the feature
space is infinite-dimensional.

Such an approach is actually what is needed to tackle the problem of
place recognition in mobile robotics. To support this claim, we show
an extensive set of experimental results obtained by comparing SVMs
and OISVMs on a real-world place recognition problem in an indoor
environment. Data images are acquired continuously by two robot
platforms under different weather conditions and across a time span of
several months. Our results show that our method achieves a speed-up
of $3.5-2.3$ times with respect to the time required by the standard SVM,
respectively with $\chi^2$ kernel and matching kernel \cite{wallraven:iccv03},
while retaining essentially the same accuracy.

The paper is structured as follows: after an overview  of background
mathematics proper to SVMs,  Section \ref{sec:opt} describes OISVMs.
Section \ref{sec:exp} shows the experimental results and
lastly, in Section \ref{sec:concl}, conclusions are drawn and future
work is outlined.
