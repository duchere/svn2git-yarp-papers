Introduced in the early 90s by Boser, Guyon and Vapnik \cite{BGV92},
\emph{Support Vector Machines} (SVMs) are a class of machine learning
algorithms deeply rooted in Statistical Learning Theory
\cite{v-edbed-82}, able to classify data taken from an unknown
probability distribution, given a set of training examples. As opposed
to analogous methods such as, e.g., artificial neural networks, they
have the main advantages that $(a)$ training is guaranteed to end up
in a global minimum, $(b)$ their generalization power is theoretically
well founded, $(c)$ they can easily work with highly dimensional,
non-linear data, and $(d)$ the solution achieved is sparse. Due to
these good properties, they have been now extensively used in, e.g.,
speech recognition, object classification and function approximation
\cite{Cristianini00}. On the other hand they have the disadvantage to
``grow'' for ever \cite{Steinwart03}, that is the complexity of the solution and the testing
time grow proportionally with the number of training samples.
SVMs can be up to 50 slower of other specialized approaches with
similar performances \cite{BurgesS96}.

This problem can be particularly important if we consider the case in
which online learning must be performed. Online learning is a
scenario in which training data is provided one example at a time, as
opposed to the batch mode in which all examples are available at once.
In the case of, e.g.,
non-stationary data, online algorithms will generally perform better
since ambiguous information (i.e., whose distribution varies over
time) is present, and could not possibly be taken into account by the
batch algorithm. Online algorithms allow to incorporate additional
training data, when it is available, without re-training from scratch.

In an online setting there is no guarantee that the flow of data will
\emph{ever} cease; therefore, applying SVMs here looks appealing but
we need a way to constraint the complexity of the solution and, at the
same time, to respect the online nature of the problem. 

In this paper we propose a method of incrementally selecting ``basis
vectors'' that are used to build the solution of the SVM training problem,
based upon \emph{linear independence in the feature space}: vectors which are
linearly dependent on already stored ones are rejected, and a smart,
incremental minimization algorithm is employed to find the new minimum
of the cost function. Our experiments
indicate that SVMs employing this idea, that we call
\emph{Online Independent Support Vector Machines} (OISVMs), do not
grow linearly with the training set, as it was the case in
\cite{Steinwart03}, but reach a limit size and then stop growing
\cite{engel2004}. In the case of finite-dimensional feature spaces
they also \emph{keep the full accuracy of standard SVMs}; whereas in
the infinite-dimensional case, at the price of a negligible loss in
accuracy, one can tune the growing rate of the machine.

To support this claim, we show an extensive set of experimental
results obtained by comparing SVMs and OISVMs on standard benchmark
databases as well as on a real-world, online application coming from
robotic vision: place recognition in an indoor environment, from
sequences acquired by robot platforms under different weather
conditions and across a time span of several months. Our results show
that, on standard benchmarks, the accuracy of OISVMs can be up to only
$0.063\%$ worse than SVMs, being more than $20$ times faster;
whereas, on the real-world application,
we get a speed-up of $3$ times required by an approximated
incremental method, while retaining essentially the same accuracy.

The paper is structured as follows: after a review of the relevant
literature, Section \ref{sec:bg} gives an overview of background
mathematics proper to SVMs; in Section \ref{sec:opt} OISVMs are
described.  Section \ref{sec:exp}  shows the experimental results
and lastly, in Section \ref{sec:concl}, conclusions are drawn and future
work is outlined.
