In this paper we have shown a promising improvement to Support Vector
Machines, that we call Online Independent Support Vector Machines
(OISVM). OISVMs can effectively solve the problem of place recognition
by a mobile robot, at least in the experiment we have shown. OISVMs
were tested on the IDOL2 image database, which consists of image
sequences acquired by robot platforms under different weather
conditions and across a time span of several months; that is, in
various lighting conditions and object placements, and acquired by two
different robot platforms. OISVMs avoid using in the solution those
support vectors which are linearly dependent of previous ones in the
feature space. The optimization problem is solved via an incremental
algorithm which benefits of the small number of the basis vectors.

As far as we know, this method is different from all
analogous procedures presented so far in literature (e.g.,
\cite{DownsGM01,nguyen2005,LeeM01,schoel06,KeerthiCDC06}) since it
is \emph{not} an after-training simplification and it assumes
\emph{no knowledge whatsoever} of the full training set beforehand.
Moreover in case of finite-dimensional kernel and $\eta=0$, the
solution is exactly the same of the standard formulation because
there is any approximation is used.

Our experimental results show that in
the case of infinite-dimensional kernels, the number of support vectors
is dramatically reduced at the price of a negligible degradation in
the accuracy. In fact in the case of $\chi^2$ kernel, we get as few
as $3.5$ times less SVs respect to the batch formulation and $3$
times less respect to the fixed-partition method, while retaining
essentially the same accuracy. In the case of the local kernel...
[\textbf{COMPLETE AFTER WE HAVE THE FINAL RESULTS}]

Since the training and testing time depend polynomially on the
number of support vectors, reducing them bring an obvious speed up. A
careful study of the relationship between $\eta$ and the degradation
in performance is being carried on; in fact, according to
\cite{engel2004}, imposing a value of $\eta$ strictly larger than zero
will eventually result in a \emph{finite} number of basis vectors,
\emph{even in the case the feature space is
infinite-dimensional}. On going research is on finding a precise
relationship between $\eta$ and this number to allow
us to precisely dimension the machine depending on the required
precision.
