Visual place recognition for robot platforms is a widely researched
topic in which online learning is critical. Incremental learning
approaches have been so far mostly used for constructing the
geometrical map, or the environment representation, online. Brunskill
et~al. \cite{emma:irca05} proposed a model using incremental PCA for
simultaneous localization and mapping (SLAM). A similar approach was
used in the only work we are aware of that uses an incremental method
in the context of place recognition \textbf{(MISSING REF?)}. In \cite{ljubjiana:icra02}
incremental PCA was used to update low-dimensional representations of
images taken by a mobile robot as it moved around in an
environment. They also tested repetitive learning of their model with
the same training images several times. While they obtained impressive
results in terms of reconstruction, their method was not tested for
classification.

As far as SVMs are concerned, the solution of a SVM is a function of
a subset of the training samples, called Support Vectors (SV). A
simplification of the decision function is proposed in
\cite{DownsGM01}, based upon linear independence of the SVs in the
feature space, performed \emph{after} the training is done. This is a
simple consequence of the fact that, if the feature space has
dimension $n$, at most $n+1$ SVs are required to build the
solution \cite{PontilV98}. The idea is useful in reducing the testing
time, but it is unfeasible in an online setting, since the
simplification must be performed every time a new sample is
acquired. Other after-training simplification methods, e.g. see
chapter 18.3 in \cite{SmolaS02} and \cite{nguyen2005}, suffer
from the same limitation and are therefore useful only in an offline
setting.

On the other hand, discarding from the sample set the linearly
dependent SVs will result in an approximation; other methods to
heuristically select a subset of the support vectors have been
proposed, e.g., in \cite{LeeM01,schoel06,KeerthiCDC06}. Besides this,
these methods require the knowledge of the full training set, and
therefore, again, they are not suited for online learning.

In order to keep the solution compact without losing accuracy, the key
is to build a low-rank approximation of the kernel
matrix. Unsupervised rank reduction methods have been proposed
\cite{Baudat03} as well as supervised ones \cite{BachJordan2005}, but
no application of these ideas appears so far, to the best of our
knowledge, in online settings. This is particularly important since it
has been shown \cite{Steinwart03} that the SV set grows linearly with
the sample set; therefore, in an online setting, the size of a SVM
would grow indefinitely, and so would the testing time. The exact
solution to online SVM learning was given by Cauwenberghs and Poggio
in 2000 \cite{CauwenberghsP00}, but their algorithm cannot be used to
reduce the number of SVs. In \cite{syed99incremental} and
\cite{pronobis:icvs06} approximate incremental
versions of the SVM are proposed, that also achieve a reduction of the
number of support vectors with small degradation of the performances.
