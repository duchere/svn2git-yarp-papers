\bf{Bit on place recognition, to be added to introduction after the end of the 'enumerate'}


The topic is  widely researched, but
incremental learning approaches have been so far mostly used for constructing the
geometrical map, or the environment representation, online\cite{emma:irca05, ljubjiana:icra02}.
Robustness to illumination changes, and more generally to realistic
visual variations in time, has been addresses in \cite{pronobis:iros06}, where it was showed that
a pure learning approach can be very effective ....


As far as SVMs are concerned, the solution of a SVM is a function of
a subset of the training samples, called Support Vectors (SV). A
simplification of the decision function is proposed in
\cite{DownsGM01}, based upon linear independence of the SVs in the
feature space, performed \emph{after} the training is done. 
The idea is useful in reducing the testing
time, but it is unfeasible in an online setting, since the
simplification must be performed every time a new sample is
acquired. Other after-training simplification methods, e.g. see
chapter 18.3 in \cite{SmolaS02} and \cite{nguyen2005}, suffer
from the same limitation and are therefore useful only in an offline
setting.

On the other hand, discarding from the sample set the linearly
dependent SVs will result in an approximation; other methods to
heuristically select a subset of the support vectors have been
proposed, e.g., in \cite{LeeM01,schoel06,KeerthiCDC06}. Besides this,
these methods require the knowledge of the full training set, and
therefore, again, they are not suited for online learning.

In order to keep the solution compact without losing accuracy, the key
is to build a low-rank approximation of the kernel
matrix. Unsupervised rank reduction methods have been proposed
\cite{Baudat03} as well as supervised ones \cite{BachJordan2005}, but
no application of these ideas appears so far, to the best of our
knowledge, in online settings. This is particularly important since it
has been shown \cite{Steinwart03} that the SV set grows linearly with
the sample set; therefore, in an online setting, the size of a SVM
would grow indefinitely, and so would the testing time. The exact
solution to online SVM learning was given by Cauwenberghs and Poggio
in 2000 \cite{CauwenberghsP00}, but their algorithm cannot be used to
reduce the number of SVs. In \cite{syed99incremental} and
\cite{pronobis:icvs06} approximate incremental
versions of the SVM are proposed, that also achieve a reduction of the
number of support vectors with small degradation of the performances.
