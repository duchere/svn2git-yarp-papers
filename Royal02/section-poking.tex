\begin{figure}[tb]
\begin{center}
\includegraphics[width=\columnwidth]{poking-sequence.eps}

%%\includegraphics[width=\textwidth]{poking-sequence.eps}
%%\includegraphics[width=14cm]{poking-sequence.eps}
\caption{ 
\label{fig:poking-sequence}
%
  The upper sequence shows an arm extending into a workspace, tapping
  an object, and retracting.  This is an exploratory mechanism for
  finding the boundaries of objects, and essentially requires the arm
  to collide with objects under normal operation, rather than as an
  occasional accident.  The lower sequence shows the shape
  identified from the tap using simple image differencing and flipper
  tracking.
%
}
\end{center}
\end{figure}



\section{Perceiving actions on objects}

Now that the robot knows something about its arm, it can start
to use it to explore its environment.
When the arm enters into contact with an object, one of several
outcomes are possible.  If the object is large, heavy, or otherwise
unyielding, motion of the arm may simply be resisted without any
visible effect.  Such objects are of little interest, except in their
role as obstacles, since the robot will not be able to manipulate
them.  But if the object is smaller, it is likely to move somewhat in
response to the nudge of the arm.  This movement will be temporally
correlated with the time of impact, and will be connected spatially to
the end-effector -- constraints that are not available in passive
scenarios~\cite{birchfield99depth}.  If the object is reasonably
rigid, and the movement has some component in parallel to the image
plane, the result is likely to be a flow field whose extent reflects
the physical boundaries of the object.  This visible response to
the robot's action can be used to refine its model of the object's
extent, which may be inaccurate.  For the example scene in
Figure~\ref{fig:setup-sequence} (a cube sitting on a table), the small
inner square on the cube's surface pattern might be selected as a
target.  The robot can certainly reach towards this target, but
grasping it would prove difficult without a correct estimate of the
object's physical extent.  In this section we show how the robot can
experimentally determine an object's extent using the same idea of
correlated motion used earlier to detect its own arm.

\subsubsection*{Making an impact}

Figure~\ref{fig:poking-sequence} shows how a ``poking'' movement can
be used to refine a target.  During this operation, the arm begins
by extending outwards from the resting position.  
\ifrevised
For this simple motivating example, the
\else
The 
\fi
end-effector (or
``flipper'') is local\ize{}d as the arm sweeps rapidly outwards using
the heuristic that it lies at the highest point of the region of optic
flow swept out by the arm in the image (the head orientation and
reaching trajectory are controlled so that this is true).  The arm is
driven outward into the \ahhneighbor{}hood of the target which we wish to
define, stopping if an unexpected obstruction is reached.  If no
obstruction is met, the flipper makes a gentle sweep of the area
around the target.  This minim\ize{}s the opportunity for the motion of
the arm itself to cause confusion; the motion of the flipper is
bounded around the endpoint whose location we know from tracking
during the extension phase, and can be subtracted easily.  Flow not
connected to the end-effector can be ignored as a distractor.


The sequence shown in Figure~\ref{fig:poking-sequence} is about the
simplest case possible for segmenting the motion of the object, since most of the arm is stationary
when contact occurs.  
In \ahhpractice{}, we would rather have less
constraints on the motion of the arm, so we can approach the object
from any convenient direction.  We found that it was possible 
to attain this flexibility without losing the simplicity of object
segmentation that poking brings, by exploiting the unique
visual opportunity afforded by the moment of impact,
as described in the next section.
%
\ifrevised
%
Another important question is, where does the target for poking come
from?  This is in fact very straightforward.  As described in
Section~\ref{sect:robot}, Cog has an attentional system that allows it
to locate and track salient visual stimuli.  This is based entirely on
low-level features such as color, motion, and binocular disparity that
are well defined on small patches of the image, as opposed to features
such as shape, size, and pose which only make sense on well-segmented
objects.  If Cog's attention system locates a patch of the image
that seems reachable (based on disparity and overall robot pose)
it will reach toward it and attempt to poke it so that
it can determine the physical extent of the object to which
that patch belongs.  A human can easily encourage this behavior
by bringing an object close to the robot, moving
it until the robot fixates it,
and then leaving it down on the table.  The robot will track the object
down to the table
(without the need or the ability to actually segment it),
 observe that it can be reached, and poke it.
%
\fi
%


\ifverbose
For simplicity, the head is kept steady throughout the poking
operation, so that simple image differencing can be used to detect
motion at a higher resolution than optic flow.  Because a poking
operation currently always starts from the same location, the arm
is local\ize{}d using a simple heuristic rather than the procedure described
in the previous section -- the first region of optic flow appearing
in the lower part of the robot's view when the reach begins
is assumed to be the arm.
\fi

\ifverbose
\begin{figure}[tbh]
\begin{center}
\includegraphics[width=\columnwidth]{cube-and-cylinder.eps}
\caption{ 
\label{fig:cube-and-cylinder}
%
  Poking can reveal a diffence in the shape of two objects without
  any prior knowledge of their appearance.
%
}
\end{center}
\end{figure}
\fi


\input{section-contact}

\subsubsection*{An operational definition of objecthood}

The poking operation gives clear results for a rigid object that is
free to move.  What happens for non-rigid objects and objects that are
attached to other objects?  Here the results of poking are likely to
be more complicated to interpret -- but in a sense this is a good
sign, since it is in just such cases that the idea of an object
becomes less well-defined.  Poking has the potential to offer an
operational theory of ``objecthood'' that is more tractable than a
vision-only approach might give, and which cleaves better to the true
nature of physical assemblages.  The idea of a physical object is
rarely completely coherent, since it depends on where you draw its
boundary and that may well be task-dependent.  Poking allows us to
determine the boundary around a mass that moves together when
disturbed, which is exactly what we need to know for manipulation.  As
an operational definition of object, this has the attractive property
of breaking down into ambiguity in the right circumstances~-- such as
for large interconnected messes, floppy formless ones, liquids, and so
on.  Poking also gives the robot the opportunity
to collect many views of a single object, and so we can hope to deal
with recogn\iz{}ing objects like the cube shown in
Figure~\ref{fig:sample-results}, which look different from every side.


\begin{figure}[tbh]
  \centerline{\includegraphics[width=9cm]{experiment-montage}}
  \caption{ 
  \label{fig:sample-results}
Results of a training session, where a toy cube was
  repeatedly offered to the robot for poking.  Each image of the cube
  corresponds to the segmentation found for it during a single poke.
  The most common failure mode is inclusion of the robot arm in the
  segmentation.  
}
\end{figure}

