[my comments marked with *** or !!!]
[!!! means "pending" -- not done yet]

REVIEWER 1
==========

Title: Grounding vision through experimental manipulation

Authors: Paul Fitzpatrick and Giorgio Metta

This paper presents a defence of the idea that manipulating objects by
an autonomous robot helps vision.

However the paper is unclear on many points.

- The term "grounding vision" is not explained in the paper, so it is
difficult for the reader to evaluate the contributions of the paper.

*** Terminology problem, "grounding" is widely used term in robotics.
*** FIX: added text in section 3, paragraph 2 (addition in capitals here):
***   "Figure [fig] illustrates three levels of causal complexity
***    we would like our robot to probe, SO 
***    THAT IT CAN DEVELOP ROBUST, EMPIRICALLY FOUNDED
***    REPRESENTATIONS OF THE WORLD AROUND IT
***    (OFTEN REFERRED TO AS ``GROUNDED'' REPRESENTATIONS [CITATION]).


- The target audience is unclear.  For instance, in section 1, it is
noted that the complexity of vision cannot be grasped intuitively.
For those who work in artificial vision, the complexity of vision is
painfully clear.  So, this note must be directed to readers naïve in
computer vision.  However, the justification of the work described
in the paper heavily relies on knowledge of previous work by Ballard
(1991) that is not described at all, thus assuming a reader specialized
in the domain.  

*** Authors did not intend "grasp at an intuitive level" to mean 
*** specialist-trained judgements -- section 1+2 are present primarily
*** for the benefit of non-specialists.
*** FIX: added text in section 1:
***   "the motivation for this work can be difficult FOR THOSE OUTSIDE
***    THE FIELD OF VISION RESEARCH to grasp at an intuitive level.
***    FOR THIS REASON, we begin our paper by seeking to clarify..."


Similarly, in p.6, the authors refer to "long-standing
problems in machine vision" without making them explicit.

*** FIX: added text in section 5, paragraph 3:
***   "... allow us to tackle long-standing problems in machine vision
***    SUCH AS FIGURE/GROUND SEPARATION AND OBJECT RECOGNITION
***    in an innovative way."


- The statement that vision is full of illusions (p.2) is true in a
philosophical sense, and in some specially designed experiments.  However,
everyday vision is very effective at correctly recognizing objects, even
with few cues.  Hence, the "crucial" contribution of tapping an object
(p.3) is a little bit overstated.

*** Terminology problem: reviewer interprets illusion in narrow technical
*** sense, authors intended it in sense used by source quoted (Berkeley 1972)
*** FIX: replaced word "illusion" with "ambiguity" in final paragraph
*** of section 2:
***    "... while it is true that vision is full of AMBIGUITY, this
***     AMBIGUITY evaporates when the robot can reach out..."

*** Context problem: "crucial" contribution of poking is not in
*** mature, everyday vision, but during development, when vision
*** is immature and more in need of guidance.
*** FIX: added text to section 3 paragraph 1:
***    "the ability to perform 'controlled experiments' DURING THE
***     PROCESS OF DEVELOPMENT, ... is crucial to getting to grips
***     with an otherwise complex and uncertain world."


- The paper suggests that tapping will lead to a "manipulation-driven
representation of objects" (p.3).  However, what their results show is
that when you move an object, it can be separated from the background,
smartly filled in and its principal axes determined.  Then, if this is
repeated many times, one can determine the probability that the object
rolls by a given angle.  Is the probability graph the representation of
the object?  

*** Very reasonable point of clarification
*** FIX: added text to section 9 (was 10).  New paragraph, paragraph 4.
***    "At the end of the learning procedure the robot has built a
***     representation of each object in terms of:
***     + Pictorial information in the form of colour histograms,
***     following [CITATION].
***     + A measure of the average area of the object, an
***       index of the elongation of the object with respect to 
***       its principal axis, and a set of Hu moments [CITATION].
***     + Detailed histograms of the displacement of the object 
***       given that a particular motor primitive was used with
***       respect to the initial orientation of the object.
***     + The summary histograms shown in Figure 14
***       which capture the overall response of each object to poking."


There is a mention of a colour segmentation method, but no
details are given.  

*** FIX: clarified text in section 9 (was 10), paragraph 6 (was paragraph 4)
*** and added citation to Swain&Ballard:
***    "based on the same COLOUR HISTOGRAM PROCEDURE used 
***     during training [CITATION]"


Maybe the whole process of building a representation
should be made more explicit.  What information is stored? 

*** FIX: detailed in new paragraph section 9 (was 10), paragraph 4, 
*** as given above.


How is
the learning procedure organized? (For example, does the object need
to be put back in the same place after each tapping?)  Both pieces of
information would help understand what "manipulation-driven representation
of objects" is.  These would also make more clear how much this approach
is suitable for truly autonomous robots.

*** Object doesn't need to be in same place or orientation.
*** We believe the paper addresses the details of the learning
*** procedure, but perhaps the overall picture gets buried under 
*** those details.
*** FIX: added explanatory anecdote, in section 9 (was 10), (new) paragraph 5:
***    "The learning procedure is designed to be robust, 
***     with data gathered opportunistically during the unconstrained 
***     interaction of a human with the robot.  For example, while the 
***     robot was being trained, a teenager visiting the laboratory 
***     happened to wander by the robot, and became curious as to what 
***     it was doing.  He put his baseball cap on Cog's table, and it 
***     promptly got poked, was correctly segmented, and became
***     part of the robot's training data (it was clustered by colour with
***     the similarly-coloured ball)."


- The work on mirror neurons is quite obscure.  On p.5, it is said that
affordance neurons and mirror neurons are found in area F5.  Both seem
to be active when observing manipulation tasks.  Are they not the same?

*** No they are not the same.  The first class of neuron is activated
*** by simply fixating an object, the second is not.
*** Confusion caused by sentence in section 4 introducing mirror neurons:
***    "They found another class of grasping neurons that also respond 
***     during observation of somebody else's action."
*** This sentence is misleading, although the one after it is clear:
***    "A typical cell of this class (called mirror neuron) indeed 
***     responds in two situations: i) when executing a 
***     manipulative gesture, and ii) when observing somebody 
***     else executing the same action."
*** FIX: replaced misleading sentence (section 4, last paragraph) with:
***    "They found a distinct class of neuron that responds
***     specifically to actions on objects, rather than the 
***     mere presence of that object at the point of fixation."


Similarly to Ballard's paper that is key in the argumentation of the
paper, Gallese's paper is key to the argumentation about mirror neurons.
The authors should give more details on the experiments described in these
papers.  In particular, the conclusion that "object and goal-directedness
of the actions represent an important component in the understanding of
the intentions of others" needs more careful elaboration.  At present,
it is unclear how "these results" support that statement.

*** The conclusion we stated was one drawn by other researchers, rather
*** than a novel claim we were trying to defend ourselves.  Nevertheless,
*** it is true that the reference to "these results" could use more
*** support.
*** FIX: added details of experiments in section 4, last paragraph:
*** To the sentence "In (Wohlschlager and Bekkering, 2002) the role of objects
*** during an initative task was tested" we added:
***    "In this experiment two situations were compared: imitating another
***     person's gesture in the presence of a target object, or without such a
***     target.  Reaction times showed that subjects were significantly faster
***     when imitating an action that is directed towards a target (such as an
***     object sitting on a table)."
*** To the sentence "In a series of experiments she [Woodward, 1998] 
*** elucidated the contribution that seeing an object makes for 5, 6, 
*** and 9 month old infants." we added:
***    "Woodward tested various group of infants using the preferential
***     looking paradigm. First, during a habituation phase, the infants
***     observed an adult reaching for one of two toys.  The positions of the
***     toys were then exchanged, and the infant saw the adult grasping the
***     new toy in the same position, hence closely replicating the same
***     trajectory used during the habituation phase.  Experiments showed that
***     the infants looked more frequently at the new grasped toy in spite of
***     the trajectory they were habituated to, which implies that they
***     encoded the object identity into their interpretation of the observed
***     action. Additional experiments showed that the same effect is not
***     present if the action is performed using a mockup of the hand rather
***     than a real human hand. Developmentally, the results showed that by 6
***     months infants start encoding elements of the understanding of
***     goal-directed actions, rather than kinematic aspects of the observed
***     action."
*** And the final sentence that "Taken together, these results provided
*** evidence that ..." was changed to:
***    "Taken together, these results lead Woodward and others to 
***     speculate that ..."
*** We believe that, with the corrections made to clear up the earlier
*** confusion about mirror neurons, our summary of the conclusions of
*** Gallese's paper is at the appropriate level of detail, given
*** that we consider the many other citations to also be important in
*** clarifying the nature and development of the representation of action.


- P.6.  What is the meaning of "bootstrap perception"?

*** Terminology problem: "bootstrap" is widely used term in computer science
*** FIX: replaced phrase in section 5, last paragraph -- was:
***    "... provide the kind of training data needed to bootstrap perception."
*** and is now:
***    "... provide the kind of training data ON OBJECT APPEARANCE AND
***     BEHAVIOUR NEEDED TO DEVELOP A ROBUST PERCEPTUAL SYSTEM."


- Figure 8 (p.10).  Is the arm "tapping the object" or "following a
pre-programmed trajectory that brings it in contact with the object"?
The first description communicates the notion of an intentional movement
directed towards an object.  As I understand it , the robot has neither
intentions nor the notion of what an object is.  (On p.11, the target
seems actually defined by the authors, not the robot).  Maybe the caption
could be corrected?

*** It is easy to slip into the use of intentional language in robotics,
*** and we welcome the reviewer's reminder of this.
*** The specific phrase in question is where we speak of an 
***       "... arm ... tapping an object"
*** Clearly the robot does not have the intention or goal of tapping 
*** the object with its arm.  It doesn't have intentions, and (at this
*** point in the paper) it doesn't know about objects.
*** Nevertheless, we believe that the statement is accurate, in the
*** same sense that one can speak of a "branch tapping a window"
*** without implying that the tree knows about or intends to tap 
*** the window.  Since this figure is intended as a simple introduction
*** to the basic idea of poking, we would prefer not to complicate
*** the language used in the caption.


(On p.11, the target seems actually defined by the authors, not the robot).

*** This is not the case, and we suspect this confusion contributes to
*** the previous objection raised.  We were not sufficiently clear
*** in the paper about where the target for poking comes from.
*** FIX: added text to section 8, last paragraph of "making an impact":
***    "Another important question is, where does the target for poking come
***     from?  This is in fact very straightforward.  As described in
***     Section 6, Cog has an attentional system that allows it
***     to locate and track salient visual stimuli.  This is based 
***     entirely on low-level features such as color, motion, and 
***     binocular disparity that are well defined on small patches 
***     of the image, as opposed to features such as shape, size, and 
***     pose which only make sense on well-segmented objects.  If Cog's 
***     attention system locates a patch of the image that seems reachable 
***     (based on disparity and overall robot pose) it will reach toward it 
***     and attempt to poke it so that it can determine the physical extent 
***     of the object to which that patch belongs.  A human can easily 
***     encourage this behavior by bringing an object close to the robot, 
***     moving it until the robot fixates it,
***     and then leaving it down on the table.  The robot will track 
***     the object down to the table (without the need or the ability 
***     to actually segment it), observe that it can be reached, and poke it."


- More about mirror neurons.  In the paper (p.16), the robot tries to
determine the movement of an operator by observing how the object moves.
Is the object required in animal experiments?  If not, the relation of
this work with mirror neurons is very tenuous.  This could be left out
of the paper to make more space for other clarifications called for above.

*** The objects is absolutely required in animal experiments.
*** this seems to be part of the same confusion about mirror neurons
*** as in an earlier problem.
*** FIX: added more text to section 4, final paragraph:
***    "Mirror neurons are striking in their specificity of response.
***     A neuron that responds to a particular grasp type applied to an
***     object will not respond if the manual grasp is replaced with
***     a tool, such as a pliers."


- P.18.  The notes about predicting the future and inverting causal
relations are very speculative, it seems, and unsupported.

*** The notes lay out an entirely reasonable next step in terms of 
*** robotic implementation, following the internal logic of the
*** developmental process.  In terms of biological evidence, it is
*** of course speculative, since there is very little knowledge of
*** the developmental trajectory that leads to mirror neurons in
*** adult primates.  
*** Also, this text is at the end of the paper.  If one cannot 
*** speculate at the end of a paper, when can one ever speculate?
*** FIX: no action taken


Overall, the authors should make more clear what problem they try to
solve and explain in more details how they solve it.

*** We believe the changes made clarify two key points of confusion that
*** made the paper difficult for this reviewer:
***  (1) We are interested in the DEVELOPMENTAL PROCESS that leads to
***      a mature, competent, visual system.  The mature system may
***      not need to interact with its environment in order to perceive
***      it correctly, but such interaction is (we claim and demonstrate) 
***      very helpful for development.
***  (2) Mirror neurons have very specific, narrowly-tuned responses
***      (that are now more clearly described in the paper).
***      We show how a simple shared activity with a human can
***      provide the kind of data needed to train up just such a 
***      representation.




REVIEWER 2
==========

This paper provides a detailed discussion as to how causality can be
probed at different levels by a robot.  It provides a linkage between the
authors' work and neurobiology.  The paper is well written and provides
a good review of the subject matter.  The discussion on examining
the results of poking and observing the motion are of some interest.
Regarding figure 14, the definition of the principal axis of a cube and
ball need to be more carefully considered.  Can the principal axis of
a ball be defined (given its symmetry)?

*** Answer: no, not reliably, and Figure 14 actually reflects this
*** through the flatness of the graph relative to the objects with
*** well-defined principal axes.
*** FIX: Added text to figure 14 caption:
***    "The principal axis is computed using the second Hu moment of the
***     object's silhouette~\cite{hu62visual}.  The ``pointiness'' or
***     anisotropy of the silhouette is also measured from a higher order
***     moment; this is low when the object has no well-defined principal
***     axis, as is the case for the ball and cube."


In addition:

* does the cube roll or slide?

*** Answer: it slides.
*** FIX: added text to figure 14 caption:
***    "The car and bottle have clear directions in which they tend to
***     roll.  In contrast, the cube slides, and the ball rolls, 
***     in any direction."


* is the direction of the ball only dependant on the direction of the
"poke"?  What about the ball's dynamics and the surface?

*** We do have data on this, omitted for space constraints.
*** FIX: added text to figure 14 caption:
       "These histograms represent the accumulation of many trials, and
        average over the complicated dynamics of the objects and the robot's
        arm to capture an overall trend that is simple enough for the robot
        to actually exploit."


The following also require attention:

* The reference to Berkeley needs to be clarified.

*** This citation is difficult, since the work is a classic dating from 
*** the 18th century, and has many publication dates and versions.
*** No two papers seem to cite it in the same way.
*** We give the first date of publication (1734) and then a complete 
*** citation to a particular copy of the book by a particular printer.
*** FIX: added some more detail to the citation (full name of printer,
*** more clearly tagged mention of original publication date)

*** The discussion of the quote from Berkeley was also changed
*** somewhat for the first reviewer.


* Some of the images are rather small, particularly 6 and 7 where it is
difficult to see the fine detail.

*** FIX: increased size of images
*** [but images are actual output of vision system, so are
***  necessarily low resolution to allow real-time performance]

I would like to see this paper published due to is high accessibility and
scope.

*** Thank you!




REVIEWER 3
==========

Fitzpatrick and Metta: Grounding vision through experimental manipulation

Summary 

The paper presents a series of experiments conducted with the upper-torso
humanoid robot Cog, whose purpose it was to achieve image segmentation
and visual scene interpretation through object manipulation.  In a first
series of experiments, background was separated from object and robot arm
by means of image differencing or background modelling, while the robot's
gaze was fixed (thus not inducing any optic flow).  In a second set of
experiments, the authors attempted to develop artificial mirror neurons
in the robot, i.e. neurons that fire either when an action is performed,
or when its performance is merely observed.  The method employed was to
learn the typical response of a small number (four) of objects to one
of four actions, and subsequently to identify the action performed by
a human operator, using this knowledge.

Discussion 

The paper is very well written, and makes interesting reading.  The
fundamental point that action should be linked with perception ("active
perception") is well taken, and also by now widely accepted in robotics.
This paper makes an interesting contribution to how this can be achieved.

Having read the paper, I would have liked to see more discussion on
two points:

(1) The idea that by actively manipulating an object and observing the
results -- taking into account proprioceptive information -- one can
separate object and background seems so obvious that one wonders why it
hasn't been done before.  In fact, the authors refer to Ballard's work of
eleven years ago, but do not enter a deeper discussion.  The experimental
results are fine, and the overall point is interesting, but I feel it
should be argued more strongly what the contribution of this paper is,
in contrast to existing work.

*** Ballard's work was key to popularizing active/animate vision,
*** but (like almost all such work) focuses on the advantages of
*** moving cameras, and so isn't specifically relevant to our paper.
*** The closest work the authors have found is that cited in the
*** Conclusions, in paragraph 2.
*** FIX: added clarification on the work of Tsikos and Bajcsy in
*** Conclusions:
***    "There is much to be gained by bringing a manipulator into the
***     equation.  For example, [Tsikos and Bajcsy 1991] demonstrated
***     how complex arrangements of blocks could be automatically 
***     separated physically using a robot-mounted suction tool.  
***     This is a very proactive, ``take charge'' style of 
***     segmentation, and it completely changes the accepted rules 
***     of the object segmentation ``game''."
*** Nothing much else to talk about!  This is a very neglected area,
*** few people feed back from object manipulation to vision


(2) The attempt to move towards a "passive" understanding of objects and
their affordances (mirror neuron, section 10) by learning the effects of
the four possible actions, and the responses of four different objects to
these actions) first, and then analysing the results of human-performed
actions, is interesting, and the results look promising.  However, it
seems that the approach is crucially dependent on the fact that there
is only a small number of possible actions, and probably also on the
fact that there is a small number of objects.  How will this scale?
Is this approach suitable for anything but the smallest set of actions
and objects?  If it isn't, that wouldn't necessarily be to the detriment
of the paper but we would like to know.

*** FIX: added some text on this in the Conclusions:
***    "This work is an integrated ``proof of concept'', and almost every
***    individual component within it could be improved considerably.  For
***    example, there are much more sophisticated techniques for object
***    recognition and localisation than ours, e.g. [CITATION].
***    The key technical contribution of this paper is not the recognition
***    method used, but the fact that the robot can autonomously
***    collect all the training data it needs using poking.  Once that is
***    possible, any recognition method could be trained up, and 
***    we expect that our system can be extended to work with
***    large numbers of objects.
***    Another rather under-developed component in our work is
***    the robot's motor control and action repertoire.  
***    It is not as clear how well our system will scale as new
***    actions are added, but we have at least demonstrated that recognizing
***    the actions of others doesn't necessarily require a full blown
***    kinematics and 3D localisation/interpretation of the motion of the
***    human body."


Also, the authors should discuss how much the method discussed in section
10 is dependent on the correct object identification. What happens if
the bottle is confused with the car?

*** Answer: the robot will do the wrong thing
*** FIX: added some text on this
***    "Another potential mistake that could occur is if the robot
***     misidentifies an object - and, for example, believes it sees a bottle
***     when it in fact sees a car.  Then the robot will poke the object the
***     wrong way even if it correctly determines the object's position and
***     orientation."


Smaller points 


Figures 6 and 7 are hard to read. 

*** Fixed for reviewer 2


The flipper is found by a heuristic (p.10).  How much does this heuristic
influence the generality of the method?

*** The discussion for poking starts with a simple motivating example, 
*** shown in Figure 8, which uses a simple heuristic to track the
*** flipper.  The main poking work does NOT use that heuristic,
*** but rather relies on detailed analysis of the moment of contact.
*** FIX: added caveat to sentence that describes the heuristic:
***    "For this simple motivating example, ...."
*** Discussion of more flexible processing is in immediately
*** following paragraph, which was updated in response to 
*** reviewer 1's comments.


The significance of section 9 is unclear.  Neither the objectives nor
the results presented there seem particularly relevant to the paper,
and the paper might benefit from the omission of that section.

*** There is a connection, but on balance, we accept this reviewer's
*** judgement.
*** FIX: removed section 9.


The term "signature explosion of movement" (p.12) should be explained.

*** FIX: replaced phrase with:
***    "very characteristic sudden appearance of optic flow"


The sixth sentence from the top on p.14 is incomplete ("simply by checking
whether its arm ...").

*** The sentence was complete, but poorly written.
*** The section it belongs to (section 9) was removed, so this is
*** not a problem anymore.


The discussion of results on p.15 should be expanded.  What is counted
as a "mistake"?  How were these measured?

*** FIX: added explanation
***    "A trial was classified as ``mistaken'' if the robot failed to poke
***     the object it was presented with in the direction that would 
***     make it roll."


*** Removed references to old Section 9
*** Added acknowledgement to feedback from reviewers

