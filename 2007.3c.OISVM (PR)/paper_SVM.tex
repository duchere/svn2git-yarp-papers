In this Section we first introduce on-line learning as a general scenario; we then
instantiate it for SVMs and then proceed to describe our
adaptation of SVMs in the on-line framework. We will always be referring
to the problem of \emph{classification}.

In standard learning (\emph{batch}), a set of (sample,label) pairs drawn from an
unknown probability distribution is given in advance (\emph{training set}); the task
is to find a function (\emph{hypothesis}) such that its sign best determines the
label of any future sample drawn from the same distribution. As opposed to this,
in on-line learning samples and labels are made available in time, so that no
knowledge of the training set can be assumed a priori. The hypothesis must therefore
be built incrementally every time a new sample is available. Let us call this operation
of building a new hypothesis, a \emph{round}.

Formally, let $\{\xx_i,y_i\}_{i=1}^l$, with $l \in \RR^+$, $\xx_i\in\RR^m$ and
$y_i\in\{-1,1\}$, be the \emph{full} training set, and let $h_i$ denote the
hypothesis built at round $i$, when only the (sample,label) pairs up to $i$ are
available. At the next round, a new sample $\xx_{i+1}$ is available and
its label is predicted using $h_i$.
The true label $y_{i+1}$ is then matched against this prediction, and a
new hypothesis $h_{i+1}$ is built taking into account the loss incurred in the
prediction. In the end, for any given sequence of samples $(\xx_1,y_1),\cdots,(\xx_l,y_l)$,
a sequence of hypotheses $h_1,\cdots,h_l$ is built such that $h_i$ depends
only on $h_{i-1}$ and $(\xx_i,y_i)$.

Note that any standard machine learning algorithm can be adapted to work in the
on-line setting just retraining from scratch each time a new sample is acquired.
However this would result in an extremely inefficient algorithm.

In the following we sketch the theory of SVM that gives us the tools
to extend it to the on-line setting in an efficient way;
the interested reader should refer to, e.g., \cite{Cristianini00} for a
comprehensive treatment of the subject.


\subsection{Support Vector Machines}

If one assumes the samples are separable via a linear function in $\RR^m$
(hyperplane) $f(\xx) = \ww\cdot\xx + b$, with $\ww \in \RR^m$ and $b \in \RR$,
the SVM algorithm finds the one for which $\|\ww\|$ is minimum,
enforcing the constraints $y_i(\ww\cdot\xx_i + b)-1\geq 0$.
In case the samples are not linearly separable, $l$ slack variables $\xi_i \geq 0$
are introduced and
%
\begin{equation}
  \label{eqn:svm_primal}
  \arg \min_{\ww,b} \frac{1}{2} \|\ww\|^2 + C \sum_{i=1}^l \xi_i^p
\end{equation}
%
\noindent is sought for, subject to the constraints $y_i (\ww\cdot\xx_i + b) \geq 1-\xi_i$,
where $C \in \RR^+$ is an error penalty coefficient and $p$ is usually $1$ or $2$.
The problem is compactly expressed in Lagrangian form by
further introducing $l$ pairs of coefficients $\alpha_i, \mu_i$ and then
minimizing
%
\begin{equation}
  \label{eqn:lp1}
  L_P = \frac{1}{2} \|\ww\|^2 - \sum_{i=1}^l \alpha_i \left(y_i
        (\ww\cdot\xx_i+b) - 1 + \xi_i \right) + C \sum_{i=1}^l \xi_i^p
        - \sum_{i=1}^l \mu_i \xi_i,
\end{equation}
%
\noindent subject to $\alpha_i,\mu_i\geq 0$. Using the Karush-Kuhn-Tucker
(KKT) optimality conditions \cite{Cristianini00}, we obtain that
%
\begin{equation}
  \label{eqn:kkt1}
  \frac{\partial L_P}{\partial \ww}= \ww - \sum_{i=1}^l \alpha_i y_i \xx_i = 0,
\end{equation}
%
\noindent that is, $\ww = \sum_{i=1}^l \alpha_i y_i \xx_i$. Hence the approximating
function $f(\xx)$ can be expressed as
%
\begin{equation}
  \label{eqn:lin_sol}
  f(\xx) = \sum_{i=1}^l \alpha_i y_i \xx \cdot \xx_i + b~.
\end{equation}

To improve the discriminative power of an SVM, the $\xx_i$s are usually
mapped to a high, possibly infinite-dimensional space (the \emph{feature
space}) via a non-linear mapping $\Phi(\xx)$; the core of the SVM becomes
then the so-called \emph{kernel function} $K(a,b) = \Phi(a) \cdot \Phi(b)$. The
\emph{kernel matrix} $\KK$ is defined alongside such that $K_{ij} = K(\xx_i,\xx_j)$
(here, and in the following, a boldface letter denotes the vector or the matrix
whose components are denoted by the same letter, non-boldface;
the term \emph{kernel dimension} will refer, as is
customary, to the dimension of the feature space).
Widely used kernels are the \emph{polynomial} one (finite-dimensional) and the \emph{Gaussian} one
(infinite-dimensional). In the end, Equation (\ref{eqn:lin_sol}) is rewritten as
%
\begin{equation}
  \label{eqn:sol}
  f(\xx) = \sum_{i=1}^l \alpha_i y_i K(\xx,\xx_i) + b~.
\end{equation}

After minimization of $L_P$, some of the $\alpha_i$s (actually most of them
in many practical applications) are equal to zero; those $\xx_i$s for which
this does not hold are called \emph{support vectors}. The solution depends on
them only and their number is proportional to the number of
training samples \cite{Steinwart03}.

The standard SVM algorithm is meant to be used batch-wise; to extend it
to the on-line setting two different approaches have been proposed:
$(i)$ the batch algorithm is adapted to examine one sample at the time
and produce a new approximate solution, such as in \cite{SyedLS99} and in 
\cite{KivinenSW04,ChengVSWC07};
$(ii)$ exact methods that incrementally update the solution as in \cite{CauwenberghsP00}.
In both cases we have that the potentially endless flow of training samples
of the on-line setting will bring sooner or later to an explosion of the number of
support vectors, and hence of the testing time.


\subsection{On-line Independent SVMs}

Consider Equation (\ref{eqn:sol}) again. The representation of $f(\xx)$ is
\emph{sparse}, meaning that the sum contains fewer members than $l$, this
being a consequence of the $\alpha_i$ which are found to be zero. In practice,
$f(\xx)$ is built by summing up as many factors as support vectors. Really, one
step further can be taken: if some of the support vectors are linearly dependent
on the others \emph{in the feature space}, some of them can be expressed as a
function of the others \cite{DownsGM01}, therefore reducing the expression
of $f(\xx)$. In these cases we can obtain different, possibly sparser,
representation of the solution of the optimization problem (\ref{eqn:svm_primal}), without
changing the solution. A possibility is to simplify the solution after each
update, which  would be extremely time consuming; this is why the method outlined in
\cite{DownsGM01}, and similar ones, cannot be applied on-line.

The main idea is then to keep a subset of the training vectors (we call them
\emph{basis} vectors) to build the classification
function (\ref{eqn:sol}), independent from the samples used to find out the
$\alpha_i$s during minimization. In fact, using the Representer Theorem \cite{ScholkopfHS01},
the solution of the optimization problem (\ref{eqn:svm_primal}) can always be written as
$\ww = \sum_{i=1}^l \beta_i \Phi(\xx_i)$ for a set
of generic coefficients $\beta_i \in \RR$. Hence directly plugging-in this expression
of $\ww$ in (\ref{eqn:svm_primal}) will not change the optimum.
Remembering the kernel trick and the definition of the matrix $\KK$, the expression of $\|\ww\|^2$
becomes
$\sum_{i,j=1}^l \beta_i \beta_j \Phi(\xx_i) \cdot \Phi(\xx_j)=\sum_{i,j=1}^l \beta_i \beta_j K_{ij}$.
Hence, we obtain the following optimization problem 
%
\begin{align}
  \label{eqn:mod_primal}
  &\arg \min_{\bb,b} 
      \frac{1}{2} \sum_{i,j=1}^l \beta_i \beta_j K_{ij}
    + C \sum_{i=1}^l \xi_i^p \\
  &\text{subject to } y_i \left( \sum_{j=1}^l \beta_i K_{ij} + b \right) \geq 1-\xi_i, \ \forall i=1,\cdots,l \nonumber
\end{align}
%
Due to his derivation, this formulation is completely equivalent to
(\ref{eqn:svm_primal}), and we can state the following theorem that links the solutions
of the two problems.
%
\begin{teorema}
Let $\{\xx_i,y_i\}_{i=1}^l$, $l$ training samples, $\aa$ the solution of
an SVM optimization problem (\ref{eqn:lp1}), and $\bb$ the solution of (\ref{eqn:mod_primal}).
Denote by $\vv$ a vector in the null space of $\KK$, we have that
$\beta_i=\alpha_i y_i+ v_i$.
\end{teorema}
%
\begin{proof}
We calculate the Lagrangian of (\ref{eqn:mod_primal}), introducing $l$ pairs of
coefficients $\alpha_i, \mu_i$
%
\begin{equation} \label{eqn:svm_primal_general}
  L'_P = \sum_{i,j}^l \left( \frac{1}{2}\beta_i-\alpha_i y_i \right) \beta_j K_{ij}
         - \sum_{i=1}^l \alpha_i (b y_i -1 +\xi_i)
         + \sum_{i=1}^l C \xi_i^p - \sum_{i=1}^l \mu_i \xi_i~. \nonumber
\end{equation}
%
Now, enforcing the KKT conditions on this, one obtains that
%
\begin{equation} \label{eqn:kt2}
  \frac{\partial L'_P}{\partial \beta_i} = \sum_{j=1}^l (\beta_j - \alpha_j y_j) K_{ij} = 0,\ i=1,\cdots,l~.
\end{equation}
\noindent This is the product of the matrix $\KK$ for the vector whose components are
$\beta_j - \alpha_j y_j$. 
Clearly, in order for (\ref{eqn:kt2}) to hold, the vector must be in the null space of
$\KK$.
\end{proof}

If $\KK$ has full rank, the null space only consists of the null
vector, and therefore $\beta_i = \alpha_i y_i$, recovering a result
already appeared in \cite{KeerthiCDC06}.
However if $\KK$ is not full rank, there are infinite equivalent solutions
to the SVM problem, and the $\beta_i$s are not
constrained at all: this agrees with Downs et al.'s \cite{DownsGM01}
after-training simplification method and generalizes it.

The above considerations suggest to optimize problem (\ref{eqn:mod_primal}),
explicitly selecting as basis vectors only independent vectors in the feature space.
Hence, instead of training and then simplifying the solution, we propose to \emph{directly}
build the solution with a small number of basis vectors.
The algorithm can then be summed up as follows:
\begin{itemize}
\item check whether the current sample is linearly independent from the basis in the feature space;
if it is, add it to basis.
\item incrementally optimize (\ref{eqn:mod_primal})
\end{itemize}
These are the core steps of our algorithm that we call On-line Independent
Support Vector Machine (OISVM). In the two following sections we describe in
more details these two steps.

\subsubsection{Exploiting linear independence on-line}
Denote the indices of the vectors in the current basis, after $l$ training
examples, by $\b$. When the algorithm receives $\xx_{l+1}$ it has to check
if it is linearly independent or not from the basis vectors.
In general, checking whether a matrix has full rank is done via some
decomposition, or by looking at the eigenvalues of the matrix; but
here we want to check whether a \emph{single} vector is linearly
independent from a matrix of vectors already known to be full-rank.
It is then simpler to exploit the definition of linear independence
and check how well the vector can be approximated by a linear combination
of the vectors in the matrix \cite{EngelMM04}. Let $d_j \in \RR$; then
let
%
\begin{equation} \label{eqn:ald1}
  \Delta = \min_\dd \left\| \sum_{j \in \b} d_j \phi(\xx_j) - \phi(\xx_{l+1}) \right\|^2~.
\end{equation}
%
\noindent If $\Delta > 0$ then $\xx_{l+1}$ is linearly independent with respect
to the basis, and $l+1$ is added to $\b$. In practice, one must check
whether $\Delta \leq \eta$ where $\eta > 0$ is a tolerance factor, and
expect that larger values of $\eta$ lead to worse accuracy, but also
to smaller bases. If $\eta$ is set to zero the solution found will be \emph{the same}
as in the classical SVM formulation; therefore, no approximation whatsoever is involved,
unless one gives it up in order to obtain even fewer support vectors
(see Section \ref{sec:exp} for a deeper discussion on this point).
From (\ref{eqn:ald1}) it is clear that the maximum meaningful value of
$\eta$ is $\max_i \|\phi(\xx_i)\|^2= \max_i K(\xx_i,\xx_i)$.

An efficient way to evaluate $\Delta$ is needed. Expanding (\ref{eqn:ald1}) and
remembering the definition of the kernel matrix $\KK$, we get
%
\begin{equation}
  \label{eqn:ald3}
  \Delta = \min_{\dd} \left( \dd^T \KK_{\b\b}\dd - 2 \dd^T \mathbf{k} + K(\xx_{l+1},\xx_{l+1}) \right),
\end{equation}
%
\noindent where $k_i = K(\xx_i,\xx_{l+1})$ with $i \in \b$, and $\KK_{\b\b}$ is the
restriction of the kernel matrix $\KK$ to the rows and columns corresponding to the
indices in $\b$. Applying the extremum conditions with respect to $\dd$ to Equation
(\ref{eqn:ald3}) we obtain that $\mdd = \KK_{\b\b}^{-1} \mathbf{k}$ and, by replacing
this in (\ref{eqn:ald3}) once,
%
\begin{equation}
  \label{eqn:ald5}
  \Delta = K(\xx_{l+1},\xx_{l+1}) - \mathbf{k}^T \mdd~.
\end{equation}
%
Note that $\KK_{\b\b}$ can be safely inverted since, by incremental
construction, it is full-rank. An efficient way to do it, exploiting
the incremental nature of the approach, is that of updating it
recursively. Using the matrix inversion lemma, after the addition of a new sample the new
$\KK_{\b\b}^{-1}$ becomes
%
\begin{equation}
  \label{eqn:inv_upd}
  \left[\begin{array}{cccc}
       &               &   & 0 \\
       & \KK_{\b\b}^{-1} &   & \vdots \\
       &               &   & 0 \\
     0 &       \cdots  & 0 & 0
  \end{array}\right]
  +
  \frac{1}{\Delta}
  \left[\begin{array}{c}
    \mdd \\
    -1
  \end{array}\right]
  \left[\begin{array}{cc}
    \mdd^T & -1
  \end{array}\right],
\end{equation}
%
\noindent where $\mdd$ and $\Delta$ are already evaluated during the
test. Similar updates have been used in \cite{CauwenberghsP00,CsatoO01}. Thanks to this
incremental evaluation, the time complexity of the linear independence
check is $O(|\b|^2)$, as one can easily see from the expression of $\mdd$
above.


\subsubsection{Incremental Training}
We want to find the solution to the optimization problem (\ref{eqn:mod_primal}),
without introducing the Lagrangian and its dual formulation. In fact the dual
problem would introduce again $l$ coefficients. Instead we want
to use just a number of coefficients equal to the number of basis vectors selected.
Hence we need a method to optimize the primal formulation of (\ref{eqn:mod_primal}).
The method that we have chosen is an adaptation of the modified Newton method found
in \cite{KeerthiDC05,KeerthiCDC06}. To apply the method we first have to set $p=2$ in
and transform it to an equivalent unconstrained minimization problem,
using the $max(\cdot,\cdot)$ function.
Let $\d \subset \{1,\ldots,l\}$; then the unconstrained problem is
%
\begin{equation}
  \label{eqn:primal}
  \arg \min_{\bb} \frac{1}{2} \bb^T \KK_{\d\d} \bb
    + \frac{1}{2} C \sum_{i=1}^l max \left(0,1-y_i \KK_{i\d} \bb \right)^2~.
\end{equation}
%
We then set $\d = \b$, which assures that the
solution to the problem is unique, since $\KK_{\b\b}$ is full rank by
construction. When a new sample $\xx_{l+1}$ is available, the Newton method goes as follows:
%
\begin{enumerate}

   \item use the current value of $\boldsymbol{\beta}$ as starting
     vector;

   \item let $o_{l+1} = K_{l+1,\b} \bb$; if $1-y_{l+1} o_{l+1} \geq 0$,
     then stop: the current solution is already optimal. Otherwise,
   
   \item let $\mathcal{I} = \{ i: 1-y_i o_i>0 \}$ where $o_i =
     K_{i,\b} \bb$ is the output of the $i$-th training sample;

   \item update $\bb$ with a Newton step:
     $\bb - \gamma \mathbf{P}^{-1}\mathbf{g} \rightarrow \bb$ where
     $\mathbf{P} = \KK_{\b\b} + C \KK_{\b\mathcal{I}} \KK_{\b\mathcal{I}}^T$ and
     $\mathbf{g} = \KK_{\b\b} \bb - C \KK_{\b\mathcal{I}}
        \left( \mathbf{y}_{\mathcal{I}}-\mathbf{o}_{\mathcal{I}}\right)$;
        
   \item let $\mathcal{I}^{new} = \{ i: 1-y_i o_i>0 \}$ where $o_i$ are
     recalculated using new $\bb$. If $\mathcal{I}^{new}$ is equal to
     $\mathcal{I}$ stop; otherwise $\mathcal{I}^{new} \rightarrow \mathcal{I}$
     and go to step 4.

\end{enumerate}
%
In Step $4$ above, we have set $\gamma$ to one, without experiencing
any convergence problem. With this choice the update of $\bb$ is $C
\mathbf{P}^{-1} \KK_{\b\mathcal{I}} \mathbf{y}_{\mathcal{I}} \rightarrow
\bb^{new}$. In order to speed up the algorithm, we maintain an
updated Cholesky decomposition of $\mathbf{P}$ and a vector with the
product $\KK_{\b\mathcal{I}} \mathbf{y}_{\mathcal{I}}$: every time a
sample enters or exits from the set $\mathcal{I}$ these two quantities
are updated. It turns out that the algorithm converges in very few
iterations, usually $1$ or $2$.

The pseudo-code of the entire OISVM algorithm is summarized in Algorithm \ref{alg:algo}.

\begin{algorithm}[t]
   \caption{Pseudo-code of OISVM.}
   \label{alg:algo}
\begin{algorithmic}
   \STATE {\bfseries Parameters:} $\eta$
   \STATE {\bfseries Initialization:} $\b=\{\}, \beta=[]$
   \FOR{each time step $t=1,\ldots,l$}
   \STATE $\mathbf{k}=K(\xx_{t},\xx_{j}), j \in \b$
   \STATE $\mdd = \KK_{\b\b}^{-1} \mathbf{k}$
   \STATE $\Delta = K(\xx_{t},\xx_{t}) - \mathbf{k}^T \mdd$
   \IF[Linear independence test]{$\Delta \geq \eta$}
   \STATE $\b=[\b, t]$
   %\STATE Update $\KK_{\b\b}^{-1}$
   \ENDIF
   \STATE $o_{t}=\mathbf{k}^T \beta$
   \IF{$o_{t} < 1$}
   \STATE $\mathcal{I}^{new} = \{ i: 1-y_i o_i>0 \}$
   \REPEAT[Incremental update of the solution]
   \STATE $\mathcal{I} = \mathcal{I}^{new}$
   \STATE $\mathbf{P} = \KK_{\b\b} + C \KK_{\b\mathcal{I}} \KK_{\b\mathcal{I}}^T$
   \STATE $\bb=C\mathbf{P}^{-1} \KK_{\b\mathcal{I}} \mathbf{y}_{\mathcal{I}}$
   \STATE Recalculate $o_{i}, i=1,\cdots,t$
   \STATE $\mathcal{I}^{new} = \{ i: 1-y_i o_i>0 \}$
   \UNTIL{$\mathcal{I}^{new} = \mathcal{I}$}
   \ENDIF
   \ENDFOR
\end{algorithmic}
\end{algorithm}

\subsubsection{Analysis}
The sparsification method used has several properties. We will examine them in this section.
First of all, note that if the feature space has finite dimension $n$, then no more than
$n$ linearly independent vectors can be found, and $\b$ will never contain
more than $n$ vectors. However even if the feature space is infinite-dimensional,
for any $\eta$ greater than zero the maximum number of basis vectors will be
finite for any training sequence (this argument has already been proved in
\cite{EngelMM04}). Therefore, this method breaks the linear dependency
between the number of SVs and the number of training samples mentioned
in \cite{Steinwart03}. Note that any other approach to extend SVM to the on-line
framework, as the ones analyzed in \cite{DomeniconiG01}, does not have such
property.
The boundedness of the solution can be easily proved for the specific case
of the Gaussian kernel, showing also an interesting property on the distribution
of the basis vectors in the input space.
\begin{teorema}
Using OISVM with a Gaussian kernel, $K(\xx_i,\xx_j)=e^{-\gamma \| \xx_{i}-\xx_{j} \|^2}$,
imposes a minimum distance in the input space
among the basis vectors of $\frac{1}{2\gamma}\log(\frac{1}{1-\eta})$
\end{teorema}
\begin{proof}
Consider plugging $\b=\left\{ i \right\}$ in (\ref{eqn:ald1}), that is,
$\xx_i$ is the only vector in the basis. Then
%
\begin{equation} \label{eqn:ald_single}
  \Delta_i = \min_{d_i} \left\| d_i \phi(\xx_i) - \phi(\xx_{l+1}) \right\|^2~.
\end{equation}
%
Obviously $\Delta_i \geq \Delta, \forall i \in \b$, so if $\Delta_i
\leq \eta$ then we have that $\Delta \leq \eta$ and the sample $l+1$
will not be added to the basis set. Remembering
(\ref{eqn:ald3})-(\ref{eqn:ald5}), the last equation can be expanded to
%
\begin{equation} \label{eqn:ald_single2}
  \Delta_i = K(\xx_{l+1},\xx_{l+1}) - \frac{K(\xx_{l+1},\xx_{i})^2}{K(\xx_{i},\xx_{i})}~.
\end{equation}
%
Remembering the definition of Gaussian kernel and the property that $K(\xx,\xx)=1$ for any $\xx$
we obtain that
%
\begin{align*}
	\Delta_i \leq \eta \Leftrightarrow \| \xx_{l+1}-\xx_{i} \|^2  \leq -\frac{1}{2\gamma}\log(1-\eta)~.
\end{align*}
\end{proof}

As a consequence of the above theorem we have that the number of basis vectors
is finite for any compact input domain, when using a Gaussian kernel.

The time complexity of the re-training step is $O(|\b|l)$, as well as its space
complexity. So setting $\eta$ greater than zero, we have that the time
complexity for training $l$ training points is $O(l^2)$, since, as said above,
after a certain number of samples $\b$ necessarily stops growing.
Hence, keeping $\b$ small speeds up the training time as well as the testing time.

%However this results is quite general and holds for any kernel.
%\begin{teorema}\label{teo:bound_support}
%Consider any sequence of training samples $\{(\xx_i,y_i)\},i=1,\cdots,\infty$
%with $\xx_i \in \X$, $\X$ a compact subset of a Banach space.
%For any $\eta>0$, the size of the \textbf{FIXME} of the OISVM algorithm is finite.
%\end{teorema}
%The proof of this theorem goes along the same lines as the proof of Theorem 3.1 in
%\cite{} so we omit it for brevity.

We can gather more insight on the role of $\eta$ observing that
OISVM approximates the kernel matrix $\KK$ by another matrix $\widehat{\KK}$
\cite{BachJordan2005} and the quality of the approximation depends on $\eta$.
In fact it is easy to show that $trace(\KK-\widehat{\KK}) \leq \eta |\b| \leq \eta l$, where $l$ is
the number of samples acquired \cite{EngelMM04}. If we consider a
normalized kernel, that is a kernel for which $K(x,x)$ is always equal
to $1$, we can write $trace(\KK-\widehat{\KK})/trace(\KK) \leq \eta$. On
the other hand, a bigger $\eta$ means of course a smaller number of
SVs, hence it controls the trade-off between accuracy and speed of OISVM.

As a last remark, OISVMs can be easily extended to multiclass
classification. Noting that the sparsification procedure makes
no use of the labels $y_i$, in the one-vs-all
multiclass setting the kernel matrix $\KK_{\b\b}$
is the same for each machine involved. All experiments shown in the next Section on multiclass databases
are performed using this methodology.
