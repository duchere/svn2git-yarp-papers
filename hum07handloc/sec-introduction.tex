\section{Introduction}\label{introduction}

A great amount of research in robotics has been devoted to the problem of 
controlling and planning reaching under visual guidance. Most 
of these works assume that visual feedback from the robot 
end-effector is available. The visual problem is often solved by 
placing markers on the robot to simplify its detection and 
segmentation. Unfortuately these solutions are not general and in most cases 
too simple to be applied in realistic 
scenarios (the most common solutions use 
color segmentation which is extremely sensitive to illumination changes).

It would be much better if robots were able to autonomously acquire and tune 
a visual representation of their own body. Visual identification of the body 
could be seen as parts of the general problem of machine vision; after all the 
hand or the arm of the robot are just objects in the environment and could be 
treated as such. But even if the robot was able to identify objects in the 
environment that looks like its body, would not be possible to discriminate 
among them and detect exactly those objects that indeed ``are'' its own body?

We believe that the localization of the body is not purely visual but is 
rather an interplay between perceived visual and motor (somato-) sensory information. 
Indeed, among all entities in the enviroment the body has unique properties 
that the robot can exploit to simplify the problem of self detection, mainly:
- the robot has direct control on the body
- the robot has access to proprioceptive information, related with where the body is and how it moves in the environment

In this paper we show how both aspects can simplify the self-identification 
problem. We focus on the identification of the hand. During movement of the 
arm, the robot uses visual motion to segment objects in the scene. The 
onset of the arm motor command signals the beginning of the detection, so that
regions that were already moving when the hand was not moving are immidiately 
discarded. Finally, among all regions that moves in the scene, the robot 
selects those that better correlate with the proprioceptive feedback from 
the arm. The idea in this case is that the robot has an expectations of the 
kind of visual motion that is generated by the arm, given the proprioceptive
feedback and improves the detection by discarding 
those area in the image that do not match well with this expectation.

The detection procedure is completely unsupervised, meaning that it does 
not required manual labeling the hand in the image, and can be performed 
robustly in a non-controlled environment.

%In this work we proposes a way to do it without having a body map or kinematic model, but using only its sensorization. After the introduction it is the moment to understand%
%But how it is possible to close the missing link between the ignorance of having a hand and how it looks like. \textit{Look like} means that the robot will be able to find and recognize its own hand in the visual field, what again means that the robot must gain precise visual information about its hand. Here a visual model of the hand is not a specific type of model, e.g.\ CAD model; it is rather having a collection of visual information about the object hand and its visual structure. %The goal of the work is also not to talk about learning nor about making a kinematic model or similar tasks, but it is a proposal to robustly and autonomously detect the hand. \newline
%Contemplating and understanding a scene is a simple task for us human beings. For an artificial system it is by far more difficult to understand a visual scene due to limits in processing images and its information. \textsc{If we want to create intelligent agents, i.e. humanoid and or anthropomorphic robots we somehow have to find ways to process information in a way that is maybe similar to the way human beings.} For grasping an object, without a kinematic model, the robot must be able to exactly locate the hand in the scene. \newline
%But how does the robot get the possibility to get to know its hand and maybe parts of the arm? %The robot is going to move his arm in front of his face. At this stage of development of this algorithm, James is not meant to move more than one joint nor to move its head. Additionally we ensured that the arm is for sure generating motion in the visual field of James. In later works maybe all these constraints can be eliminated or at least incrementally eliminated. \newline%But let us agree to have these constraints being the single ones for this master's thesis. 
%The idea we propose is that the robot firstly has to localize moving regions before it is able to classify them as a part of its body. %a moving region in the visual space as its own body part. 
%Visual detection of whatever object is only possible if you know where to find it and how to distinguish that from the rest of the scene. So, localization is a minimum requirement for detection. This however turns up to be a new problem: "How can I localize my hand as long as I don't know:
%\begin{itemize}
%	\item what I have to look for?,
%	\item how it looks like?,
%	\item how is it making itself remarkable?"
%\end{itemize}
%The last question is the first to be answered. A possibility to localize an object in an unstructured and unknown environment, is to mark it. For instance by placing a unambiguous, unique sign on that object, e.g. a point of a specific colour or similar helpers. %That would be a nice idea, but the fact is that our system is a one without any prior knowledge and besides this does not know what to look for (see above the $1^{st}$ question). So marking objects is not really supporting and neither does coincide with our philosophy about autonomous development of artificial systems. 
%One way to omit markers or other helpers that unambiguously identify the target, is to move it. Generating movement attracts an attention system that is stimulated by motion and is a hint where to look for something interesting. Let us assume that the robot classified all the different moving objects in the visual scene and that one of these objects is the hand. So among some other objects there is the hand and therefore the visual information is already present. At latest by this time the first question turns up again. As already seen in chapter \ref{intro:ds} where we have learned more about embodiment/developmental science and know that an intelligent self conscious (biological) system firstly has to get first-person experience \cite{ADAP03} and firstly need to build a sense for its own body \cite{DERBS04-02} before being able to explore the environment. Mapping causal relation in the visive field with proprioceptive body-sensory information helps to eliminate all the other objects that caused some behaviour in the field of attention.
%We reflect, our main goal and result of this work is that a humanoid robot is able to discover its hand on its own. 
%As for an embodied system exploration of its environment and itself for self-exploration is important to grow in development and self-awareness \ref{intro:ds}, such a system needs some instrument supporting this exploration. A possible path to learn what a robot can control or manipulate in order to perform self-exploration (level 1 of self-awareness in section \cite{FLSAUEL03-02}) is to differentiate itself from its environment by intermodal linking of seen movements with perception of body proprioception to understand a situated self. P. Rochat sees the proof of level 2 self-awareness in the demonstration where about 4 months old infants systematically reach for objects they see. In this ``touch all'' stage of infancy children express a selective and systematic hand-eye coordination. This means that children do not only have a sense of being something different in the environment, but also have a sense of situation in relation to actions that can be performed in the environment. 

%To close the connection with our work, it is important to somehow enable the robot to self-explore and then reach for objects in the environment. A first step on that discovery of the body is first to perceptually discover its hand. 
