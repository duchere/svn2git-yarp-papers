This file was created with JabRef 2.2b2.
Encoding: Cp1252

@MISC{ITBVME03-12,
  author = {Christian Beder},
  title = {Informationstheorie, Bildverarbeitung und Mustererkennung},
  month = {Dezember},
  year = {2003},
  note = {Summaries - very useful},
  pdf = {G:\Science\Computerscience\Robotics\imageprocessing\ITBVME03-12.pdf},
  timestamp = {2006.08.04}
}

@TECHREPORT{PILKFT99,
  author = {Jean-Yves Bouguet},
  title = {Pyramidal implementation of the Lucas Kanade feature tracker: Description
	of the algorithm},
  institution = {Intel Research Laboratory},
  year = {1999},
  pdf = {D:\bucko\literatureDB\Science\ImageProcessing\PILKFT99.pdf},
  timestamp = {2007.03.15}
}

@INPROCEEDINGS{brooks98alternative,
  author = {Rodney A. Brooks and Cynthia Breazeal and Robert Irie and Charles
	C. Kemp and Matthew Marjanovic and Brian Scassellati and Matthew
	M. Williamson},
  title = {Alternative Essences of Intelligence},
  booktitle = {Association for the Advancement of Artificial Intelligence},
  year = {1998},
  pages = {961-968},
  abstract = {We present a novel methodology for building human-like artificially
	intelligent systems. We take as a model the only existing systems
	which are universally accepted as intelligent: humans. We emphasize
	building intelligent systems which are not masters of a single domain,
	but, like humans, are adept at performing a variety of complex tasks
	in the real world. Using evidence from cognitive science and neuroscience,
	we suggest four alternative essences of intelligence to those held
	by classical AI. These are the parallel themes of development, social
	interaction, embodiment, and integration. Following a methodology
	based on these themes, we have built a physical humanoid robot.
	In this paper we present our methodology and the insights it affords
	for facilitating learning, simplifying the computation underlying
	rich behavior, and building systems that can scale to more complex
	tasks in more challenging environments.},
  pdf = {D:\bucko\literatureDB\Science\behaviour\brooks98alternative.pdf},
  url = {citeseer.ist.psu.edu/brooks98alternative.html}
}

@CONFERENCE{BICS06,
  author = {Andrea Colombari and Andrea Fusiello and Vittorio Murino},
  title = {Background Initialization in Cluttered Sequences},
  booktitle = {Conference on Computer Vision and Pattern Recognition Workshop},
  year = {2006},
  address = {Verona , Italy},
  abstract = {In this paper we propose a technique to robustly estimate the background
	in a cluttered sequence, i.e., a sequence where occluding objects
	persist in the same position for a considerable portion of time.
	As pixel-level heuristic are not sufficient in this case, we introduce
	spatial support. First the sequence is subdivided in patches that
	are clustered along the time-line in order to narrow down the number
	of background candidates. Then the background is grown incrementally
	by selecting at each step the best continuation of the current background,
	according to the principles of visual grouping. The method rests
	on sound principles in all its stages, and only few, intelligible
	parameters are needed. Experiments with real sequences illustrate
	the approach.},
  institution = {Dipartimento di Informatica - Università degli Studi di Verona},
  isbn = {0-7695-2646-2},
  journal = {Conference on Computer Vision and Pattern Recognition Workshop},
  pdf = {D:\bucko\literatureDB\Science\ImageProcessing\BICS06.pdf},
  timestamp = {2006.10.09},
  url = {http://profs.sci.univr.it/~fusiello/resume/pap-eng.html}
}

@INPROCEEDINGS{RTTNROMS00,
  author = {D. Comaniciu and V. Ramesh and P. Meer},
  title = {Real-Time Tracking of Non-Rigid Objects using Mean Shift},
  year = {2000},
  pages = {142--151},
  timestamp = {2007.06.20},
  url = {citeseer.ist.psu.edu/comaniciu00realtime.html}
}

@INPROCEEDINGS{GF00-09,
  author = {G. Farnebäck},
  title = {Fast and Accurate Motion Estimation using Orientation Tensors and
	Parametric Motion Models},
  booktitle = {Proceedings of 15th International Conference on Pattern Recognition},
  year = {2000},
  volume = {1},
  pages = {135--139},
  address = {Barcelona, Spain},
  month = {September},
  organization = {IAPR},
  abstract = {Motion estimation in image sequences is an important step in many
	computer vision and image processing applications. Several methods
	for solving this problem have been proposed, but very few manage
	to achieve a high level of accuracy without sacrificing processing
	speed.
	
	
	This paper presents a novel motion estimation algorithm, which gives
	excellent results on both counts. The algorithm starts by computing
	3D orientation tensors from the image sequence. These are combined
	under the constraints of a parametric motion model to produce velocity
	estimates. Evaluated on the well-known Yosemite sequence, the algorithm
	shows an accuracy which is substantially better than for previously
	published methods. Computationally the algorithm is simple and can
	be implemented by means of separable convolutions, which also makes
	it fast.},
  annote = {Also as Technical Report LiTH-ISY-R-2254},
  pdf = {G:\Science\Computerscience\Robotics\imageprocessing\GF00-09.pdf},
  timestamp = {2006.09.07},
  url = {http://www.isy.liu.se/cvl/ScOut/Publications/Papers/icpr00_f.pdf}
}

@MISC{WWWFHG07,
  author = {Faulhaber},
  title = {FAULHABER GROUP - FAULHABER Group Brushless Motors and DC Motors,
	http://www.faulhaber-group.com},
  year = {2007},
  organization = {Dr. Fritz Faulhaber GmbH \& Co. KG, Daimlerstr. 23, D-71101 Schönaich,
	Germany},
  timestamp = {2007.03.09},
  url = {http://www.faulhaber-group.com/}
}

@MISC{ROC04-03,
  author = {T. Fawcett},
  title = {ROC Graphs: Notes and Practical Considerations for Data Mining Researchers},
  howpublished = {Kluwer Academic Publishers. Printed in the Netherlands},
  month = {March},
  year = {2003},
  abstract = {Receiver Operating Characteristics (ROC) graphs are a useful technique
	for organizing classifiers and visualizing their performance. ROC
	graphs are commonly used in medical decision making, and in recent
	years have been increasingly adopted in the machine learning and
	data mining research communities. Although ROC graphs are apparently
	simple, there are some common misconceptions and pitfalls when using
	them in practice. This article serves both as a tutorial introduction
	to ROC graphs and as a practical guide for using them in research.},
  keywords = {classification, classifier evaluation, ROC, visualization},
  pdf = {D:\bucko\literatureDB\Science\Signaltheory\ROC04-03.pdf},
  text = {T. Fawcett. ROC Graphs: Notes and Practical Considerations for Data
	Mining Researchers. Technical Report HPL-2003-4, HP Labs, 2003.},
  timestamp = {2007.04.13},
  url = {citeseer.ist.psu.edu/fawcett03roc.html}
}

@MISC{WWWYARP07,
  author = {Paul Fitzpatrick and Giorgio Metta and Lorenzo Natale},
  title = {YARP: Yet Another Robot Platform, http://yarp0.sourceforge.net},
  year = {2007},
  address = {Genova, Italy},
  comment = {http://sourceforge.net/projects/yarp0},
  keywords = {software platform, interprocess communication, humanoid robotics},
  organization = {LIRA-Lab},
  timestamp = {2007.03.07},
  url = {http://eris.liralab.it/yarp/}
}

@MANUAL{YUM07-03,
  title = {YARP User Manual},
  author = {Paul Fitzpatrick and Giorgio Metta and Lorenzo Natale},
  organization = {LIRA-Lab},
  address = {Genova, Italy},
  month = {March},
  year = {2007},
  pdf = {G:\Science\ProgrammingAndTutorials\YUM07-03.pdf},
  timestamp = {2007.03.07},
  url = {http://yarp0.sourceforge.net}
}

@PROCEEDINGS{LAOTA03,
  title = {Learning About Objects Through Action - Initial Steps Towards Artificial
	Cognition},
  year = {2003},
  volume = {3},
  address = {Taipei, Taiwan},
  month = {September},
  abstract = {Within the field of Neuro Robotics we are driven primarily by the
	desire to understand how humans and animals live and grow and solve
	every day’s problems. To this aim we adopted a “learn by doing”
	approach by building artificial systems, e.g. robots that not only
	look like human beings but also represent a model of some brain
	process. They should, ideally, behave and interact like human beings
	(being situated). The main emphasis in robotics has been on systems
	that act as a reaction to an external stimulus (e.g. tracking, reaching),
	rather than as a result of an internal drive to explore or “understand”
	the environment. We think it is now appropriate to try to move from
	acting, in the sense explained above, to “understanding”. As a starting
	point we addressed the problem of learning about the effects and
	consequences of self-generated actions. How does the robot learn
	how to pull an object toward itself or to push it away? How does
	the robot learn that spherical objects roll while a cube only slides
	if pushed? Interacting with objects is important because it implicitly
	explores object representation, event understanding, and can provide
	definition of objecthood that could not be grasped with a mere passive
	observation of the world. Further, learning to understand what one’s
	own body can do is an essential step toward learning by imitation.
	In this view two actions are similar not only if their kinematics
	and dynamics are similar but rather if the effects on the external
	world are the same. Along this line of research we discuss some
	recent experiments performed at the AILab at MIT and at the LIRA-Lab
	at the University of Genova on COG and Babybot respectively. We
	show how the humanoid robots can learn how to poke and prod objects
	to obtain a consistently repeatable effect (e.g. sliding in a given
	direction), to help visual segmentation, and to interpret a poking
	action performed by a human manipulator.},
  author = {Paul Fitzpatrick and Giorgio Metta and Lorenzo Natale and Sajit Rao
	and Giulio Sandini},
  booktitle = {Proc. IEEE International Conference on Robotics and Automation (ICRA)},
  citeseerurl = {citeseer.ist.psu.edu/fitzpatrick03learning.html},
  pages = {3140- 3145},
  pdf = {D:\bucko\literatureDB\Science\Robotics\LAOTA03.pdf},
  timestamp = {2007.04.05}
}

@UNPUBLISHED{SCOPRI,
  author = {Paul Fitzpatrick and Amy Needham and Lorenzo Natale and Giorgio Metta},
  title = {Shared Challenges in Object Perception for Robots and Infants},
  note = {unpublished yet, penultimate draft},
  year = {2006},
  journal = {Journal of Infant and Child Development},
  timestamp = {2007.04.11},
  url = {http://www.robotcub.org/misc/review2/06_Fitzpatrick_Needham_Natale_Metta.pdf}
}

@BOOK{DPEROOS95-01,
  title = {Design Patterns: Elements of Reusable Object-Oriented Software},
  publisher = {Addison-Wesley Professional},
  year = {1995},
  author = {Gamma, E. and Helm, R. and Johnson, R. and Vlissides, J. },
  month = {January},
  keywords = {design patterns},
  timestamp = {2007.03.08}
}

@ARTICLE{LAWC06,
  author = {Kevin Gold and Brian Scassellati},
  title = {Learning Acceptable Windows of Contingency},
  journal = {Connection Science},
  year = {2006},
  volume = {18},
  pages = {217-228},
  number = {2},
  month = {June},
  note = {To appear},
  abstract = {By learning a range of possible times over which the effect of an
	action can take place, a robot can reason more effectively about
	causal and contingent relationships in the world. However, learning
	these time windows in a noisy environment where random events interfere
	can pose a challenge. We present an algorithm for learning the interval
	[t1min, t1max] of possible times during which a response to an action
	can take place, and implement the model on a physical robot for
	the domains of visual self-recognition and auditory social-partner
	recognition. The environment model that we use to justify our error
	bounds assumes that natural environments generate Poisson distributions
	of random events at all scales. From this assumption, we derive
	a lineartime algorithm, which we call Poisson threshold learning,
	for finding a threshold T that provides an arbitrarily small rate
	of background events lambda(T) if such a threshold exists for the
	specified error rate. We can then use this rate to calculate an
	expected number of false positives in our sample data and discard
	them. We implement the principles of our method using a motion detection
	module as our input stream in the visual domain, and sampled audio
	energy in the auditory domain. In this way, we find time windows
	for self-generated motion, self-generated audio, and verbal social
	responses. We also present data on the distributions of these events,
	showing that while our self-generated action had a normal distribution,
	the social events were better modeled by a Poisson process. Finally,
	we present several applications for which such simple classifiers
	could potentially prove useful, such as mirror selfrecognition and
	learning the meanings of the words “I” and “you.”},
  institution = {Department of Computer Science Yale University, New Haven, CT, USA},
  pdf = {G:\Science\Computerscience\Robotics\LAWC06.pdf},
  publisher = {Taylor and Francis Ltd},
  timestamp = {2006.06.05},
  url = {http://www.cs.yale.edu/homes/scaz/publications.html}
}

@BOOK{ACEPG03-11,
  title = {The ACE Programmers Guide: Practical Design Patterns for Network
	and Systems Programming},
  publisher = {Addison-Wesley Longman Publishing Co., Inc.},
  year = {2003},
  author = {Stephen D. Huston and James Johnson and Umar Syyid},
  pages = {576},
  address = {Boston, MA},
  month = {November},
  note = {ISBN 0201699710},
  abstract = {"The ACE Programmer's Guide" is a practical, hands-on book to ACE
	for C++ programmers building networked applications and next-generation
	middleware. The book first introduces ACE to beginners, then explains
	how to tap design patterns, frameworks, and ACE to produce effective,
	easily maintained software systems with less time and effort.
	
	
	Synopsis
	
	"If you're designing software and systems that must be portable, flexible,
	extensible, predictable, reliable, and affordable, this book and
	the ACE toolkit will enable you to be more effective in all of these
	areas. Even after spending over a decade developing ACE and using
	it to build networked software applications, I find that I've learned
	a great deal from this book, and I'm confident that you will, too."
	- Douglas C. Schmidt, Inventor of ACE, from the Foreword. "This
	book is a must-have for every ACE programmer. For the beginner,
	it explains step-by-step how to start using ACE. For the more experienced
	programmer, it explains in detail the features used daily, and is
	a perfect reference manual. It would have saved me a lot of time
	if this book had been available some years ago!" - Johnny Willemsen,
	Senior Software Engineer, Remedy IT, The Netherlands. "With a large
	C++ code base, we rely on ACE to enable a cross-platform client-server
	framework for data quality and data integration. ACE has improved
	our design and smoothed over OS idiosyncrasies without sacrificing
	performance or flexibility. The combination of online reference
	materials and printed "big picture" guides is indispensable for
	us, and The ACE Programmer's Guide earns top-shelf status in my
	office." - John Lilley, Chief Scientist, Data Lever Corporation.
	"In SITA air-ground division, we are one of the major suppliers
	of communication services to the airline industry. We started using
	ACE about a year ago and are now moving most of our new communication-related
	development to it. I can say that using this toolkit can reduce
	the development and testing time by at least 50 per cent in our
	type of application." - Jean Millo, Senior Architect, SITA. The
	ADAPTIVE Communication Environment (ACE) is an open-source software
	toolkit created to solve network programming challenges. Written
	in C++, with the help of 30 core developers and 1,700 contributors,
	this portable middleware has evolved to encapsulate and augment
	a wide range of native OS capabilities essential to support performance-driven
	software systems. "The ACE Programmer's Guide" is a practical, hands-on
	guide to ACE for C++ programmers building networked applications
	and next-generation middleware. The book first introduces ACE to
	beginners. It then explains how you can tap design patterns, frameworks,
	and ACE to produce effective, easily maintained software systems
	with less time and effort. The book features discussions of programming
	aids, interprocess communication (IPC) issues, process and thread
	management, shared memory, the ACE Service Configurator framework,
	timer management classes, the ACE Naming Service, and more. The
	accompanying CD-ROM contains the complete ACE toolkit, including
	installable kits for Windows, Solaris, and HP-UX; complete reference
	documentation for all of the ACE classes; and source code for every
	example in the book.},
  timestamp = {2007.03.08}
}

@ARTICLE{TVSC96-10,
  author = {S. A. Hutchinson and G. D. Hager and P. I. Corke},
  title = {A tutorial on visual servo control},
  journal = {IEEE Trans. Robotics and Automation},
  year = {1996},
  volume = {12},
  pages = {651--670},
  number = {5},
  month = {October},
  timestamp = {2007.04.08},
  url = {citeseer.ist.psu.edu/hutchinson96tutorial.html}
}

@TECHREPORT{TVSC96-05,
  author = {Seth Hutchinson and Greg Hager and Peter Corke},
  title = {A tutorial on visual servo control},
  institution = {Department of Electrical and Computer Engineering, The Beckman Institute
	for Advanced Science and Technology, University of Illinois at Urbana-Champaign
	and Department of Computer Science, Yale University and CSIRO Division
	of Manufacturing Technology},
  year = {1996},
  address = {Kenmore, Australia},
  month = {May},
  timestamp = {2007.04.08}
}

@MISC{WWWOCV07,
  author = {Intel},
  title = {Open Source Computer Vision Library, http://www.intel.com/technology/computing/opencv\-/index.htm},
  year = {2007},
  comment = {http://opencvlibrary.sourceforge.net},
  organization = {Intel Corp.},
  timestamp = {2007.03.08},
  url = {http://www.intel.com/technology/computing/opencv/index.htm}
}

@MISC{WWWIS07,
  author = {Intersence},
  title = {Intersense Inc. Precision Motion Tracking Solutions, http://www.isense.com},
  year = {2007},
  keywords = {Tracking, Gyroscope},
  organization = {Intersense Inc., 36 Crosby Drive, Suite 150, Bedford, MA 01730, USA},
  pdf = {G:\Science\james\WWWISO07.pdf},
  timestamp = {2007.03.11},
  url = {http://www.isense.com/}
}

@ARTICLE{DCAR99,
  author = {A. K. Jain and M. N. Murty and P. J. Flynn},
  title = {Data clustering: a review},
  journal = {ACM Computing Surveys},
  year = {1999},
  volume = {31},
  pages = {264--323},
  number = {3},
  pdf = {D:\bucko\literatureDB\Science\Classification\DCAR99.pdf},
  timestamp = {2007.04.06},
  url = {citeseer.ist.psu.edu/jain99data.html}
}

@INPROCEEDINGS{JHRAUW06-12,
  author = {Lorenzo Jamone and Giorgio Metta and Franscesco Nori and Giulio Sandini},
  title = {James: A Humanoid Robot Acting over an Unstructured World},
  booktitle = {6th IEEE-RAS International Conference on Humanoid Robots},
  year = {2006},
  pages = {143-150},
  address = {Genova, Italy},
  month = {December},
  note = {Humanoids},
  abstract = {The recent trend of humanoid robotics research has been deeply influenced
	by concepts such as embodiment, embodied interaction and emergence.
	In our view, these concepts, beside shaping the controller, should
	guide the very design process of the modern humanoid robotic platforms.
	In this paper, we discuss how these principles have been applied
	to the design of a humanoid robot called James. James has been designed
	by considering an object manipulation scenario and by explicitly
	taking into account embodiment, interaction and the exploitation
	of smart design solutions. The robot is equipped with moving eyes,
	neck, arm and hand, and a rich set of sensors, enabling proprioceptive,
	kinesthetic, tactile and visual sensing. A great deal of effort
	has been devoted to the design of the hand and touch sensors. Experiments,
	e.g. tactile object classification, have been performed, to validate
	the quality of the robot perceptual capabilities.},
  pdf = {G:\Science\james\JHRAUW06-12.pdf},
  timestamp = {2007.03.10},
  url = {http://www.robotcub.org/misc/review2/06\_Jamone\_Metta\_Nori\_Sandini.pdf}
}

@MISC{jung-rectangle,
  author = {Claudio Rosito Jung and Rodrigo Schramm},
  title = {Rectangle Detection based on a Windowed Hough Transform},
  url = {citeseer.ist.psu.edu/rositojung04rectangle.html}
}

@ARTICLE{APII01-07,
  author = {John van der Kamp and Geert Savelsbergh},
  title = {Action and perception in infancy},
  journal = {Infant Behaviour and Development},
  year = {2000},
  volume = {23},
  pages = {237-251},
  number = {3-4},
  month = {March},
  note = {Elsevier},
  abstract = {In this introduction to the special issue on Action and perception
	in infancy, we summarize recent arguments of Milner and Goodale
	for a dissociation between vision for action and vision for perception
	in adults. We propose that this dissociation is probably present
	from birth, and further that vision for action and vision for perception
	follow different developmental trajectories. Based on this framework,
	we discern four themes of interest for infancy research (i.e., information
	and action, exploration and action, information and perception,
	and the interaction between action and perception) and introduce
	the contributions of the special issue accordingly.},
  doi = {doi:10.1016/S0163-6383(01)00071-6},
  keywords = {Action; Perception; Exploration; Information; Vision; Infancy},
  pdf = {G:\Science\InfantBehaviourAndDevelopment\APII01-07.pdf},
  timestamp = {2007.02.05}
}

@MISC{WCIC06,
  author = {Charles C. Kemp and Aaron Edsinger},
  title = {What Can I Control?: The Development of Visual Categories for a Robot's
	Body and the World that it Influences},
  year = {2006},
  abstract = {We present a perceptual system for a humanoid robot that autonomously
	discovers its hand from less than 5 minutes of natural interaction
	with a human. The perceptual system combines simple proprioceptive
	sensing with a visual attention system that uses motion to select
	salient regions. We show that during natural interactions with a
	person, the selected visual regions primarily correspond with the
	person’s head, eyes, hands, and fingers, and the robot’s hand and
	fingers. The system clusters the selected image regions, models
	their spatial distribution over a sensory ego-sphere, and uses mutual
	information to determine how well the robot’s arm can control each
	cluster. In our tests, the cluster that is best controlled by the
	robot’s arm primarily contains images of the robot’s hand. It also
	has a spatial distribution that can predict the visual location
	of the robot’s hand as a function of the arm’s configuration.},
  institution = {Computer Science and Artificial Intelligence Laboratory, Massachusetts
	Institute of Technology, Cambridge, Massachusetts},
  pdf = {G:\Science\Computerscience\Robotics\WCIC06.pdf},
  timestamp = {2006.06.05},
  url = {http://people.csail.mit.edu/cckemp/publications.shtml}
}

@INPROCEEDINGS{WCIC06new,
  author = {Charles C. Kemp and Aaron Edsinger},
  title = {What Can I Control?: The Development of Visual Categories for a Robot's
	Body and the World that it Influences},
  booktitle = {Proceedings of the Fifth International Conference on Development
	and Learning, Special Session on Autonomous Mental Development.},
  year = {2006},
  month = {June},
  abstract = {We present a perceptual system for a humanoid robot that autonomously
	discovers its hand from less than 5 minutes of natural interaction
	with a human. The perceptual system combines simple proprioceptive
	sensing with a visual attention system that uses motion to select
	salient regions. We show that during natural interactions with a
	person, the selected visual regions primarily correspond with the
	person’s head, eyes, hands, and fingers, and the robot’s hand and
	fingers. The system clusters the selected image regions, models
	their spatial distribution over a sensory ego-sphere, and uses mutual
	information to determine how well the robot’s arm can control each
	cluster. In our tests, the cluster that is best controlled by the
	robot’s arm primarily contains images of the robot’s hand. It also
	has a spatial distribution that can predict the visual location
	of the robot’s hand as a function of the arm’s configuration.},
  institution = {Computer Science and Artificial Intelligence Laboratory, Massachusetts
	Institute of Technology, Cambridge, Massachusetts},
  pdf = {G:\Science\Computerscience\Robotics\WCIC06new.pdf},
  timestamp = {2006.06.08},
  url = {http://people.csail.mit.edu/cckemp/publications.shtml}
}

@TECHREPORT{SVSM01,
  author = {Danica Kragic and Henrik I Christensen},
  title = {Survey on Visual Servoing for Manipulation},
  institution = {Computer Vision and Active Perception lab},
  year = {2001},
  address = {Stockholm, Sweden},
  month = {January},
  timestamp = {2007.04.09}
}

@MISC{WWWLL-07,
  author = {LIRA-Lab},
  title = {Laboratorio Integrato di Robotica Avanzata (LIRA-Lab), http://www.liralab.it},
  year = {2007},
  note = {Laboratory for Integrated Advanced Robotics},
  address = {Genova, Italy},
  institution = {Department of Communication, Computer and Systems Science (DIST)}
}

@INPROCEEDINGS{IIRTASV81,
  author = {Bruce D. Lucas and Takeo Kanade},
  title = {An Iterative Image Registration Technique with an Application to
	Stereo Vision},
  booktitle = {IJCAI81},
  year = {1981},
  pages = {674-679},
  timestamp = {2007.03.13},
  url = {citeseer.ist.psu.edu/lucas81iterative.html}
}

@ARTICLE{DRS04,
  author = {Max Lungarella and Giorgio Metta and Rolf Pfeifer and Giulio Sandini},
  title = {Developmental robotics: a survey},
  journal = {Connection Science},
  year = {2003},
  volume = {15},
  pages = {151 - 190},
  number = {4},
  month = {December},
  note = {Taylor \& Francis Ltd},
  abstract = {Developmental robotics is an emerging field located at the intersection
	of robotics, cognitive science and developmental sciences. This
	paper elucidates the main reasons and key motivations behind the
	convergence of fields with seemingly disparate interests, and shows
	why developmental robotics might prove to be beneficial for all
	fields involved. The methodology advocated is synthetic and two-pronged:
	on the one hand, it employs robots to instantiate models originating
	from developmental sciences; on the other hand, it aims to develop
	better robotic systems by exploiting insights gained from studies
	on ontogenetic development. This paper gives a survey of the relevant
	research issues and points to some future research directions.},
  doi = {DOI: 10.1080/09540090310001655110},
  keywords = {development; embodied cognitive science; robotics; synthetic methodology},
  pdf = {G:\Science\ConnectionScience\DRS04.pdf},
  timestamp = {2007.02.05},
  url = {http://www.tandf.co.uk/journals}
}

@MISC{BVTM03-07,
  author = {Giorgio Metta and Paul Fitzpatrick},
  title = {Better Vision Through Manipulation},
  month = {July},
  year = {2003},
  abstract = {Vision and manipulation are inextricably intertwined in the primate
	brain. Tantalizing results from neuroscience are shedding light
	on the mixed motor and sensory representations used by the brain
	during reaching, grasping, and object recognition. We now know a
	great deal about what happens in the brain during these activities,
	but not necessarily why. Is the integration we see functionally
	important, or just a reflection of evolution’s lack of enthusiasm
	for sharp modularity? We wish to instantiate these results in robotic
	form to probe the technical advantages and to find any lacunae in
	existing models. We believe it would be missing the point to investigate
	this on a platform where dextrous manipulation and sophisticated
	machine vision are already implemented in their mature form, and
	instead follow a developmental approach from simpler primitives.
	We begin with a precursor to manipulation, simple poking and prodding,
	and show how it facilitates object segmentation, a long-standing
	problem in machine vision. The robot can familiarize itself with
	the objects in its environment by acting upon them. It can then
	recognize other actors (such as humans) in the environment through
	their effect on the objects it has learned about. We argue that
	following causal chains of events out from the robot's body into
	the environment allows for a very natural developmental progression
	of visual competence, and we relate this idea to results in neuroscience.},
  keywords = {humanoid robotics, active segmentation, epigenesis},
  pdf = {H:\LiteratureLibrary\BVTM03-07.pdf},
  timestamp = {2006.05.12}
}

@ARTICLE{YARP06-03,
  author = {Giorgio Metta and Paul Fitzpatrick and Lorenzo Natale},
  title = {YARP: Yet Another Robot Platform},
  journal = {ARS International : Advanced Robotics Systems},
  year = {2006},
  volume = {3},
  pages = {43-48},
  number = {1},
  month = {March},
  abstract = {We describe YARP, Yet Another Robot Platform, an open-source project
	that encapsulates lessons from our experience in building humanoid
	robots. The goal of YARP is to minimize the effort devoted to infrastructure-level
	software development by facilitating code reuse, modularity and
	so maximize research-level development and collaboration. Humanoid
	robotics is a “bleeding edge” field of research, with constant flux
	in sensors, actuators, and processors. Code reuse and maintenance
	is therefore a significant challenge. We describe the main problems
	we faced and the solutions we adopted. In short, the main features
	of YARP include support for inter-process communication, image processing
	as well as a class hierarchy to ease code reuse across different
	hardware platforms. YARP is currently used and tested on Windows,
	Linux and QNX6 which are common operating systems used in robotics.},
  keywords = {software platform, interprocess communication, humanoid robotics},
  pdf = {H:\LiteratureLibrary\YARP06-03.pdf},
  timestamp = {2006.05.18}
}

@TECHREPORT{PRTL07-01,
  author = {Giorgio Metta and Lorenzo Natale and Franscesco Nori},
  title = {Precise Reaching through learning},
  institution = {LIRA-Lab and IIT},
  year = {2007},
  address = {Genova, Italy},
  month = {January},
  timestamp = {2007.03.24}
}

@ARTICLE{DAVGRAS99-12,
  author = {Giorgio Metta and Giulio Sandini and J. Konczak},
  title = {A developmental approach to visually-guided reaching in artificial},
  journal = {Neural Networks},
  year = {1999},
  volume = {12},
  pages = {1413-1427},
  abstract = {The aim of the present paper is to propose that the adoption of a
	framework of biological development is suitable for the construction
	of artificial systems. We will argue that a developmental approach
	does provide unique insights on how to build highly complex and
	adaptable artificial systems. To illustrate our point, we will use
	as an example the acquisition of goal-directed reaching. In the
	initial part of the paper we will outline (a) how mechanisms of
	biological development can be adapted to the artificial world, and
	(b) how this artificial development differs from traditional engineering
	approaches to robotics. An experiment performed on an artificial
	system initially controlled by motor reflexes is presented, showing
	the acquisition of visuo-motor maps for ballistic control of reaching
	without explicit knowledge of the system’s kinematic parameters.},
  keywords = {Development; Learning; Motor control; Human infants; Robotics; Artificial
	systems},
  pdf = {H:\LiteratureLibrary\ArtificialIntelligence\DAVGRAS99-12.pdf},
  timestamp = {2006.05.15}
}

@CONFERENCE{ADAP03,
  author = {Giorgio Metta and Giulio Sandini and Lorenzo Natale and Riccardo
	Manzotti},
  title = {Artificial Development Approach to Presence},
  booktitle = {In Presence 2003. Aalborg, DK. Oct 6-8th, 2003.},
  year = {2003},
  note = {In Presence 2003. Aalborg, DK. Oct 6-8th, 2003.},
  abstract = {In Presence 2003. Aalborg, DK. Oct 6-8th, 2003.},
  pdf = {G:\Science\Computerscience\Robotics\ADAP03.pdf},
  timestamp = {2007.02.05}
}

@PHDTHESIS{HRDAG04,
  author = {Lorenzo Natale},
  title = {Linking Action to Perception in a Humanoid Robot: A Developmental
	Approach to Grasping},
  school = {LIRA-Lab, DIST, University Of Genoa},
  year = {2004},
  address = {Genova, Italy},
  abstract = {In this thesis we propose a developmental approach to the design of
	a humanoid robot. We present a possible sequence of developmental
	stages which starting from limited knowledge enables the robot to
	autonomously learn to perform goal directed actions on objects (reaching,
	pushing, and a simple form of grasping). The robot initial knowledge
	consists in a few visual algorithms (disparity, tracking, motion
	detection) and motor synergies providing a rudimentary form of sensorimotor
	coordination useful to begin interaction with the environment. During
	the initial steps of development the robot learns to recognize and
	control its own body (gazing, localization of the hand); based on
	these abilities it moves afterward to the exploration of the external
	world (reaching and grasping).
	
	We stress the importance of the physical interaction between the robot’s
	body and the environment and the advantage of exploiting actions
	to simplify and learn perceptual as well as motor tasks (e.g. distinguishing
	the hand from the background, recognizing objects based on tactile
	experience, pushing/pulling objects on a table).
	
	This approach is inspired by the observation of how mature behaviors
	emerge in infants during development and by recent theories in neural
	sciences proposing that the link between action and perception might
	be at the basis of higher level, abstract functions like action
	recognition, imitation and language. These considerations and the
	experimental results reported in the thesis support the conviction
	that our approach is indeed worth pursuing as it is perhaps the
	only route toward the realization of cognitive abilities in an artificial
	system.},
  pdf = {G:\Science\Computerscience\Robotics\HRDAG04.pdf},
  timestamp = {2006.06.08}
}

@MISC{DAG05-03,
  author = {Lorenzo Natale and Giorgio Metta and Giulio Sandini},
  title = {A developmental Approach to Grasping},
  howpublished = {Developmental Robotics 2005 AAAI Spring Symposium, Stanford University,
	Stanford, CA},
  month = {March},
  year = {2005},
  timestamp = {2007.04.11}
}

@MISC{LAOTG05,
  author = {Lorenzo Natale and Francesco Orabona and Giorgio Metta and Giulio
	Sandini},
  title = {Sensorimotor coordination in a humanoid robot: learning about objects
	through grasping},
  year = {2005},
  abstract = {This paper describes a developmental approach to the design of a humanoid
	robot. The robot, equipped with initial perceptual and motor competencies,
	explores the “shape” of its own body before devoting its attention
	to the external environment. The initial form of sensorimotor coordination
	consists of a set of reflexive and explorative motor behaviors coupled
	to an ensemble of visual routines providing a bottomup attention
	system. The developmental path leads the robot from the construction
	of a “body schema” to the exploration of the world of objects. The
	“body schema” allows controlling the arm and hand to reach and touch
	objects within the robot’s workspace. Eventually, the interaction
	between the environment and the robot’s body is exploited to acquire
	a visual model of the objects the robot interacts with. In turn
	object models can be used to guide a top-down attention system.
	We discuss the implications of our approach in the study of cognition
	and our effort to build a cognitive artificial system.},
  address = {DIST, University of Genova, Viale Causa 13, 16145 Genova, Italy},
  institution = {LIRA-Lab},
  keywords = {humanoid robotics, attention system, body-schema, development, grasping},
  pdf = {G:\Science\Computerscience\Robotics\LAOTG05.pdf},
  timestamp = {2006.05.18}
}

@MISC{ACMP06,
  author = {Franscesco Nori and Giorgio Metta and L. Jamone and Giulio Sandini},
  title = {Adaptive combination of motor primitives},
  year = {2006},
  abstract = {Recently there has been a growing interest in modeling motor control
	systems with modular structures. Such control structures have many
	interesting properties, which have been described in recent studies.
	We here focus on some properties which are related to the fact that
	specific set of contexts can themselves be modeled modularly.},
  pdf = {H:\LiteratureLibrary\ACMP06.pdf},
  timestamp = {2006.05.12}
}

@MISC{LVSRRME02,
  author = {Francesco Panerai and Giorgio Metta and Giulio Sandini},
  title = {Learning visual stabilization reflexes in robots with moving eyes},
  year = {2002},
  abstract = {This work addresses the problem of learning stabilization reflexes
	in robots with moving eyes. Most essential in achieving efficient
	visual stabilization is the exploitation/integration of different
	motion related sensory information. In our robot, self-motion is
	measured inertially with an artificial vestibular system and visually
	using optic flow algorithms. The first sensory system provides short
	latency measurements of rotations and translations of the robot’s
	head, the second, a delayed estimate of the motion across the image
	plane. A self-tuning neural network learns to combine these two
	measurements and generates oculo-motor compensatory behaviors that
	stabilize the visual scene. We describe the network architecture
	and the learning scheme. The stabilization performance is evaluated
	quantitatively using direct measurements on the image plane.},
  keywords = {Robotics; Artificial neural networks; Optic flow; Inertial information;
	Image
	
	stabilization; Oculo-motor reflexes; Sensori-motor maps},
  pdf = {H:\LiteratureLibrary\LVSRRME02.pdf},
  timestamp = {2006.05.18}
}

@MISC{WWWPGDF07,
  author = {PointGreyResearch},
  title = {PointGrey Dragonfly CCD Camera, http://www.ptgrey.com/products/dragonfly/index.asp},
  year = {2007},
  comment = {Eyes of James},
  keywords = {CCD camera, IEEE-1394 board level camera},
  organization = {Point Grey Research},
  pdf = {G:\Science\james\WWWPGDF07.pdf},
  timestamp = {2007.03.10},
  url = {http://www.ptgrey.com/products/dragonfly/index.asp}
}

@ARTICLE{FLSAUEL03-02,
  author = {Philippe Rochat},
  title = {Five levels of self-awareness as they unfold early in life},
  journal = {Consciousness and Cognition},
  year = {2003},
  volume = {12},
  pages = {717-731},
  number = {4},
  month = {December},
  note = {Elsevier},
  abstract = {When do children become aware of themselves as differentiated and
	unique entity in the world? When and how do they become self-aware?
	Based on some recent empirical evidence, 5 levels of self-awareness
	are presented and discussed as they chronologically unfold from
	the moment of birth to approximately 4–5 years of age. A natural
	history of children’s developing self-awareness is proposed as well
	as a model of adult self-awareness that is informed by the dynamic
	of early development. Adult self-awareness is viewed as the dynamic
	flux between basic levels of consciousness that develop chronologically
	early in life.},
  doi = {doi:10.1016/S1053-8100(03)00081-3},
  pdf = {G:\Science\ConsciousnessAndCognition\FLSAUEL03-02.pdf},
  timestamp = {2007.02.05},
  url = {http://www.psychology.emory.edu/cognition/rochat/publications.html}
}

@ARTICLE{PSII00-06,
  author = {Philippe Rochat and Tricia Striano},
  title = {Perceived self in infacy},
  journal = {Infant Behaviour and Development},
  year = {2000},
  volume = {23},
  pages = {513-530},
  number = {3-4},
  month = {March},
  note = {Elsevier},
  abstract = {Research is presented suggesting that an implicit sense of self is
	developing from birth, long before children begin to manifest explicit
	(conceptual) self-knowledge by the second year. Implicit selfknowledge
	in infancy is rooted in intermodal perception and action. Studies
	are reported showing that at least from 2 months of age, infants
	become increasingly systematic and deliberate in the exploration
	of their own body and the perceptual consequences of self-produced
	action. From such exploration, infants develop a sense of their
	own body as a differentiated entity, situated and agent in the environment.
	Based on recent empirical findings, the perceptual determinants
	of such implicit sense of self are discussed.
	
	© 2000 Elsevier Science Inc. All rights reserved.},
  doi = {doi:10.1016/S0163-6383(01)00055-8},
  keywords = {Perception; Self; Infancy},
  pdf = {G:\Science\InfantBehaviourAndDevelopment\PSII00-06.pdf},
  timestamp = {2007.02.05},
  url = {http://www.psychology.emory.edu/cognition/rochat/publications.html}
}

@ARTICLE{ESEBOI99,
  author = {Philippe Rochat and Tricia Striano},
  title = {Emerging self-exploration by 2-month-old infants},
  journal = {Developmental Science},
  year = {1999},
  volume = {2},
  pages = {206-218},
  number = {2},
  month = {May},
  note = {Blackwell Publishers},
  abstract = {Two-month-olds and newborns were tested in a situation where they
	had the opportunity to experience different auditory consequences
	of their own oral activity on a dummy pacifier. Modulation of oral
	activity was scored and analyzed relative to two types of contingent
	auditory feedback, either analog or non-analog to the effort exerted
	by the infant on the pacifier. The dummy pacifier was connected
	to an air pressure transducer for recording of oral action. In two
	different experimental conditions, each time the infant sucked above
	a certain pressure threshold they heard a perfectly contingent sound
	of varying pitch. In one condition, the pitch variation was analog
	to the pressure applied by the infant on the pacifier (analog condition).
	In another, the pitch variation was random (non-analog condition).
	As rationale, a differential modulation of oral activity in these
	two conditions was construed as indexing some voluntary control
	and the sense of a causal link between sucking and its auditory
	consequences, beyond mere temporal contingency detection and response–stimulus
	association. Results indicated that 2-month-olds showed clear signs
	of modulation of their oral activity on the pacifier as a function
	of analog versus non-analog condition. In contrast, newborns did
	not show any signs of such modulation either between experimental
	conditions (analog versus non-analog contingent sounds) or between
	baseline (no contingent sounds condition) and experimental conditions.
	These observations are interpreted as evidence of self-exploration
	and the emergence of a sense of self-agency by 2 months of age.},
  doi = {doi:10.1111/1467-7687.00069},
  pdf = {G:\Science\DevelopmentalScience\ESEBOI99.pdf},
  timestamp = {2007.02.05},
  url = {http://www.psychology.emory.edu/cognition/rochat/publications.html}
}

@ARTICLE{ROFREC04-11,
  author = {Giulio Sandini and Giorgio Metta and David Vernon et al.},
  title = {RobotCub: An Open Framework for Research in Embodied Cognition},
  journal = {International Journal of Humanoid Robotics},
  year = {2004},
  volume = {8},
  pages = {1-20},
  number = {2},
  month = {November},
  keywords = {Cognition; Co-Development; Open Szstems; Embodiment; Exploration;
	Manipulation; Interaction; Imitation},
  pdf = {D:\bucko\literatureDB\Science\Robotics\ROFREC04-11.pdf},
  timestamp = {2007.04.03}
}

@INPROCEEDINGS{RORIEC04,
  author = {Giulio Sandini and Giorgio Metta and David Vernon},
  title = {RobotCub: An Open Research Initiative in Embodied Cognition},
  booktitle = {Third International Conference on Development and Learning (ICDL'04)
	Developing Social Brains},
  year = {2004},
  address = {La Jolla, CA, USA},
  month = {October},
  pdf = {D:\bucko\literatureDB\Science\Robotics\04_Sandini_Metta_Vernon_ICDL.pdf},
  timestamp = {2007.04.03}
}

@MISC{WWWACE-06,
  author = {Douglas C. Schmidt},
  title = {The ADAPTIVE Communication Environment (ACE), http://www.cs.wustl.edu/~schmidt/ACE.html},
  year = {2006},
  timestamp = {2007.03.08},
  url = {http://www.cs.wustl.edu/~schmidt/ACE.html}
}

@ARTICLE{NCIS00-08,
  author = {Jianbo Shi and Jitendra Malik},
  title = {Normalized Cuts and Image Segmentation},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {2000},
  volume = {22},
  pages = {888-905},
  number = {8},
  month = {August},
  citeseer = {citeseer.ist.psu.edu/shi97normalized.html},
  keywords = {Grouping, image segmentation, graph partitioning},
  pdf = {D:\bucko\literatureDB\Science\ImageProcessing\NCIS00-08.pdf},
  url = {http://www.cs.berkeley.edu/~malik/papers/SM-ncut.pdf}
}

@INPROCEEDINGS{GFT94-06,
  author = {Jianbo Shi and Carlo Tomasi},
  title = {Good Features to Track},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR'94)},
  year = {1994},
  address = {Seattle},
  month = {June},
  abstract = {No feature-based vision system can work unless good features can be
	identified and tracked from frame to frame. Although tracking itself
	is by and large a solved problem, selecting features that can be
	tracked well and correspond to physical points in the world is still
	hard. We propose a feature selection criterion that is optimal by
	construction because it is based on how the tracker works, and a
	feature monitoring method that can detect occlusions, disocclusions,
	and features that do not correspond to points in the world. These
	methods are based on a new tracking algorithm that extends previous
	Newton-Raphson style search methods to work under affine image transformations.
	We test performance with several simulations and experiments.},
  pdf = {D:\bucko\literatureDB\Science\ImageProcessing\GFT94-06.pdf},
  timestamp = {2007.02.23},
  url = {citeseer.ist.psu.edu/shi94good.html}
}

@MISC{MCRSN06,
  author = {Olaf Sporns and Jeremy Karnowski and Max Lungarella},
  title = {Mapping Causal Relations in Sensorimotor Networks},
  year = {2006},
  abstract = {The identification and quantification of couplings between the individual
	components of a complex system can shed light on its hidden dynamics
	and provide insights about its mechanistic basis. Embodied cognition
	emerges and develops largely from the dynamic interactions in the
	coupled system formed by brain, body, and environment. A crucial
	problem is how to quantify the informational structure present in
	the dynamic network formed by such interactions. The focus of this
	paper is on the identification and mapping of causal relations in
	sensorimotor networks. Our analyses of several sensory and motor
	variables collected from two different robotic platforms reveal
	the presence of causal structure induced by dynamically coupled
	sensorimotor activity. Causal links between sensory and motor states
	are spatially and temporally specific, and are sensitive to changing
	environments and movement strategies. Mapping causal relations across
	brain, body and environment allows the objective analysis of embodied
	sensorimotor interaction.},
  institution = {Department of Psychological and Brain Sciences, Indiana University,
	Bloomington, IN 47405, USA
	
	Department of Mechano-Informatics, University of Tokyo, 113-8656 Tokyo,
	Japan},
  keywords = {Embodiment, Robotics, Complexity, Causal Networks, Development},
  pdf = {G:\Science\Computerscience\Robotics\MCRSN06.pdf},
  timestamp = {2006.06.09}
}

@INPROCEEDINGS{DERBS04-02,
  author = {Alexander Stoytchev},
  title = {Development and Extension of the Robot Body Schema},
  booktitle = {Proceedings Third International Workshop on Epigenetic Robotics:
	Modeling Cognitive Development in Robotic Systems},
  year = {2004},
  volume = {101},
  pages = {179-180},
  month = {February},
  abstract = {Stoytchev, Alexander (2003) Development and Extension of the Robot
	Body Schema. In Prince, Christopher G. and Berthouze, Luc and Kozima,
	Hideki and Bullock, Daniel and Stojanov, Georgi and Balkenius, Christian,
	Eds. Proceedings Third International Workshop on Epigenetic Robotics:
	Modeling Cognitive Development in Robotic Systems 101, pages pp.
	179-180, Boston, MA, USA.},
  institution = {Sweden, Lund: Lund University Cognitive Studies.},
  keywords = {sense of body, robotic body schema, body icon, robotic manipulator},
  pdf = {G:\Science\Computerscience\Robotics\DERBS04-02.pdf},
  timestamp = {2007.02.05},
  url = {http://cogprints.org/3354/}
}

@MISC{WWWQNX07,
  author = {QNX Software Systems},
  title = {QNX Software Systems. Realtime operating system software, development
	tools, and services for superior embedded design., http://www.qnx.com/},
  year = {2007},
  timestamp = {2007.05.02},
  url = {http://www.qnx.com/}
}

@CONFERENCE{HHAADSUM05,
  author = {Winnie Tsang and Karan Singh and Eugene Fiume},
  title = {Helping Hand: An Anatomically Accurate Inverse Dynamics Solution
	For Unconstrained Hand Motion},
  booktitle = {Eurographics - ACM SIGGRAPH Symposium on Computer Animation 2005},
  year = {2005},
  editor = {K. Anjyo and P. Faloutsos},
  pages = {1-10},
  organization = {Department of Computer Science, University of Toronto},
  timestamp = {2007.04.19}
}

@INBOOK{LOPF39,
  pages = {71-88},
  title = {Laws of Organization in Perceptual Forms},
  publisher = {Harcourt Brace, New York},
  year = {1938},
  editor = {Ellis, W},
  author = {Max Wertheimer},
  series = {A source book of Gestalt psychology},
  note = {First published as Untersuchungen zur Lehre von der Gestalt II, in
	Psycologische Forschung, 4, 301-350. Translation published in Ellis,
	W. (1938). A source book of Gestalt psychology (pp. 71-88). London:
	Routledge \& Kegan Paul.[1]},
  journal = {A source book of Gestalt psychology},
  keywords = {Gestalt psychology, Gestalt theory, definition},
  pdf = {D:\bucko\literatureDB\Science\psychology\LOPF23.pdf},
  timestamp = {2007.03.29}
}

@ARTICLE{ULGII23,
  author = {Max Wertheimer},
  title = {Untersuchungen zur Lehre von der Gestalt II},
  journal = {Psychologische Forschung},
  year = {1923},
  volume = {3},
  pages = {301-350},
  number = {4},
  eid = {103-426-461},
  keywords = {Gestalt psycholgy},
  timestamp = {2007.03.29}
}

@INPROCEEDINGS{VHMTT04,
  author = {Torsten Wilhelm and Christian Martin},
  title = {Vergleich von hautfarbbasierten Multi-Target-Trackern},
  booktitle = {Fortschritt-Berichte VDI},
  year = {2004},
  number = {743},
  series = {10},
  pages = {pp. 27-36},
  address = {Ilmenau},
  publisher = {VDI-Verlag},
  note = {3rd Workshop on Self-Organization of AdaptiVE Behavior (SOAVE), 2004},
  abstract = {Für den Einsatz auf einem mobilen Robotersystem wurden zwei hautfarbbasierte
	Multi-Target-Tracker implementiert. Um die Hautfarberkennung unabhängig
	von variierenden Beleuchtungsverhältnissen zu machen, wurde ein
	automatischer Weißabgleich entwickelt. Beide Verfahren sind Erweiterungen
	des klassischen Partikelfilters, wobei eines eine hochdimensionale
	Sample-Konfiguration verwendet, um mehrere Objekte zu beschreiben
	und das andere mehrere einzelne Partikelfilter. Beide Verfahren
	werden theoretisch hinsichtlich der verwendeten Heuristiken und
	praktisch anhand von experimentellen Untersuchungen gegenübergestellt.},
  pdf = {G:\Science\Computerscience\Robotics\VHMTT04.pdf},
  timestamp = {2007.02.12}
}

@INPROCEEDINGS{DIMMRBS03-03,
  author = {Yuichiro Yoshikawa and Koh Hosoda and Minoru Asada},
  title = {Does the invariance in multi-modalities represent the body scheme?
	- a case study with vision and proprioception -},
  booktitle = {Proceedings of the 2nd International Symposium on Adaptive Motion
	of Animals and Machines},
  year = {2003},
  address = {Kyoto, Japan},
  month = {March},
  note = {Proceedings of the 2nd International Symposium
	
	on Adaptive Motion of Animals and Machines,
	
	Kyoto, March.4-8, 2003, Sa-P-II-1.},
  abstract = {Adaptability to the changes in the environment and the robot body
	itself fundamentally depends on the robot body representation, which
	is usually given by the designer and therefore fixed in many cases.
	In order for the robot to adapt its body representation to the changes,
	the robot should have acquired its own body representation by itself.
	This paper argues how the robot can construct such representation,
	that is, body scheme or body image from the uninterpreted row sensory
	information. Supposing that the invariance in multi sensory data
	represents the body, a cross modal map is proposed as the structure
	which learns the invariance. A preliminary experiment to learn to
	represent the body surfaces of the robot by the cross modal mapping
	between vision and proprioception is performed and future issues
	are discussed.},
  institution = {Osaka University},
  pdf = {G:\Science\Computerscience\Robotics\DIMMRBS03-03.pdf},
  timestamp = {2007.02.05},
  url = {http://www.kimura.is.uec.ac.jp/amam2003/PAPERS/N06-yoshikawa.pdf}
}

@INPROCEEDINGS{3DRULIR03,
  author = {Remo Ziegler and Wojciech Matusik and Hanspeter Pfister and Leonard
	McMillan},
  title = {3D reconstruction using labeled image regions},
  booktitle = {SGP '03: Proceedings of the 2003 Eurographics/ACM SIGGRAPH symposium
	on Geometry processing},
  year = {2003},
  pages = {248--259},
  address = {Aire-la-Ville, Switzerland, Switzerland},
  publisher = {Eurographics Association},
  isbn = {1-58113-687-0},
  location = {Aachen, Germany},
  timestamp = {2007.06.05}
}

@INPROCEEDINGS{BFTETCR02,
  author = {Zoran Zivkovic and Ferdinand van der Heijden},
  title = {Better features to track by estimating the tracking convergence region},
  booktitle = {International Conference Pattern Recognition},
  year = {2002},
  address = {Kanada},
  month = {August},
  abstract = {Reliably tracking key points and textured patches from frame to frame
	is the basic requirement for many bottom-up computer vision algorithms.
	The problem of selecting the features that can be tracked well is
	addressed here. The Lucas-Kanade tracking procedure is commonly
	used. We propose a method to estimate the size of the tracking procedure
	convergence region for each feature. The features that have a wider
	convergence region around them should be tracked better by the tracker.
	The size of the convergence region as a new feature goodness measure
	is compared with the widely accepted Shi-Tomasi feature selection
	criteria.},
  institution = {Laboratory for Measurement and Instrumentation, Faculty of Eletrical
	Engineering, University of Twente, Enschede, Netherlands},
  timestamp = {2007.04.19}
}

@comment{jabref-meta: selector_author:A. K. Jain;Aaron Edsinger;Alexan
der Stoytchev;Amy Needham;Brian Scassellati;Bruce D. Lucas;Carlo Toma
si;Charles C. Kemp;Christian Beder;Christian Martin;Cynthia Breazeal;
David Vernon;Douglas C. Schmidt;Francesco Orabona;Francesco Panerai;F
ranscesco Nori;Geert Savelsbergh;Giorgio Metta;Giulio Sandini;Greg Ha
ger;J. Konczak;James Johnson;Jean-Yves Bouguet;Jeremy Karnowski;Jianb
o Shi;Jitendra Malik;John van der Kamp;Kevin Gold;Koh Hosoda;L. Jamon
e;Lorenzo Jamone;Lorenzo Natale;M. N. Murty;Matthew M. Williamson;Mat
thew Marjanovic;Max Lungarella;Max Wertheimer;Minoru Asada;Olaf Sporn
s;P. J. Flynn;Paul Fitzpatrick;Peter Corke;Philippe Rochat;Riccardo M
anzotti;Robert Irie;Rodney A. Brooks;Rolf Pfeifer;Sajit Rao;Seth Hutc
hinson;Stephen D. Huston;Takeo Kanade;Torsten Wilhelm;Tricia Striano;
Umar Syyid;Yuichiro Yoshikawa;}

